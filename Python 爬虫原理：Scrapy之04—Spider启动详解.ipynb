{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spider启动详解\n",
    "　　参考：[scrapy启动流程图（超详细）——cmdline.py解析](https://blog.csdn.net/bf96163/article/details/112834661)<br>\n",
    "　　　　　[Scrapy crawler.py 代码详细解析](https://blog.csdn.net/bf96163/article/details/112536234)<br>\n",
    "　　　　　[Scrapy源码剖析（二）Scrapy是如何运行起来的？](https://zhuanlan.zhihu.com/p/272370864)<br>\n",
    "　　　　　[scrapy 源码解析 （一）：启动流程源码分析(一)命令行启动](https://www.cnblogs.com/qiu-hua/p/12930422.html)<br>\n",
    "　　　　　[Scrapy源码阅读分析_3_核心组件](https://blog.csdn.net/freeking101/article/details/87639667)<br>\n",
    "　　　　　[Scrapy 源码阅读（二）：看源码](https://zhuanlan.zhihu.com/p/150120517)<br>\n",
    "　　　　　[Scrapy源码阅读分析_2_启动流程](https://blog.csdn.net/freeking101/article/details/87639139)<br>\n",
    "　　　　　[Scrapy源码阅读分析_4_请求处理流程](https://blog.csdn.net/freeking101/article/details/87707531)<br>\n",
    "\n",
    "## 一、简介\n",
    "　　我们知道命令 scrapy crawler spidername 可以启动并运行爬虫程序，那么这个命令背后都做了些什么，Spider是如何被启动的呢？本单元就来研究这个内容。<br>\n",
    "　　总体来讲，scrapy爬虫启动做了两件事：<br>\n",
    "　　1、导入项目，并根据settings初始化项目。<br>\n",
    "　　2、启动一个CrawlerProcess进程，用于维护爬虫的运行，并获取reactor事件循环，用于等待执行各个爬虫的异步任务。<br>\n",
    "## 二、命令入口点\n",
    "### 2.1、命令里的scrapy是一个可执行文件，后面的crawl myspider是scrapy的参数。<BR>\n",
    "　　可执行文件scrapy在/usr/local/python/bin目录里（linux系统使用which scrapy查看），是一个python脚本，有效代码为：<BR>\n",
    ">from scrapy.cmdline import execute<BR>\n",
    "<BR>\n",
    "if __name__ == '__main__':<BR>\n",
    "　　sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv\\[0])<BR>\n",
    "　　sys.exit(execute())<BR>\n",
    "\n",
    "　　这个文件的作用就是从命令行里读取命令，然后传递给scrapy.cmdline包的execute()方法进行下一步操作（scrapy项目启动入口点）。<BR>\n",
    "### 2.2 scrapy命令的生成原理\n",
    "　　安装scrapy后, scrapy的setup.py文件生明并指定了程序的入口点为：scrapy.cmdline.execute方法。  \n",
    "　　首先我们查看一下setup.py：<br>\n",
    ">entry_points={<br>\n",
    "　　'console_scripts': \\['scrapy = scrapy.cmdline:execute'\\]<br>\n",
    "    },<br>\n",
    "\n",
    "　　可以看到，框架唯一的入口点是命令行的scrapy命令，对应scrapy.cmdline下的execute方法。<br>\n",
    "　　也就是说，我们在安装 Scrapy 的过程中，setuptools 这个包管理工具，就会把上述代码生成好并放在可执行路径下，这样当我们调用 scrapy 命令时，就会调用 Scrapy 模块下的 cmdline.py 的 execute 方法。<br>\n",
    "## 三、cmdline.py与execute方法\n",
    "　　![cmdline.py的代码逻辑](./images/scrapy_cmdline爬虫启动逻辑.png)<br>\n",
    "　　cmdline.py的excute方法，可以看做整个scrapy项目的起点，其整体操作围绕着两个部分，一个是命令解析与配置初始化;一个是crawler/crawprocess爬虫类加载与运行。<br>\n",
    "\n",
    "\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-1-1291515c5548>, line 46)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-1291515c5548>\"\u001b[1;36m, line \u001b[1;32m46\u001b[0m\n\u001b[1;33m    d[cmdname] = cmd(）              # 构造符合条件的命令的对象实列映射集\u001b[0m\n\u001b[1;37m                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "# --coding:utf-8--\n",
    "# scrapy.cmdline.py\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import optparse\n",
    "import cProfile\n",
    "import inspect\n",
    "import pkg_resources\n",
    " \n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.commands import ScrapyCommand\n",
    "from scrapy.exceptions import UsageError\n",
    "from scrapy.utils.misc import walk_modules\n",
    "from scrapy.utils.project import inside_project, get_project_settings\n",
    "from scrapy.utils.python import garbage_collect\n",
    " \n",
    " \n",
    "def _iter_command_classes(module_name):\n",
    "    # TODO: add `name` attribute to commands and and merge this function with\n",
    "    # scrapy.utils.spider.iter_spider_classes\n",
    "    # XX实现从模块名字到模块的映射XX\n",
    "    \"\"\" 递归迭代module_name下的所有模块和子模块,并判断该模块是否是 Scrapycommand 的子类，是的话就\n",
    "    抛出这个子模块；其中walk_modules 是将一个模块内的所有可用模块列出，包括子模块；\n",
    "    vars（obj）函数返回的是一个包含obj的所有属性与其值的字典对象\"\"\"\n",
    "    for module in walk_modules(module_name):\n",
    "        for obj in vars(module).values():\n",
    "            if (inspect.isclass(obj)   # 是类\n",
    "                and issubclass(obj, ScrapyCommand)    # 是Scrapy命令基类的子类\n",
    "                and obj.__module__ == module.__name__    # 是module下面的类 而不是其他模块下的类\n",
    "                and not obj == ScrapyCommand       # 是继承后的而不是本体\n",
    "            ):\n",
    "                yield obj\n",
    " \n",
    "\n",
    "def _get_commands_from_module(module, inproject):\n",
    "    \"\"\"给定一个模块 和是否在项目内的flag（inproject，用于区别全局命令和项目命令），检查\n",
    "    _iter_command_classes返回的 Scrapycommand 的子类，并按inproject筛选符合条件的命令集的实列返回\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    for cmd in _iter_command_classes(module):\n",
    "        # 在项目内时候，且目标函数不需要在项目内的时候 添加到函数字典中\n",
    "        if inproject or not cmd.requires_project:     # ScrapyCommand.requires_project 默认为false\n",
    "            cmdname = cmd.__module__.split('.')[-1]    # 命令名称字符串\n",
    "            d[cmdname] = cmd(）              # 构造符合条件的命令的对象实列映射集\n",
    "    return d\n",
    " \n",
    "\n",
    "def _get_commands_from_entry_points(inproject, group='scrapy.commands'):\n",
    "    \"\"\"本函数用于扩展自定义的scrapy命令的引入。方法意义在于项目set_up文件定义entry_point时，\n",
    "    可以导入自定义的命令；或是encry_point.txt配置文件\n",
    "    \"\"\"\n",
    "    cmds = {}\n",
    "    for entry_point in pkg_resources.iter_entry_points(group):\n",
    "        obj = entry_point.load()    # 获取指定包下的对象\n",
    "        if inspect.isclass(obj):\n",
    "            cmds[entry_point.name] = obj()   # 命令名称与命令对象实例\n",
    "        else:\n",
    "            raise Exception(f\"Invalid entry point {entry_point.name}\")\n",
    "    return cmds\n",
    " \n",
    " \n",
    "def _get_commands_dict(settings, inproject):\n",
    "    \"\"\" 从上面的两个方法中拿到所有的模块，同时如果setting中有 'COMMANDS_MODULE' 再把这些模块键入到模块列表中\"\"\"\n",
    "    cmds = _get_commands_from_module('scrapy.commands', inproject)   # 返回scrapy框架本身定义的命令与实列的映射集\n",
    "    # 寻求其他group拓展入口点的命令映射集，本处使用默认scrapy.commands,既无自定义命令，前后cmds不变\n",
    "    cmds.update(_get_commands_from_entry_points(inproject))  \n",
    "    cmds_module = settings['COMMANDS_MODULE']     # 在settings.py中指明的自定义commands，加入到cmds\n",
    "    if cmds_module:     # COMMANDS_MODULE默认''\n",
    "        cmds.update(_get_commands_from_module(cmds_module, inproject))\n",
    "    return cmds\n",
    " \n",
    " \n",
    "def _pop_command_name(argv):\n",
    "    # 功能是从命令行参数中找到第一个不以 - 开头的命令，返回这个命令：\n",
    "    i = 0\n",
    "    for arg in argv[1:]:\n",
    "        if not arg.startswith('-'):\n",
    "            del argv[i]\n",
    "            return arg\n",
    "        i += 1\n",
    " \n",
    " \n",
    "def _print_header(settings, inproject):\n",
    "    version = scrapy.__version__\n",
    "    if inproject:\n",
    "        print(f\"Scrapy {version} - project: {settings['BOT_NAME']}\\n\")\n",
    "    else:\n",
    "        print(f\"Scrapy {version} - no active project\\n\")\n",
    " \n",
    "\n",
    "def _print_commands(settings, inproject):\n",
    "    #将对应所有的方法打印出来\n",
    "    _print_header(settings, inproject)\n",
    "    print(\"Usage:\")\n",
    "    print(\"  scrapy <command> [options] [args]\\n\")\n",
    "    print(\"Available commands:\")\n",
    "    cmds = _get_commands_dict(settings, inproject)\n",
    "    for cmdname, cmdclass in sorted(cmds.items()):\n",
    "        print(f\"  {cmdname:<13} {cmdclass.short_desc()}\")\n",
    "    if not inproject:\n",
    "        print()\n",
    "        print(\"  [ more ]      More commands available when run from project directory\")\n",
    "    print()\n",
    "    print('Use \"scrapy <command> -h\" to see more info about a command')\n",
    " \n",
    " \n",
    "def _print_unknown_command(settings, cmdname, inproject):\n",
    "    _print_header(settings, inproject)\n",
    "    print(f\"Unknown command: {cmdname}\\n\")\n",
    "    print('Use \"scrapy\" to see available commands')\n",
    " \n",
    "\n",
    "def _run_print_help(parser, func, *a, **kw):\n",
    "    # 包装了一下输入函数，当func报错时输出help信息。\n",
    "    #调用函数 出错的话 用parser 传递相应问题\n",
    "    try:\n",
    "        func(*a, **kw)\n",
    "    except UsageError as e:\n",
    "        if str(e):\n",
    "            parser.error(str(e))\n",
    "        if e.print_help:\n",
    "            parser.print_help()\n",
    "        sys.exit(2)\n",
    "\n",
    "\n",
    "def execute(argv=None, settings=None):\n",
    "    # 首先是配置了argv和settings变量\n",
    "    # 是否用其他方式传入命令参数，否的话使用命令行参数\n",
    "    if argv is None:\n",
    "        argv = sys.argv\n",
    " \n",
    "    if settings is None:\n",
    "        # 没指定setting的话，就调用默认方法\n",
    "        # 从全局配置文件scrapy.cfg载入settings，再从环境中载入scrapy相关的settings到setting对象里\n",
    "        settings = get_project_settings()  \n",
    "        # set EDITOR from environment if available 用于编辑文件\n",
    "        try:\n",
    "            editor = os.environ['EDITOR']\n",
    "        except KeyError:\n",
    "            pass\n",
    "        else:\n",
    "            settings['EDITOR'] = editor\n",
    "    check_deprecated_settings(settings)   # 检查已弃用的设置，如发现，输出一行警告信息。\n",
    " \n",
    "    inproject = inside_project()  # 判断执行环境是否在项目中 主要检查scrapy.cfg配置文件是否存在\n",
    "    cmds = _get_commands_dict(settings, inproject)     # 拿到当前项目状态下所有可用命令集\n",
    "    cmdname = _pop_command_name(argv)          # 获取启动输入的命令\n",
    "    parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(),\n",
    "                                   conflict_handler='resolve')   # 指定一个 optparse 解析器\n",
    "    if not cmdname:      # 未解析出命令行输入命令\n",
    "        _print_commands(settings, inproject)\n",
    "        sys.exit(0)\n",
    "    elif cmdname not in cmds: # 解析出的命令字符不在可用命令集内\n",
    "        _print_unknown_command(settings, cmdname, inproject)\n",
    "        sys.exit(2)\n",
    " \n",
    "    cmd = cmds[cmdname]    # 拿到输入命令的实列\n",
    "    # 使用cmd的属性（usage和description）对于parser的属性做了设定。\n",
    "    # 然后将settings赋值给了cmd.settings，将parser赋给了cmd.add_options。\n",
    "    parser.usage = f\"scrapy {cmdname} {cmd.syntax()}\"\n",
    "    parser.description = cmd.long_desc()\n",
    "    settings.setdict(cmd.default_settings, priority='command')   # 将命令中的设置弄到settings中\n",
    "    cmd.settings = settings\n",
    "    cmd.add_options(parser)\n",
    "    opts, args = parser.parse_args(args=argv[1:])\n",
    " \n",
    "    # 运行命令\n",
    "    _run_print_help(parser, cmd.process_options, args, opts)   # 主要是写一些setting\n",
    "    # 生成CrawlerProcess实例，并给命令实例添加crawler_process属性\n",
    "    cmd.crawler_process = CrawlerProcess(settings)\n",
    "    _run_print_help(parser, _run_command, cmd, args, opts) # 调用cmd的run方法启动采集任务\n",
    "    sys.exit(cmd.exitcode)\n",
    " \n",
    " \n",
    "def _run_command(cmd, args, opts):\n",
    "    if opts.profile:\n",
    "        _run_command_profiled(cmd, args, opts)\n",
    "    else:\n",
    "        cmd.run(args, opts)      # 调用cmd的run方法启动采集任务\n",
    " \n",
    "\n",
    "def _run_command_profiled(cmd, args, opts):\n",
    "    # 是对cProfile做了包装，当log比较多的时候，使用cProfile可以比较方便的整理和查看。\n",
    "    # 用cpython的porfiler 运行 cmd.run(args, opts) 并传入变量\n",
    "    if opts.profile:\n",
    "        sys.stderr.write(f\"scrapy: writing cProfile stats to {opts.profile}\\n\")\n",
    "    loc = locals()           # 拿到所有变量\n",
    "    p = cProfile.Profile()\n",
    "    p.runctx('cmd.run(args, opts)', globals(), loc)     # 调用函数 并传入 global 和 locals 中的变量\n",
    "    if opts.profile:\n",
    "        p.dump_stats(opts.profile)\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        execute()\n",
    "    finally:\n",
    "        # Twisted prints errors in DebugInfo.__del__, but PyPy does not run gc.collect() on exit:\n",
    "        # http://doc.pypy.org/en/latest/cpython_differences.html\n",
    "        # ?highlight=gc.collect#differences-related-to-garbage-collection-strategies\n",
    "        garbage_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、project.py与项目初始化配置\n",
    "　　首先第一步，根据环境初始化配置。这主要和环境变量和 scrapy.cfg 有关，通过调用 get_project_settings 方法，最终生成一个 Settings 实例。在初始配置时，会加载默认的配置文件 default_settings.py，主要逻辑在 Settings 类中。<br>\n",
    "　　检查运行环境是否在项目中<br>\n",
    "　　初始化完配置之后，下面一步是检查运行环境是否在爬虫项目中。我们知道，scrapy 命令有的是依赖项目运行的，有的命令则是全局的。这里主要通过就近查找 scrapy.cfg 文件来确定是否在项目环境中，主要逻辑在 inside_project 方法中,就是判断能否找到 scrapy.cfg 文件，如果能找到，则说明是在爬虫项目中，否则就认为是执行的全局命令。<br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --coding:utf-8--\n",
    "# scrapy.utils.project.py\n",
    "import os\n",
    "from six.moves import cPickle as pickle\n",
    "import warnings\n",
    "\n",
    "from importlib import import_module\n",
    "from os.path import join, dirname, abspath, isabs, exists\n",
    "\n",
    "from scrapy.utils.conf import closest_scrapy_cfg, get_config, init_env\n",
    "from scrapy.settings import Settings\n",
    "from scrapy.exceptions import NotConfigured\n",
    "from scrapy.exceptions import ScrapyDeprecationWarning\n",
    "\n",
    "ENVVAR = 'SCRAPY_SETTINGS_MODULE'\n",
    "DATADIR_CFG_SECTION = 'datadir'\n",
    "\n",
    "\n",
    "def inside_project():\n",
    "    # 试着导入了一下setting文件，如果导入成功，说明当前目录是工程文件的根目录，返回True。\n",
    "    # 如果失败了，则需要递归的检查一下是否是在工程的子目录中，如是，返回True，不在工程目录中，返回False。\n",
    "    scrapy_module = os.environ.get('SCRAPY_SETTINGS_MODULE')\n",
    "    if scrapy_module is not None:\n",
    "        try:\n",
    "            import_module(scrapy_module)\n",
    "        except ImportError as exc:\n",
    "            warnings.warn(\"Cannot import scrapy settings module %s: %s\" % (scrapy_module, exc))\n",
    "        else:\n",
    "            return True\n",
    "    # 如果环境变量没有 就近查找scrapy.cfg 找得到就认为是在项目环境中\n",
    "    return bool(closest_scrapy_cfg())\n",
    "\n",
    "\n",
    "def project_data_dir(project='default'):\n",
    "    \"\"\"Return the current project data dir, creating it if it doesn't exist\"\"\"\n",
    "    if not inside_project():\n",
    "        raise NotConfigured(\"Not inside a project\")\n",
    "    cfg = get_config()\n",
    "    if cfg.has_option(DATADIR_CFG_SECTION, project):\n",
    "        d = cfg.get(DATADIR_CFG_SECTION, project)\n",
    "    else:\n",
    "        scrapy_cfg = closest_scrapy_cfg()\n",
    "        if not scrapy_cfg:\n",
    "            raise NotConfigured(\"Unable to find scrapy.cfg file to infer project data dir\")\n",
    "        d = abspath(join(dirname(scrapy_cfg), '.scrapy'))\n",
    "    if not exists(d):\n",
    "        os.makedirs(d)\n",
    "    return d\n",
    "\n",
    "\n",
    "def data_path(path, createdir=False):\n",
    "    \"\"\"\n",
    "    Return the given path joined with the .scrapy data directory.\n",
    "    If given an absolute path, return it unmodified.\n",
    "    \"\"\"\n",
    "    if not isabs(path):\n",
    "        if inside_project():\n",
    "            path = join(project_data_dir(), path)\n",
    "        else:\n",
    "            path = join('.scrapy', path)\n",
    "    if createdir and not exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def get_project_settings():\n",
    "    \"\"\"创建一个Setting对象，使用'SCRAPY_SETTINGS_MODULE'初始化该对象，再返回该对象。\"\"\"\n",
    "    # 首先判断是否设置了SCRAPY_SETTINGS_MODULE环境变量，这个环境变量用来指定工程的配置模块；\n",
    "    # SCRAPY_SETTINGS_MODULE环境变量不存在时,将初始化环境变量init_env(project)\n",
    "    if ENVVAR not in os.environ:   \n",
    "        # 初始化环境:找到用户配置文件settings.py的SCRAPY_PROJECT，设置到环境变量SCRAPY_SETTINGS_MODULE中\n",
    "        project = os.environ.get('SCRAPY_PROJECT', 'default')    # 默认为default\n",
    "        init_env(project)  \n",
    "\n",
    "    settings = Settings()  # 加载默认配置文件default_settings.py 生成settings实例\n",
    "    settings_module_path = os.environ.get(ENVVAR)  # 取得用户配置文件\n",
    "    if settings_module_path:    # 如果有用户配置 则覆盖默认配置\n",
    "        settings.setmodule(settings_module_path, priority='project')\n",
    "\n",
    "    # XXX: remove this hack  # 如果环境变量中有其他scrapy相关配置也覆盖\n",
    "    pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n",
    "    if pickled_settings:\n",
    "        warnings.warn(\"Use of environment variable \"\n",
    "                      \"'SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE' \"\n",
    "                      \"is deprecated.\", ScrapyDeprecationWarning)\n",
    "        settings.setdict(pickle.loads(pickled_settings), priority='project')\n",
    "\n",
    "    # XXX: deprecate and remove this functionality\n",
    "    env_overrides = {k[7:]: v for k, v in os.environ.items() if\n",
    "                     k.startswith('SCRAPY_')}\n",
    "    if env_overrides:\n",
    "        settings.setdict(env_overrides, priority='project')\n",
    "\n",
    "    return settings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、conf.py\n",
    "　　init_env首先调用get_config()获取cfg配置文件，这个配置文件获取的优先级是：<BR>\n",
    "　　1、/etc/scrapy.cfg，c:\\scrapy\\scrapy.cfg<BR>\n",
    "　　2、XDG_CONFIG_HOME环境变量指定的目录下的scrapy.cfg<BR>\n",
    "　　3、~/.scrapy.cfg<BR>\n",
    "　　4、当前执行目录下的scrapy.cfg或者父目录中的scrapy.cfg<BR>\n",
    "　　由于1，2，3默认都不设置，所以就使用当前执行命令下的scrapy.cfg，也就是工程目录下的scrapy.cfg。<BR>\n",
    "　　根据default = projectName.settings找到对应的配置模块，后面会执行一系列导入settings.py配置项的操作。<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --coding:utf-8--\n",
    "# scrapy.utils.conf.py\n",
    "import os\n",
    "import sys\n",
    "import numbers\n",
    "from operator import itemgetter\n",
    "\n",
    "import six\n",
    "if six.PY2:\n",
    "    from ConfigParser import SafeConfigParser as ConfigParser\n",
    "else:\n",
    "    from configparser import ConfigParser\n",
    "\n",
    "from scrapy.settings import BaseSettings\n",
    "from scrapy.utils.deprecate import update_classpath\n",
    "from scrapy.utils.python import without_none_values\n",
    "\n",
    "\n",
    "def build_component_list(compdict, custom=None, convert=update_classpath):\n",
    "    \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n",
    "\n",
    "    def _check_components(complist):\n",
    "        if len({convert(c) for c in complist}) != len(complist):\n",
    "            raise ValueError('Some paths in {!r} convert to the same object, '\n",
    "                             'please update your settings'.format(complist))\n",
    "\n",
    "    def _map_keys(compdict):\n",
    "        if isinstance(compdict, BaseSettings):\n",
    "            compbs = BaseSettings()\n",
    "            for k, v in six.iteritems(compdict):\n",
    "                prio = compdict.getpriority(k)\n",
    "                if compbs.getpriority(convert(k)) == prio:\n",
    "                    raise ValueError('Some paths in {!r} convert to the same '\n",
    "                                     'object, please update your settings'\n",
    "                                     ''.format(list(compdict.keys())))\n",
    "                else:\n",
    "                    compbs.set(convert(k), v, priority=prio)\n",
    "            return compbs\n",
    "        else:\n",
    "            _check_components(compdict)\n",
    "            return {convert(k): v for k, v in six.iteritems(compdict)}\n",
    "\n",
    "    def _validate_values(compdict):\n",
    "        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n",
    "        for name, value in six.iteritems(compdict):\n",
    "            if value is not None and not isinstance(value, numbers.Real):\n",
    "                raise ValueError('Invalid value {} for component {}, please provide ' \\\n",
    "                                 'a real number or None instead'.format(value, name))\n",
    "\n",
    "    # BEGIN Backward compatibility for old (base, custom) call signature\n",
    "    if isinstance(custom, (list, tuple)):\n",
    "        _check_components(custom)\n",
    "        return type(custom)(convert(c) for c in custom)\n",
    "\n",
    "    if custom is not None:\n",
    "        compdict.update(custom)\n",
    "    # END Backward compatibility\n",
    "\n",
    "    _validate_values(compdict)\n",
    "    compdict = without_none_values(_map_keys(compdict))\n",
    "    return [k for k, v in sorted(six.iteritems(compdict), key=itemgetter(1))]\n",
    "\n",
    "\n",
    "def arglist_to_dict(arglist):\n",
    "    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n",
    "    dict\n",
    "    \"\"\"\n",
    "    return dict(x.split('=', 1) for x in arglist)\n",
    "\n",
    "\n",
    "def closest_scrapy_cfg(path='.', prevpath=None):\n",
    "    \"\"\"Return the path to the closest scrapy.cfg file by traversing the current\n",
    "    directory and its parents\n",
    "    在当前文件夹下，寻找有没有scrapy.cfg文件，如果没有找到，则递归遍历父文件夹，直到遍历到根目录。\n",
    "    如果找到了scrapy.cfg文件，则返回scrapy.cfg的绝对路径。搜索最靠近当前当前路径的scrapy.cfg配置文件并返回其路径。\n",
    "    \"\"\"\n",
    "    if path == prevpath:\n",
    "        return ''\n",
    "    path = os.path.abspath(path)\n",
    "    cfgfile = os.path.join(path, 'scrapy.cfg')\n",
    "    if os.path.exists(cfgfile):\n",
    "        return cfgfile\n",
    "    return closest_scrapy_cfg(os.path.dirname(path), path)\n",
    "\n",
    "\n",
    "def init_env(project='default', set_syspath=True):\n",
    "    \"\"\"Initialize environment to use command-line tool from inside a project\n",
    "    dir. This sets the Scrapy settings module and modifies the Python path to\n",
    "    be able to locate the project module.\n",
    "    首先把'SCRAPY_SETTINGS_MODULE'设置为.cfg下指明的settings文件，然后调用\n",
    "    closest_scrapy_cfg()函数，将工程目录添加到系统的path当中。\n",
    "    \"\"\"\n",
    "    cfg = get_config()\n",
    "    # 获取到配置文件后设置系统环境变量SCRAPY_SETTINGS_MODULE为配置模块路径，\n",
    "    if cfg.has_option('settings', project):\n",
    "        # 将项目配置模块路径设置进系统环境变量\n",
    "        os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('settings', project)\n",
    "    closest = closest_scrapy_cfg()\n",
    "    # 将最近的scrapy.cfg模块路径放入系统路径使Python能够找到该模块导入\n",
    "    if closest:\n",
    "        projdir = os.path.dirname(closest)\n",
    "        if set_syspath and projdir not in sys.path:\n",
    "            sys.path.append(projdir)\n",
    "\n",
    "\n",
    "def get_config(use_closest=True):\n",
    "    \"\"\"Get Scrapy config file as a ConfigParser\n",
    "    返回一个SafeConfigParser()示例，用于对.cfg文件的包装。\n",
    "    \"\"\"\n",
    "    sources = get_sources(use_closest)\n",
    "    cfg = ConfigParser()\n",
    "    cfg.read(sources)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_sources(use_closest=True):\n",
    "    # 返回了一系列.cfg文件可能存在的路径，其中包括当前文件夹使用closest_scrapy_cfg()函数获取到的路径。\n",
    "    xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or \\\n",
    "        os.path.expanduser('~/.config')\n",
    "    sources = ['/etc/scrapy.cfg', r'c:\\scrapy\\scrapy.cfg',\n",
    "               xdg_config_home + '/scrapy.cfg',\n",
    "               os.path.expanduser('~/.scrapy.cfg')]\n",
    "    if use_closest:\n",
    "        sources.append(closest_scrapy_cfg())\n",
    "    return sources\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、命令执行\n",
    "　　这里以crawl命令为例！<br>\n",
    "　　scrapy/commands/crawl.py#Command:\n",
    ">def run(self, args, opts):<BR>\n",
    "　　if len(args) < 1:<BR>\n",
    "　　　　raise UsageError()<BR>\n",
    "　　elif len(args) > 1:<BR>\n",
    "　　　　raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")<BR>\n",
    "　　spname = args[0]<BR>\n",
    "<BR>\n",
    "　　self.crawler_process.crawl(spname, **opts.spargs)<BR>\n",
    "　　self.crawler_process.start()<BR>\n",
    "\n",
    "　　**这里有2个重要操作：**<BR>\n",
    "　　1.调用CrawlerProcess的crawl方法，执行初始化（创建新爬虫对象）。<BR>\n",
    "　　2.调用CrawlerProcess的start方法，正式运行。<BR>\n",
    "CrawlerProcess便是scrapy运行过程中最根本的进程，是所有爬虫运行的基础。<BR>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --coding:utf-8--\n",
    "import logging\n",
    "import pprint\n",
    "import signal\n",
    "import warnings\n",
    " \n",
    "from twisted.internet import defer\n",
    "from zope.interface.exceptions import DoesNotImplement\n",
    " \n",
    "try:\n",
    "    # zope >= 5.0 only supports MultipleInvalid\n",
    "    from zope.interface.exceptions import MultipleInvalid\n",
    "except ImportError:\n",
    "    MultipleInvalid = None\n",
    " \n",
    "from zope.interface.verify import verifyClass\n",
    " \n",
    "from scrapy import signals, Spider\n",
    "from scrapy.core.engine import ExecutionEngine\n",
    "from scrapy.exceptions import ScrapyDeprecationWarning\n",
    "from scrapy.extension import ExtensionManager\n",
    "from scrapy.interfaces import ISpiderLoader\n",
    "from scrapy.settings import overridden_settings, Settings\n",
    "from scrapy.signalmanager import SignalManager\n",
    "from scrapy.utils.log import (\n",
    "    configure_logging,\n",
    "    get_scrapy_root_handler,\n",
    "    install_scrapy_root_handler,\n",
    "    log_scrapy_info,\n",
    "    LogCounterHandler,\n",
    ")\n",
    "from scrapy.utils.misc import create_instance, load_object\n",
    "from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n",
    "from scrapy.utils.reactor import install_reactor, verify_installed_reactor\n",
    " \n",
    "# scrapy 中文解释及其注释\n",
    "logger = logging.getLogger(__name__)\n",
    " \n",
    "#实际爬取执行的类\n",
    "class Crawler:\n",
    " \n",
    "    def __init__(self, spidercls, settings=None):\n",
    "        if isinstance(spidercls, Spider):\n",
    "            #要求传入的是类而不是实例\n",
    "            raise ValueError('The spidercls argument must be a class, not an object')\n",
    " \n",
    "        if isinstance(settings, dict) or settings is None:\n",
    "            #转化为setting对象\n",
    "            settings = Settings(settings)\n",
    " \n",
    "        self.spidercls = spidercls\n",
    "        self.settings = settings.copy()\n",
    "        #然后使用spidercls类的update_setting方式来更新设置\n",
    "        self.spidercls.update_settings(self.settings)\n",
    " \n",
    "        self.signals = SignalManager(self)\n",
    "        #从类的setting中的STATS_CLASS拿到stats\n",
    "        self.stats = load_object(self.settings['STATS_CLASS'])(self)\n",
    "        #从setting中拿到loglevel 将初始化的LogCounterHandler 加入到logging.root\n",
    "        handler = LogCounterHandler(self, level=self.settings.get('LOG_LEVEL'))\n",
    "        logging.root.addHandler(handler)\n",
    "        # 显示出来所有被复写的setting\n",
    "        d = dict(overridden_settings(self.settings))\n",
    "        logger.info(\"Overridden settings:\\n%(settings)s\",\n",
    "                    {'settings': pprint.pformat(d)})\n",
    " \n",
    "        if get_scrapy_root_handler() is not None:\n",
    "            # scrapy root handler already installed: update it with new settings\n",
    "            install_scrapy_root_handler(self.settings)\n",
    "        # lambda is assigned to Crawler attribute because this way it is not\n",
    "        # garbage collected after leaving __init__ scope\n",
    "        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n",
    "        #将该signals.engine_stopped信号的callback注册到self.__remove_handler函数上\n",
    "        self.signals.connect(self.__remove_handler, signals.engine_stopped)\n",
    "        # log格式指定\n",
    "        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n",
    "        self.logformatter = lf_cls.from_crawler(self)\n",
    "        # 扩展 还没看\n",
    "        self.extensions = ExtensionManager.from_crawler(self)\n",
    " \n",
    "        self.settings.freeze()\n",
    "        self.crawling = False\n",
    "        self.spider = None\n",
    "        self.engine = None\n",
    "    #defer.inlineCallbacks 装饰器 是指当使用异步调用该方法时候，\n",
    "    # 该方法可以用类似同步语法的方法写异步的工作，其中yield deferred对象后\n",
    "    # 后续代码会等待这个yield出去的deferred成功返回后再进行下一步\n",
    "    # 其中等待时间交还给reactor。\n",
    "    @defer.inlineCallbacks\n",
    "    def crawl(self, *args, **kwargs):\n",
    "        if self.crawling:\n",
    "            raise RuntimeError(\"Crawling already taking place\")\n",
    "        self.crawling = True\n",
    " \n",
    "        try:\n",
    "            self.spider = self._create_spider(*args, **kwargs)\n",
    "            self.engine = self._create_engine()\n",
    "            #从self.spider.start_requests()中拿到requests\n",
    "            start_requests = iter(self.spider.start_requests())\n",
    "            #调用异步方法，开始爬虫爬取工作\n",
    "            yield self.engine.open_spider(self.spider, start_requests)\n",
    "            #调用核心的start方法，并将返回值包装成deferred对象\n",
    "            yield defer.maybeDeferred(self.engine.start)\n",
    "        except Exception:\n",
    "            self.crawling = False\n",
    "            if self.engine is not None:\n",
    "                yield self.engine.close()\n",
    "            raise\n",
    " \n",
    "    def _create_spider(self, *args, **kwargs):\n",
    "        #调用传入spider类的from_crawler(self, args, *kwargs)\n",
    "        return self.spidercls.from_crawler(self, *args, **kwargs)\n",
    " \n",
    "    def _create_engine(self):\n",
    "        #ExecutionEngine(self, lambda _: self.stop()) 传入stop函数 实例化engine\n",
    "        return ExecutionEngine(self, lambda _: self.stop())\n",
    " \n",
    "    #重置self.crawling 为false 同时发出 异步命令self.engine.stop 包装一下self.engine.stop成为deferred对象\n",
    "    @defer.inlineCallbacks\n",
    "    def stop(self):\n",
    "        \"\"\"Starts a graceful stop of the crawler and returns a deferred that is\n",
    "        fired when the crawler is stopped.\"\"\"\n",
    "        if self.crawling:\n",
    "            self.crawling = False\n",
    "            yield defer.maybeDeferred(self.engine.stop)\n",
    " \n",
    "# 简单的说 如果你自己的应用用到reactor 可以考虑用这个类控制spider启停等，不然用CrawlerProcess\n",
    "class CrawlerRunner:\n",
    "    \"\"\"\n",
    "    This is a convenient helper class that keeps track of, manages and runs\n",
    "    crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n",
    "    The CrawlerRunner object must be instantiated with a\n",
    "    :class:`~scrapy.settings.Settings` object.\n",
    "    This class shouldn't be needed (since Scrapy is responsible of using it\n",
    "    accordingly) unless writing scripts that manually handle the crawling\n",
    "    process. See :ref:`run-from-script` for an example.\n",
    "    \"\"\"\n",
    "    #property()函数是用来指定当前属性的文件描述符类的方法，这里就是把lambda作为 他的getter 返回的是self._crawlers\n",
    "    crawlers = property(\n",
    "        lambda self: self._crawlers,\n",
    "        doc=\"Set of :class:`crawlers <scrapy.crawler.Crawler>` started by \"\n",
    "            \":meth:`crawl` and managed by this class.\"\n",
    "    )\n",
    "    #从setting里 建立spiderloader实例\n",
    "    @staticmethod\n",
    "    def _get_spider_loader(settings):\n",
    "        \"\"\" Get SpiderLoader instance from settings \"\"\"\n",
    "        cls_path = settings.get('SPIDER_LOADER_CLASS')\n",
    "        #load_object(cls_path) 是将xx.xx路径转为实例的类\n",
    "        loader_cls = load_object(cls_path)\n",
    "        excs = (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement\n",
    "        try:\n",
    "            verifyClass(ISpiderLoader, loader_cls)\n",
    "        except excs:\n",
    "            warnings.warn(\n",
    "                'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n",
    "                'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n",
    "                'Please add all missing methods to avoid unexpected runtime errors.',\n",
    "                category=ScrapyDeprecationWarning, stacklevel=2\n",
    "            )\n",
    "        #这里返回的是 setting中spiderloader加载目前的setting后的实例\n",
    "        return loader_cls.from_settings(settings.frozencopy())\n",
    " \n",
    "    def __init__(self, settings=None):\n",
    "        if isinstance(settings, dict) or settings is None:\n",
    "            settings = Settings(settings)\n",
    "        self.settings = settings\n",
    "        self.spider_loader = self._get_spider_loader(settings)\n",
    "        self._crawlers = set()\n",
    "        self._active = set()\n",
    "        self.bootstrap_failed = False\n",
    "        self._handle_twisted_reactor()\n",
    " \n",
    "    @property\n",
    "    def spiders(self):\n",
    "        warnings.warn(\"CrawlerRunner.spiders attribute is renamed to \"\n",
    "                      \"CrawlerRunner.spider_loader.\",\n",
    "                      category=ScrapyDeprecationWarning, stacklevel=2)\n",
    "        return self.spider_loader\n",
    " \n",
    "    #生成一个crawler对象 然后调用其crawl方法，其中有些管理crawler的deferred的部分 在_crawl里\n",
    "    def crawl(self, crawler_or_spidercls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Run a crawler with the provided arguments.\n",
    "        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n",
    "        keeping track of it so it can be stopped later.\n",
    "        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n",
    "        instance, this method will try to create one using this parameter as\n",
    "        the spider class given to it.\n",
    "        Returns a deferred that is fired when the crawling is finished.\n",
    "        :param crawler_or_spidercls: already created crawler, or a spider class\n",
    "            or spider's name inside the project to create it\n",
    "        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n",
    "            :class:`~scrapy.spiders.Spider` subclass or string\n",
    "        :param args: arguments to initialize the spider\n",
    "        :param kwargs: keyword arguments to initialize the spider\n",
    "        \"\"\"\n",
    "        if isinstance(crawler_or_spidercls, Spider):\n",
    "            raise ValueError(\n",
    "                'The crawler_or_spidercls argument cannot be a spider object, '\n",
    "                'it must be a spider class (or a Crawler object)')\n",
    "        crawler = self.create_crawler(crawler_or_spidercls)\n",
    "        return self._crawl(crawler, *args, **kwargs)\n",
    " \n",
    "    def _crawl(self, crawler, *args, **kwargs):\n",
    "        self.crawlers.add(crawler) #这个集合里是crawler\n",
    "        d = crawler.crawl(*args, **kwargs)\n",
    "        self._active.add(d) # 这个集合里是执行crawl后的deferred对象\n",
    " \n",
    "        def _done(result):\n",
    "            # discard 相当于不报错的remove\n",
    "            self.crawlers.discard(crawler)\n",
    "            self._active.discard(d)\n",
    "            #a|=2等价于a=a|2(按位或)\n",
    "            self.bootstrap_failed |= not getattr(crawler, 'spider', None)\n",
    "            return result\n",
    "        # 运行crawl前 加入到管理集合，给其deferred对象添加结束后清理管理集合的代码\n",
    "        return d.addBoth(_done)\n",
    " \n",
    "    def create_crawler(self, crawler_or_spidercls):\n",
    "        \"\"\"\n",
    "        Return a :class:`~scrapy.crawler.Crawler` object.\n",
    "        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n",
    "        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n",
    "          is constructed for it.\n",
    "        * If ``crawler_or_spidercls`` is a string, this function finds\n",
    "          a spider with this name in a Scrapy project (using spider loader),\n",
    "          then creates a Crawler instance for it.\n",
    "        \"\"\"\n",
    "        if isinstance(crawler_or_spidercls, Spider):\n",
    "            raise ValueError(\n",
    "                'The crawler_or_spidercls argument cannot be a spider object, '\n",
    "                'it must be a spider class (or a Crawler object)')\n",
    "        if isinstance(crawler_or_spidercls, Crawler):\n",
    "            return crawler_or_spidercls\n",
    " \n",
    "        return self._create_crawler(crawler_or_spidercls)\n",
    " \n",
    "    def _create_crawler(self, spidercls):\n",
    "        if isinstance(spidercls, str):\n",
    "            spidercls = self.spider_loader.load(spidercls)\n",
    "        # 实际实例化Crawler进行的地方 传入的是spider的类和setting\n",
    "        return Crawler(spidercls, self.settings)\n",
    " \n",
    "    def stop(self):\n",
    "        \"\"\"\n",
    "        Stops simultaneously all the crawling jobs taking place.\n",
    "        Returns a deferred that is fired when they all have ended.\n",
    "        \"\"\"\n",
    "        #跟crawler不一样，这个是一个由每个crawler执行stop函数后返回的deferred对象列表\n",
    "        return defer.DeferredList([c.stop() for c in list(self.crawlers)])\n",
    " \n",
    "    # 跟多进程的join类似，等待所有crawler完成任务\n",
    "    # 上面说到这个_active集合是所有crawler的deferred对象 把他们yield出去 以便调用后续callback\n",
    "    @defer.inlineCallbacks\n",
    "    def join(self):\n",
    "        \"\"\"\n",
    "        join()\n",
    "        Returns a deferred that is fired when all managed :attr:`crawlers` have\n",
    "        completed their executions.\n",
    "        \"\"\"\n",
    "        while self._active:\n",
    "            yield defer.DeferredList(self._active)\n",
    "    # 这个方法返回从传入setting 字符中加载 用 load_object()加载进来的recator实例 或者啥也不做\n",
    "    def _handle_twisted_reactor(self):\n",
    "        if self.settings.get(\"TWISTED_REACTOR\"):\n",
    "            verify_installed_reactor(self.settings[\"TWISTED_REACTOR\"])\n",
    " \n",
    "#这个crawlerProcess 是用来在不用recator的应用里使用 同时在一个进程里使用多个spider的\n",
    "# 如果只是使用scrapy就不用改这个，除非想把scrapy放到你自己应用里　\n",
    "# 实际上这个是上一个 CrawlerRunner 添加了reactor后的东西\n",
    "class CrawlerProcess(CrawlerRunner):\n",
    "    \"\"\"\n",
    "    A class to run multiple scrapy crawlers in a process simultaneously.\n",
    "    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n",
    "    for starting a :mod:`~twisted.internet.reactor` and handling shutdown\n",
    "    signals, like the keyboard interrupt command Ctrl-C. It also configures\n",
    "    top-level logging.\n",
    "    This utility should be a better fit than\n",
    "    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n",
    "    :mod:`~twisted.internet.reactor` within your application.\n",
    "    The CrawlerProcess object must be instantiated with a\n",
    "    :class:`~scrapy.settings.Settings` object.\n",
    "    :param install_root_handler: whether to install root logging handler\n",
    "        (default: True)\n",
    "    This class shouldn't be needed (since Scrapy is responsible of using it\n",
    "    accordingly) unless writing scripts that manually handle the crawling\n",
    "    process. See :ref:`run-from-script` for an example.\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, settings=None, install_root_handler=True):\n",
    "        super().__init__(settings)\n",
    "        #将shutdownhandler 加载为函数_signal_shutdown\n",
    "        install_shutdown_handlers(self._signal_shutdown)\n",
    "        configure_logging(self.settings, install_root_handler)\n",
    "        log_scrapy_info(self.settings)\n",
    " \n",
    "    def _signal_shutdown(self, signum, _):\n",
    "        from twisted.internet import reactor\n",
    "        #将shutdownhandler注册为 _signal_kill\n",
    "        install_shutdown_handlers(self._signal_kill)\n",
    "        signame = signal_names[signum]\n",
    "        logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \",\n",
    "                    {'signame': signame})\n",
    "        #使用reactor.callFromThread(self._graceful_stop_reactor)命令调用自身的结束语句\n",
    "        reactor.callFromThread(self._graceful_stop_reactor)\n",
    " \n",
    "    def _signal_kill(self, signum, _):\n",
    "        from twisted.internet import reactor\n",
    "        #将shutdownhandler注册为signal.SIG_IGN\n",
    "        install_shutdown_handlers(signal.SIG_IGN)\n",
    "        signame = signal_names[signum]\n",
    "        logger.info('Received %(signame)s twice, forcing unclean shutdown',\n",
    "                    {'signame': signame})\n",
    "        #直接将recator关闭\n",
    "        reactor.callFromThread(self._stop_reactor)\n",
    " \n",
    "    def start(self, stop_after_crawl=True):\n",
    "        \"\"\"\n",
    "        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n",
    "        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n",
    "        based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n",
    "        If ``stop_after_crawl`` is True, the reactor will be stopped after all\n",
    "        crawlers have finished, using :meth:`join`.\n",
    "        :param bool stop_after_crawl: stop or not the reactor when all\n",
    "            crawlers have finished\n",
    "        \"\"\"\n",
    "        from twisted.internet import reactor\n",
    "        #设置如果爬完后关闭recator的话 就添加相应的callback结束callback 这就是上面join的用处 ，如果这里不设置为true\n",
    "        # 那么这个reactor就会留着不销毁\n",
    "        if stop_after_crawl:\n",
    "            d = self.join()\n",
    "            # Don't start the reactor if the deferreds are already fired\n",
    "            if d.called:\n",
    "                return\n",
    "            d.addBoth(self._stop_reactor)\n",
    "        #加载一个配置 threadpool和dns_resolver的配置到recator\n",
    "        resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n",
    "        resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n",
    "        resolver.install_on_reactor()\n",
    "        tp = reactor.getThreadPool()\n",
    "        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n",
    "        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n",
    "        #启动reactor并阻塞\n",
    "        reactor.run(installSignalHandlers=False)  # blocking call\n",
    " \n",
    "    def _graceful_stop_reactor(self):\n",
    "        # 给所有crawler 的deferred对象后面添加一个完成后销毁的动作\n",
    "        d = self.stop()\n",
    "        d.addBoth(self._stop_reactor)\n",
    "        return d\n",
    " \n",
    "    def _stop_reactor(self, _=None):\n",
    "        from twisted.internet import reactor\n",
    "        try:\n",
    "            reactor.stop()\n",
    "        except RuntimeError:  # raised if already stopped or in shutdown stage\n",
    "            pass\n",
    " \n",
    "    def _handle_twisted_reactor(self):\n",
    "        if self.settings.get(\"TWISTED_REACTOR\"):\n",
    "            install_reactor(self.settings[\"TWISTED_REACTOR\"], self.settings[\"ASYNCIO_EVENT_LOOP\"])\n",
    "        super()._handle_twisted_reactor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
