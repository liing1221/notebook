{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spidermw中间件管理器详解\n",
    "　　参考：[scrapy源码5：middleware的源码分析](https://cuiyonghua.blog.csdn.net/article/details/108319046)<br>\n",
    "　　　　　[scrapy源码4：spidermw的源码分析](https://cuiyonghua.blog.csdn.net/article/details/108318722)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  spidermw.py\n",
    "import six\n",
    "from twisted.python.failure import Failure\n",
    "# 导入six包和导入failure\n",
    "\n",
    "from scrapy.middleware import MiddlewareManager\n",
    "# 这里导入了一个中间件管理的基类，应该适用于后续的继承的吧。 \n",
    "\n",
    "from scrapy.utils.defer import mustbe_deferred\n",
    "from scrapy.utils.conf import build_component_list\n",
    "# 这里从defer里面和conf里面导入2个方法。 先看看具体实现方法吧。 \n",
    "\n",
    "def mustbe_deferred(f, *args, **kw):\n",
    "    \"\"\"Same as twisted.internet.defer.maybeDeferred, but delay calling\n",
    "    callback/errback to next reactor loop\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = f(*args, **kw)\n",
    "    # FIXME: Hack to avoid introspecting tracebacks. This to speed up\n",
    "    # processing of IgnoreRequest errors which are, by far, the most common\n",
    "    # exception in Scrapy - see #125\n",
    "    except IgnoreRequest as e:\n",
    "        return defer_fail(failure.Failure(e))\n",
    "    except:\n",
    "        return defer_fail(failure.Failure())\n",
    "    else:\n",
    "        return defer_result(result)\n",
    "# 这个方法有点类似于defer_result 的意思。 不管啥请求都是调用了defer_fail或者defer_result . 都等100ms完成读写。\n",
    "\n",
    "def build_component_list(compdict, custom=None, convert=update_classpath):\n",
    "    \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n",
    "\n",
    "    def _check_components(complist):\n",
    "        if len({convert(c) for c in complist}) != len(complist):\n",
    "            raise ValueError('Some paths in {!r} convert to the same object, '\n",
    "                             'please update your settings'.format(complist))\n",
    "\n",
    "    def _map_keys(compdict):\n",
    "        if isinstance(compdict, BaseSettings):\n",
    "            compbs = BaseSettings()\n",
    "            for k, v in six.iteritems(compdict):\n",
    "                prio = compdict.getpriority(k)\n",
    "                if compbs.getpriority(convert(k)) == prio:\n",
    "                    raise ValueError('Some paths in {!r} convert to the same '\n",
    "                                     'object, please update your settings'\n",
    "                                     ''.format(list(compdict.keys())))\n",
    "                else:\n",
    "                    compbs.set(convert(k), v, priority=prio)\n",
    "            return compbs\n",
    "        else:\n",
    "            _check_components(compdict)\n",
    "            return {convert(k): v for k, v in six.iteritems(compdict)}\n",
    "\n",
    "    def _validate_values(compdict):\n",
    "        \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n",
    "        for name, value in six.iteritems(compdict):\n",
    "            if value is not None and not isinstance(value, numbers.Real):\n",
    "                raise ValueError('Invalid value {} for component {}, please provide ' \\\n",
    "                                 'a real number or None instead'.format(value, name))\n",
    "\n",
    "    # BEGIN Backwards compatibility for old (base, custom) call signature\n",
    "    if isinstance(custom, (list, tuple)):\n",
    "        _check_components(custom)\n",
    "        return type(custom)(convert(c) for c in custom)\n",
    "\n",
    "    if custom is not None:\n",
    "        compdict.update(custom)\n",
    "    # END Backwards compatibility\n",
    "\n",
    "    _validate_values(compdict)\n",
    "    compdict = without_none_values(_map_keys(compdict))\n",
    "    return [k for k, v in sorted(six.iteritems(compdict), key=itemgetter(1))]\n",
    "# 这个方法真的长 啊， 内部嵌套了几个方法。 \n",
    "# 我们把内部嵌套的几个方法都分析下吧。 \n",
    "    def _check_components(complist):\n",
    "        if len({convert(c) for c in complist}) != len(complist):\n",
    "            raise ValueError('Some paths in {!r} convert to the same object, '\n",
    "                             'please update your settings'.format(complist))\n",
    "# 这个方法从名字上看是检查组件的。先判断下长度相等不， 如果不等就抛出异常了。 \n",
    "# 判断过程中用到了convert方法， 发现这个方法有默认值的。也就是convert使用默认的update_classpath方法处理，我们定位过去看看。\n",
    "def update_classpath(path):\n",
    "    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n",
    "    for prefix, replacement in DEPRECATION_RULES:\n",
    "        if path.startswith(prefix):\n",
    "            new_path = path.replace(prefix, replacement, 1)\n",
    "            warnings.warn(\"`{}` class is deprecated, use `{}` instead\".format(path, new_path),\n",
    "                          ScrapyDeprecationWarning)\n",
    "            return new_path\n",
    "    return path\n",
    "# 这里有个规则啊 ，如果以旧的开头， 将他换成新的，然后提示一个警告信息。 返回新的路径。\n",
    "# 规则我这里也粘贴过来吧。 \n",
    "\n",
    "DEPRECATION_RULES = [\n",
    "    ('scrapy.contrib_exp.downloadermiddleware.decompression.', 'scrapy.downloadermiddlewares.decompression.'),\n",
    "    ('scrapy.contrib_exp.iterators.', 'scrapy.utils.iterators.'),\n",
    "    ('scrapy.contrib.downloadermiddleware.', 'scrapy.downloadermiddlewares.'),\n",
    "    ('scrapy.contrib.exporter.', 'scrapy.exporters.'),\n",
    "    ('scrapy.contrib.linkextractors.', 'scrapy.linkextractors.'),\n",
    "    ('scrapy.contrib.loader.processor.', 'scrapy.loader.processors.'),\n",
    "    ('scrapy.contrib.loader.', 'scrapy.loader.'),\n",
    "    ('scrapy.contrib.pipeline.', 'scrapy.pipelines.'),\n",
    "    ('scrapy.contrib.spidermiddleware.', 'scrapy.spidermiddlewares.'),\n",
    "    ('scrapy.contrib.spiders.', 'scrapy.spiders.'),\n",
    "    ('scrapy.contrib.', 'scrapy.extensions.'),\n",
    "    ('scrapy.command.', 'scrapy.commands.'),\n",
    "    ('scrapy.dupefilter.', 'scrapy.dupefilters.'),\n",
    "    ('scrapy.linkextractor.', 'scrapy.linkextractors.'),\n",
    "    ('scrapy.telnet.', 'scrapy.extensions.telnet.'),\n",
    "    ('scrapy.spider.', 'scrapy.spiders.'),\n",
    "    ('scrapy.squeue.', 'scrapy.squeues.'),\n",
    "    ('scrapy.statscol.', 'scrapy.statscollectors.'),\n",
    "    ('scrapy.utils.decorator.', 'scrapy.utils.decorators.'),\n",
    "    ('scrapy.spidermanager.SpiderManager', 'scrapy.spiderloader.SpiderLoader'),\n",
    "]\n",
    "# 看完这个方法，我们就明白了上面的代码\n",
    "  def _check_components(complist):\n",
    "        if len({convert(c) for c in complist}) != len(complist):\n",
    "            raise ValueError('Some paths in {!r} convert to the same object, '\n",
    "                             'please update your settings'.format(complist))\n",
    "    # 这个他判定长度不等的情况一般是有新的旧的设置，重复导致的。 让你检查你的设置。\n",
    "\n",
    "def _map_keys(compdict):\n",
    "    if isinstance(compdict, BaseSettings):\n",
    "        compbs = BaseSettings()\n",
    "        for k, v in six.iteritems(compdict):\n",
    "            prio = compdict.getpriority(k)\n",
    "            if compbs.getpriority(convert(k)) == prio:\n",
    "                raise ValueError('Some paths in {!r} convert to the same '\n",
    "                                    'object, please update your settings'\n",
    "                                    ''.format(list(compdict.keys())))\n",
    "            else:\n",
    "                compbs.set(convert(k), v, priority=prio)\n",
    "        return compbs\n",
    "    else:\n",
    "        _check_components(compdict)\n",
    "        return {convert(k): v for k, v in six.iteritems(compdict)}\n",
    "# 这个判定下compdict是basesettings的子类， 如果是的话，构造一个basesettings， 遍历compdict\n",
    "# 获取指定key的优先级prio, 如果优先级有相等的是要抛出异常的。其他情况下， 把优先级设置为compdict中指定的优先级。 \n",
    "# 如果不是basesetting的子类。 就调用check_components去检查设置重复，然后返回一个dict对象。 \n",
    "\n",
    "def _validate_values(compdict):\n",
    "    \"\"\"Fail if a value in the components dict is not a real number or None.\"\"\"\n",
    "    for name, value in six.iteritems(compdict):\n",
    "        if value is not None and not isinstance(value, numbers.Real):\n",
    "            raise ValueError('Invalid value {} for component {}, please provide ' \\\n",
    "                                'a real number or None instead'.format(value, name))\n",
    "    # 这个方法就是判定compdict里面value不是none或者real的话就抛出异常。 \n",
    "\n",
    "# 几个内嵌的小方法看完了， 我们还是回到这个build_component_list 这个方法。 \n",
    "\n",
    "# 如果custom是list,tuple的实例的话调用检查个数， 返回指定类对象。 这里返回一个list，或者元组。 \n",
    "\n",
    "# 如果custome 不是none的话，就更新下comdict \n",
    "# 验证下compdict,都是数值的。 \n",
    "def without_none_values(iterable):\n",
    "    \"\"\"Return a copy of `iterable` with all `None` entries removed.\n",
    "\n",
    "    If `iterable` is a mapping, return a dictionary where all pairs that have\n",
    "    value `None` have been removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return {k: v for k, v in six.iteritems(iterable) if v is not None}\n",
    "    except AttributeError:\n",
    "        return type(iterable)((v for v in iterable if v is not None))\n",
    "        # 这个方法就是把none去掉。 如果是映射的话去掉。 \n",
    "        # 这个方法的异常不知道为何要写这个。 方法吧， 可能这个方法其他地方还有其他用地。\n",
    "# 然后这个方法返回一个排序的key数值， 具体排序方法使用了itemgetter(1) ，定位过去看下。 \n",
    "    \"\"\"\n",
    "    Return a callable object that fetches the given item(s) from its operand.\n",
    "    After f = itemgetter(2), the call f(r) returns r[2].\n",
    "    After g = itemgetter(2, 5, 3), the call g(r) returns (r[2], r[5], r[3])\n",
    "    \"\"\"\n",
    "# 这个说明够详细了吧。 根据value去排序key的上面的语句。 \n",
    "\n",
    "\n",
    "# 接下来就是爬虫中间件的具体实现代码了。 我们这里可以看到他继承了。 中间件管理类， 我们看看， 如果简单的话， 就先看看\n",
    "# 如果复杂的话就先放放。\n",
    "\n",
    "def _get_mwlist_from_settings(cls, settings):\n",
    "    return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))\n",
    "# 从名字上， 我们知道这个是从settings里面获取中间件的列表的。没问题的。 \n",
    "\n",
    "  def _add_middleware(self, mw):\n",
    "        super(SpiderMiddlewareManager, self)._add_middleware(mw)\n",
    "        if hasattr(mw, 'process_spider_input'):\n",
    "            self.methods['process_spider_input'].append(mw.process_spider_input)\n",
    "        if hasattr(mw, 'process_spider_output'):\n",
    "            self.methods['process_spider_output'].insert(0, mw.process_spider_output)\n",
    "        if hasattr(mw, 'process_spider_exception'):\n",
    "            self.methods['process_spider_exception'].insert(0, mw.process_spider_exception)\n",
    "        if hasattr(mw, 'process_start_requests'):\n",
    "            self.methods['process_start_requests'].insert(0, mw.process_start_requests)\n",
    "# 这个定义了一个添加中间件的方法\n",
    "# 先调用基类的add方法， 然后判断判定是否有 process_spider_input 等等方法。 \n",
    "# 如果有的话， 把这个中间件的方法添加到对应的方法链上去。 \n",
    "# 这里有4个。 分别是。\n",
    "process_spider_input，\n",
    "process_spider_output\n",
    "process_spider_exception\n",
    "process_start_requests\n",
    "# 我们这里可以看出， 如果我们自己要写爬虫中间件， 重点是这4个方法的。 切记切记。 \n",
    "\n",
    "# scrape_response 这个方法太长了。内部也嵌套了几个方法， 我们还是先看看内部的小方法吧。 \n",
    "def process_spider_input(response):\n",
    "    for method in self.methods['process_spider_input']:\n",
    "        try:\n",
    "            result = method(response=response, spider=spider)\n",
    "            assert result is None, \\\n",
    "                    'Middleware %s must returns None or ' \\\n",
    "                    'raise an exception, got %s ' \\\n",
    "                    % (fname(method), type(result))\n",
    "        except:\n",
    "            return scrape_func(Failure(), request, spider)\n",
    "    return scrape_func(response, request, spider)\n",
    "# 处理爬虫的中间件个各个 process_spider_input 方法。\n",
    "\n",
    "def process_spider_exception(_failure):\n",
    "    exception = _failure.value\n",
    "    for method in self.methods['process_spider_exception']:\n",
    "        result = method(response=response, exception=exception, spider=spider)\n",
    "        assert result is None or _isiterable(result), \\\n",
    "            'Middleware %s must returns None, or an iterable object, got %s ' % \\\n",
    "            (fname(method), type(result))\n",
    "        if result is not None:\n",
    "            return result\n",
    "    return _failure \n",
    "# 处理爬虫中间件的各个process_spider_exception方法。结果必须是none或者可迭代的。 \n",
    "\n",
    "def process_spider_output(result):\n",
    "    for method in self.methods['process_spider_output']:\n",
    "        result = method(response=response, result=result, spider=spider)\n",
    "        assert _isiterable(result), \\\n",
    "            'Middleware %s must returns an iterable object, got %s ' % \\\n",
    "            (fname(method), type(result))\n",
    "    return result\n",
    "\n",
    "# 处理爬虫中间件的各个process_spider_exception方法。结果可迭代的。\n",
    "\n",
    "# 这个scrape_response 方法， fname是获取到类的名字，  方法的名字\n",
    "\n",
    "dfd = mustbe_deferred(process_spider_input, response)\n",
    "dfd.addErrback(process_spider_exception)\n",
    "dfd.addCallback(process_spider_output)\n",
    "\n",
    "# 这段代码， 创建延迟对象， 添加错误回调方法，添加成功回调方法。 \n",
    "\n",
    "\n",
    "def process_start_requests(self, start_requests, spider):\n",
    "    return self._process_chain('process_start_requests', start_requests, spider)\n",
    "# 这个方法， 就是处理开始请求的。调用了_process_chain处理链， 接受开始的请求和对应的爬虫。 具体还是需要去基类去看看这个方法的。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# middleware.py\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import pprint\n",
    "\n",
    "# 这几个都是引用默认字典， 日志， 打印的，没啥问题。 \n",
    "\n",
    "from scrapy.exceptions import NotConfigured\n",
    "from scrapy.utils.misc import load_object\n",
    "# 导入了notconfigure没有配置的异常， 导入了load_object去完成字符串到对应类对象的方法。前面已经提到了。 \n",
    "\n",
    "from scrapy.utils.defer import process_parallel, process_chain, process_chain_both\n",
    "# 这几个方法都在defer里面。 我们定位过去看看。\n",
    "def process_parallel(callbacks, input, *a, **kw):\n",
    "    \"\"\"Return a Deferred with the output of all successful calls to the given\n",
    "    callbacks\n",
    "    \"\"\"\n",
    "    dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n",
    "    d = defer.DeferredList(dfds, fireOnOneErrback=1, consumeErrors=1)\n",
    "    d.addCallbacks(lambda r: [x[1] for x in r], lambda f: f.value.subFailure)\n",
    "    return d\n",
    "# 这个方法完成的功能就是返回一个带有所有成功输出的defrred，通过给定的callback方法。\n",
    "\n",
    "# 并行处理，得到dfds， 添加一个成功回调。 一个错误回调。 \n",
    "def process_chain(callbacks, input, *a, **kw):\n",
    "    \"\"\"Return a Deferred built by chaining the given callbacks\"\"\"\n",
    "    d = defer.Deferred()\n",
    "    for x in callbacks:\n",
    "        d.addCallback(x, *a, **kw)\n",
    "    d.callback(input)\n",
    "    return d  # 这个方法将所有回调方法添加给deferred对象上， 然后给input\n",
    "\n",
    "def process_chain_both(callbacks, errbacks, input, *a, **kw):\n",
    "    \"\"\"Return a Deferred built by chaining the given callbacks and errbacks\"\"\"\n",
    "    d = defer.Deferred()\n",
    "    for cb, eb in zip(callbacks, errbacks):\n",
    "        d.addCallbacks(cb, eb, callbackArgs=a, callbackKeywords=kw,\n",
    "            errbackArgs=a, errbackKeywords=kw)\n",
    "    if isinstance(input, failure.Failure):\n",
    "        d.errback(input)\n",
    "    else:\n",
    "        d.callback(input)\n",
    "    return d\n",
    "# 这个是上面的升级版吧， 添加回调。 \n",
    "\n",
    "logger = logging.getLogger(__name__) 全局的一个日志对象。\n",
    "\n",
    "\n",
    "def __init__(self, *middlewares):\n",
    "    self.middlewares = middlewares\n",
    "    self.methods = defaultdict(list)\n",
    "    for mw in middlewares:\n",
    "        self._add_middleware(mw)\n",
    "# 构造函数， 接受中间件列表， 构造方法的默认dict ， 添加中间件。\n",
    "\n",
    "@classmethod\n",
    "def _get_mwlist_from_settings(cls, settings):\n",
    "    raise NotImplementedError\n",
    "# 这个方法什么鬼， 直接抛出异常？， 应该是写一个方法打个桩子吧， 以后可能后去完善它， 然后调用它的吧。 或者子类里面实现吧 。\n",
    "# 如果子类不实现就抛出异常， 感觉应该是第二种情况， 这个其实和c++的接口是一样的。 强制子类去实现指定的方法。 \n",
    "\n",
    "def from_settings(cls, settings, crawler=None):\n",
    "    mwlist = cls._get_mwlist_from_settings(settings)\n",
    "    middlewares = []\n",
    "    enabled = []\n",
    "    for clspath in mwlist:\n",
    "        try:\n",
    "            mwcls = load_object(clspath)\n",
    "            if crawler and hasattr(mwcls, 'from_crawler'):\n",
    "                mw = mwcls.from_crawler(crawler)\n",
    "            elif hasattr(mwcls, 'from_settings'):\n",
    "                mw = mwcls.from_settings(settings)\n",
    "            else:\n",
    "                mw = mwcls()\n",
    "            middlewares.append(mw)\n",
    "            enabled.append(clspath)\n",
    "        except NotConfigured as e:\n",
    "            if e.args:\n",
    "                clsname = clspath.split('.')[-1]\n",
    "                logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n",
    "                                {'clsname': clsname, 'eargs': e.args[0]},\n",
    "                                extra={'crawler': crawler})\n",
    "\n",
    "    logger.info(\"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n",
    "                {'componentname': cls.component_name,\n",
    "                    'enabledlist': pprint.pformat(enabled)},\n",
    "                extra={'crawler': crawler})\n",
    "    return cls(*middlewares)\n",
    "# 使用子类实现的方法_get_mwlist_from_settings 完成从settings里面获取中间件， 遍历中间件列表。 \n",
    "# 如果中间件有from_crawler,from settings 这些方法，就调用下，去构造一个中间件对象。\n",
    "# 添加到对应的中间件对象列表中去，这里mwlist只是中间件的类名字列表， middlearess存储的是中间件的对象。\n",
    "# enabled 启用的中间件类列表。如果有异常， 说明配置文件给定的中间件不存在或者没法实例化。 \n",
    "# 日志信息记录启动了那些中间件。返回中间件。\n",
    "@classmethod\n",
    "def from_crawler(cls, crawler):\n",
    "    return cls.from_settings(crawler.settings, crawler)\n",
    "# 调用对应的中间件方法from_settings 方法去完成类实例的创建\n",
    "\n",
    "\n",
    "def _add_middleware(self, mw):\n",
    "    if hasattr(mw, 'open_spider'):\n",
    "        self.methods['open_spider'].append(mw.open_spider)\n",
    "    if hasattr(mw, 'close_spider'):\n",
    "        self.methods['close_spider'].insert(0, mw.close_spider)\n",
    "# 添加中间件的房， 如果有open_spider，close_spider方法的话， 添加到对应方法去。 \n",
    "# 我们这里可以发现， open是append的close是insert 0位置。 \n",
    "# 也就是说， 如果一个中间件的open添加早那么他的close就后关闭的。\n",
    "\n",
    "def _process_parallel(self, methodname, obj, *args):\n",
    "    return process_parallel(self.methods[methodname], obj, *args)\n",
    "# 处理平行的， 这个方法不知道具体怎么并行的。 \n",
    "def _process_chain(self, methodname, obj, *args):\n",
    "    return process_chain(self.methods[methodname], obj, *args)\n",
    "    # 处理方法链\n",
    "def _process_chain_both(self, cb_methodname, eb_methodname, obj, *args):\n",
    "    return process_chain_both(self.methods[cb_methodname], \\\n",
    "        self.methods[eb_methodname], obj, *args)\n",
    "# 处理成功和错误两个链\n",
    "def open_spider(self, spider):\n",
    "    return self._process_parallel('open_spider', spider)\n",
    "\n",
    "def close_spider(self, spider):\n",
    "    return self._process_parallel('close_spider', spider)\n",
    "# 打开爬虫， 关闭爬虫， 都是并行处理的。 \n",
    "\n",
    "# 从这个文件可以看出来， 我们要自己写个中间件的话， 要实现open_spider,close_spider, from_crawler，from_setting这些方法。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
