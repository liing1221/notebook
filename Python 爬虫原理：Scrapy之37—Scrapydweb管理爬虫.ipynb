{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapydweb管理爬虫\n",
    "　　参考：[]()<br>\n",
    "　　　　　[gitee文档](https://gitee.com/mirrors/ScrapydWeb)<br>\n",
    "　　　　　[第八章 第二节 使用scrapydweb来管理scrapyd](https://zhuanlan.zhihu.com/p/99449687)<br>\n",
    "　　　　　[scrapyd 批量清除pending任务](https://blog.csdn.net/Refrain__WG/article/details/111830637)<br>\n",
    "　　　　　[scrapydweb的使用](https://blog.csdn.net/chang995196962/article/details/116199842)<br>\n",
    "　　　　　[爬虫的远程启停、监控](https://mp.weixin.qq.com/s/lrVEtPcqCUmC9kzFSH7VoQ)<br>\n",
    "　　　　　[]()<br>\n",
    "　　　　　[]()<br>\n",
    "## 一、Srapydweb简介\n",
    "　　ScrapydWeb 是一个用于 Scrapyd 集群管理的 web 应用，支持 Scrapy 日志分析和可视化。<br>\n",
    "　　**特性：**<br>\n",
    "　　1、Scrapyd 集群管理<br>\n",
    "　　　　scrapy项目管理与部署<br>\n",
    "　　　　支持所有 Scrapyd JSON API<br>\n",
    "　　　　支持通过分组和过滤来选择若干个节点<br>\n",
    "　　　　一次操作, 批量执行<br>\n",
    "　　2、Scrapy 日志分析<br>\n",
    "　　　　集成 LogParser<br>\n",
    "　　　　数据统计<br>\n",
    "　　　　进度可视化<br>\n",
    "　　　　日志分类<br>\n",
    "　　3、增强功能\n",
    "　　　　自动打包项目<br>\n",
    "　　　　定时任务<br>\n",
    "　　　　监控和警报<br>\n",
    "　　　　支持手机 UI<br>\n",
    "　　　　web UI 支持基本身份认证<br>\n",
    "　　4、框架依赖：<br>\n",
    "　　　　前端：Element、ECharts<br>\n",
    "　　　　后端：Flask<br>\n",
    "    \n",
    "## 二、安装和配置Scrapydweb\n",
    "### 2.1、安装配置 Scrapyd\n",
    ">pip install scrapyd<br>\n",
    "pip install scrapyd-client<br>\n",
    "### 2.2、安装配置 Scrapydweb\n",
    ">pip install scrapydweb\n",
    "### 2.3、安装 logparser\n",
    "　　logparser是一个日志解析工具, 可以从scrapyd的日志中解析并且发送给scrapydweb.<br>\n",
    ">pip install logparser\n",
    "### 2.4、配置scrapydweb\n",
    "　　在项目的根目录下, 创建一个scrapydweb文件夹<br>\n",
    ">cd scrapydweb<br>\n",
    "scrapydweb<br>\n",
    "\n",
    "　　配置scrapyd服务器节点\n",
    "　　运行scrapydweb会自动生成scrapydweb_settings_v10.py的文件，打开文件, 并修改\n",
    ">SCRAPYD_SERVERS = [　　# 支持字符串和元组两种配置格式，支持添加认证信息和分组/标签<br>\n",
    "　　    '127.0.0.1:6800',<br>\n",
    "　　    # 'username:password@localhost:6801#group',<br>\n",
    "　　    # ('username', 'password', 'localhost', '6801', 'group'),<br>\n",
    "]<br>\n",
    "\n",
    "　　有三种方式:<br>\n",
    "1、127.0.0.1:6800, 直接指定url:port<br>\n",
    "2、username:password@localhost:6801#group, group是一个组名, 也就是可以把scrapyd自动划分成组<br>\n",
    "3、('username', 'password', 'localhost', '6801', 'group)<br>\n",
    "\n",
    "　　配置日志解析<br>\n",
    "有两种方案<br>\n",
    "一、scrapydweb和scrapyd在同一台服务器<br>\n",
    ">LOCAL_SCRAPYD_LOGS_DIR = '' # 指定scrapyd的logs文件路径<br>\n",
    "ENABLE_LOGPARSER = False # 不自动启用logparser<br>\n",
    "\n",
    "二、scrapydweb和scrapyd不在同一个服务器<br>\n",
    "　　在scrapyd服务器上安装logparser, 然后运行<br>\n",
    ">logparser -dir scrapyd的日志目录<br>\n",
    "\n",
    "　　安装完成以后, 通过http://127.0.0.1:6800/logs/stats.json就可以看到logparser解析出来的日志了<br>\n",
    "\n",
    "三、运行scrapydweb<br>\n",
    ">scrapydweb<br>\n",
    "\n",
    "　　启动后,访问 http://127.0.0.1:5000 就可以看到scrapydweb的界面了。<br>\n",
    "\n",
    "## 三、scrapydweb的使用\n",
    "### 3.1、部署爬虫\n",
    "　　打开配置文件<br>\n",
    ">SCRAPY_PROJECTS_DIR=爬虫项目的上一级目录, 也就是`scrapy.cfg`文件所在的上一级目录<br>\n",
    "\n",
    "　　然后点击scrapydweb页面左侧Deploy Project按钮，就可一键部署上去。<br>\n",
    "\n",
    "### 3.2、运行爬虫\n",
    "　　点击左侧Run Spider 然后选择要运行的服务器、项目、版本、爬虫, 以及要覆盖的配置项、传给爬虫的参数，点击Check CMD按钮生成命令，然后点击Run Spider就可以手动运行爬虫了。<br>\n",
    "　　timer task是添加一个定时任务.<br>\n",
    "\n",
    "### 3.3、查看爬虫运行情况以及日志\n",
    "　　1、点击左侧的Job按钮, 就可以看到我们所有运行过的任务。<br>\n",
    "　　2、点击蓝色的Stats即可查看爬虫运行产生的日志信息。<br>\n",
    "　　3、点击绿色的Start的按钮，可以再次运行爬虫。<br>\n",
    "\n",
    "### 3.4、创建定时任务爬虫\n",
    "　　1、在运行爬虫的时候可以配置定时任务爬虫。<br>\n",
    "　　2、点击Timer Tasks也可以创建定时任务。<br>\n",
    "　　3、点击+号, 配置界面和运行爬虫是一样的。<br>\n",
    "　　编辑完成以后, 就可以在列表看到我们配置的定时任务爬虫了<br>\n",
    "\n",
    "### 3.5、邮件通知\n",
    "查看https://github.com/my8100/files/blob/master/scrapydweb/README_CN.md<br>\n",
    "\n",
    "### 3.6、开启scrapydweb安全认证\n",
    "　　在配置文件scrapydweb_settings_v10.py中<br>\n",
    "># The default is False, set it to True to enable basic auth for the web UI.<br>\n",
    "ENABLE_AUTH = True # False是禁用<br>\n",
    "\\# In order to enable basic auth, both USERNAME and PASSWORD should be non-empty strings.<br>\n",
    "USERNAME = '123' # 用户名<br>\n",
    "PASSWORD = '123' # 密码<br>\n",
    "\n",
    "　　开启以后重启服务, 刷新就可以看到需要输入用户名和密码了<br>\n",
    "\n",
    "### 3.7、开启https\n",
    "　　在配置文件scrapydweb_settings_v10.py中<br>\n",
    ">ENABLE_HTTPS = True # True是开启, False关闭<br>\n",
    "\\# e.g. '/home/username/cert.pem'<br>\n",
    "CERTIFICATE_FILEPATH = ''<br>\n",
    "\\# e.g. '/home/username/cert.key'<br>\n",
    "PRIVATEKEY_FILEPATH = ''<br>\n",
    "\n",
    "### 3.8、运行爬虫的默认设置\n",
    "　　在配置文件scrapydweb_settings_v10.py中<br>\n",
    ">SCHEDULE_EXPAND_SETTINGS_ARGUMENTS = False # Run Spider页面是否自动展开settings & arguments选项卡<br>\n",
    "SCHEDULE_CUSTOM_USER_AGENT = 'Mozilla/5.0' # 调度爬虫时默认的UA<br>\n",
    "SCHEDULE_USER_AGENT = ['custom', 'Chrome', 'iPhone', 'iPad', 'Android']  # 可选择的UA 列表<br>\n",
    "SCHEDULE_ROBOTSTXT_OBEY = None # 是否开启检测robots.txt文件<br>\n",
    "SCHEDULE_COOKIES_ENABLED = None # 是否开启cookie<br>\n",
    "SCHEDULE_CONCURRENT_REQUESTS = None # 并发请求数<br>\n",
    "SCHEDULE_DOWNLOAD_DELAY = None # 下载延迟时间<br>\n",
    "SCHEDULE_ADDITIONAL = \"-d setting=CLOSESPIDER_TIMEOUT=60\\r\\n-d setting=CLOSESPIDER_PAGECOUNT=10\\r\\n-d arg1=val1\" # Run Spider界面自动附加参数<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
