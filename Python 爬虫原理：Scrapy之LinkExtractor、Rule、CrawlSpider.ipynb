{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy之LinkExtractor、Rule、CrawlSpider详解\n",
    "　　参考：[]()<br>\n",
    "  \n",
    "### 一、LinkExtractor\n",
    "LxmlLinkExtractor 是一种强大的链接提取器，使用他能很方便的进行从网页(scrapy.http.Response )中抽取会被follow的链接的对象，他是通过xml中强大的HTMLParser实现的。\n",
    "每个LinkExtractor有唯一的公共方法是 extract_links()，它接收一个 Response 对象，并返回一个 scrapy.link.Link 对象。\n",
    "Link Extractors要实例化一次，并且 extract_links 方法会根据不同的 response 调用多次提取链接｡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .lxmlhtml import LxmlLinkExtractor as LinkExtractor\n",
    "\n",
    "class LxmlLinkExtractor(FilteringLinkExtractor):\n",
    "\n",
    "    def __init__(self, allow=(), deny=(), allow_domains=(), deny_domains=(), restrict_xpaths=(),\n",
    "                 tags=('a', 'area'), attrs=('href',), canonicalize=False,\n",
    "                 unique=True, process_value=None, deny_extensions=None, restrict_css=(),\n",
    "                 strip=True, restrict_text=None):\n",
    "        tags, attrs = set(arg_to_iter(tags)), set(arg_to_iter(attrs))\n",
    "        tag_func = lambda x: x in tags\n",
    "        attr_func = lambda x: x in attrs\n",
    "        lx = LxmlParserLinkExtractor(\n",
    "            tag=tag_func,\n",
    "            attr=attr_func,\n",
    "            unique=unique,\n",
    "            process=process_value,\n",
    "            strip=strip,\n",
    "            canonicalized=canonicalize\n",
    "        )\n",
    "\n",
    "        super(LxmlLinkExtractor, self).__init__(lx, allow=allow, deny=deny,\n",
    "                                                allow_domains=allow_domains, deny_domains=deny_domains,\n",
    "                                                restrict_xpaths=restrict_xpaths, restrict_css=restrict_css,\n",
    "                                                canonicalize=canonicalize, deny_extensions=deny_extensions,\n",
    "                                                restrict_text=restrict_text)\n",
    "\n",
    "    def extract_links(self, response):\n",
    "        base_url = get_base_url(response)\n",
    "        if self.restrict_xpaths:\n",
    "            docs = [subdoc\n",
    "                    for x in self.restrict_xpaths\n",
    "                    for subdoc in response.xpath(x)]\n",
    "        else:\n",
    "            docs = [response.selector]\n",
    "        all_links = []\n",
    "        for doc in docs:\n",
    "            links = self._extract_links(doc, response.url, response.encoding, base_url)\n",
    "            all_links.extend(self._process_links(links))\n",
    "        return unique_list(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数说明：\n",
    ">allow=(一个正则表达式或者正则表达式的列表) 只有与之相匹配的url才能被提取出来\n",
    "deny=(一个正则表达式或者正则表达式的列表) 一个正则表达式（或正则表达式列表），（绝对）urls必须匹配才能排除（即不提取）。它优先于allow参数。如果没有给出（或为空），它不会排除任何链接。\n",
    "allow_domains=(str或者list) 允许提取链接的域名的字符串列表或者单个字符串，例如：allow_domain = \\['baidu.com'\\]则只能提取baidu.com的域名内的链接\n",
    "deny_domains=() 与上述的意思刚刚相反\n",
    "restrict_xpaths=(str或list) - 是一个XPath（或XPath的列表），它定义响应中应从中提取链接的区域。如果给出，只有那些XPath选择的文本将被扫描链接。\n",
    "targs=('a','area') 标签或在提取链接时要考虑的标签列表。默认为。('a', 'area') 也就是默认只有a标签与area标签的链接才能被提取\n",
    "attrs=('href',) 在查找要提取的链接时应该考虑的属性或属性列表（仅适用于参数中指定的那些标签tags ）。默认为('href',)\n",
    "cononicalize=(boolean) 规范化每个提取的url（使用w3lib.url.canonicalize_url）。默认为True。\n",
    "unique=(boolean) 是否应对提取的链接应用重复过滤。\n",
    "process_value=(callable) 接收从标签提取的每个值和扫描的属性并且可以修改值并返回新值的函数，或者返回None以完全忽略链接。如果没有给出，那么process_value默认为:lambda x:x 。用法示例如下：\n",
    "要从此代码中提取链接：\\<a href=\"javascript\\:goToPage('../other/page.html'); return false\">Link text</a>您可以使用以下功能process_value：\n",
    "def process_value(value):\n",
    "    m = re.search(\"javascript\\:goToPage\\('(.*?)'\", value)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "deny_extensions=(list) -包含在提取链接时应该忽略的扩展的单个值或字符串列表。如果没有给出，它将默认为IGNORED_EXTENSIONS在scrapy.linkextractors包中定义的 列表 。\n",
    "restrict_css=() 一个CSS选择器（或选择器列表），用于定义响应中应提取链接的区域。有相同的行为restrict_xpaths。\n",
    "strip=True 这个是把地址前后多余的空格删除，很有必要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、Rule\n",
    "在rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作定义了特定操作。如果多个rule匹配了相同的链接，则根据规则在本集合中被定义的顺序，第一个会被使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _identity(request, response):\n",
    "    return request\n",
    "def _get_method(method, spider):\n",
    "    if callable(method):\n",
    "        return method\n",
    "    elif isinstance(method, six.string_types):\n",
    "        return getattr(spider, method, None)\n",
    "\n",
    "class Rule(object):\n",
    "\n",
    "    def __init__(self, link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None):\n",
    "        self.link_extractor = link_extractor or _default_link_extractor\n",
    "        self.callback = callback\n",
    "        self.cb_kwargs = cb_kwargs or {}\n",
    "        self.process_links = process_links\n",
    "        self.process_request = process_request or _identity\n",
    "        self.process_request_argcount = None\n",
    "        self.follow = follow if follow is not None else not callback\n",
    "\n",
    "    def _compile(self, spider):\n",
    "        self.callback = _get_method(self.callback, spider)\n",
    "        self.process_links = _get_method(self.process_links, spider)\n",
    "        self.process_request = _get_method(self.process_request, spider)\n",
    "        self.process_request_argcount = len(get_func_args(self.process_request))\n",
    "        if self.process_request_argcount == 1:\n",
    "            msg = 'Rule.process_request should accept two arguments (request, response), accepting only one is deprecated'\n",
    "            warnings.warn(msg, category=ScrapyDeprecationWarning, stacklevel=2)\n",
    "\n",
    "    def _process_request(self, request, response):\n",
    "        \"\"\"\n",
    "        Wrapper around the request processing function to maintain backward\n",
    "        compatibility with functions that do not take a Response object\n",
    "        \"\"\"\n",
    "        args = [request] if self.process_request_argcount == 1 else [request, response]\n",
    "        return self.process_request(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数说明：\n",
    ">link_extractor：是一个Link Extractor对象。其定义了如何从爬取到的页面提取链接。\n",
    "callback：是一个callable或string（该Spider中同名的函数将会被调用）。从link_extractor中每获取到链接时将会调用该函数。该回调函数接收一个response作为其第一个参数，并返回一个包含Item以及Request对象(或者这两者的子类)的列表。\n",
    "cb_kwargs：包含传递给回调函数的参数（keyword argument）的字典。\n",
    "follow：是一个boolean值，指定了根据该规则从response提取的链接是否需要跟进。如果callback为None，follow默认设置True，否则默认False。\n",
    "当follow为True时，爬虫会从获取的response中取出符合规则的url，再次进行爬取，如果这次爬取的response中还存在符合规则的url，则再次爬取，无限循环，直到不存在符合规则的url。当follow为False是，爬虫只从start_urls 的response中取出符合规则的url，并请求.\n",
    "process_links：是一个callable或string（该Spider中同名的函数将会被调用）。从link_extrator中获取到链接列表时将会调用该函数。该方法主要是用来过滤。\n",
    "process_request：是一个callable或string（该spider中同名的函数都将会被调用）。该规则提取到的每个request时都会调用该函数。该函数必须返回一个request或者None。用来过滤request。\n",
    "\n",
    "注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、CrawlSpider\n",
    "通过下面的命令可以快速创建 CrawlSpider模板 的代码：\n",
    ">scrapy genspider -t crawl tencent tencent.com\n",
    "\n",
    "class scrapy.spiders.CrawlSpider\n",
    "它是Spider的派生类，Spider类的设计原则是只爬取start_url列表中的网页，而CrawlSpider类定义了一些规则(rule)来提供跟进link的方便的抓取机制，从爬取的网页中获取link并继续爬取的应用更适合使用该模板。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrawlSpider(Spider):\n",
    "    rules = ()\n",
    "    def __init__(self, *a, **kw):\n",
    "        super(CrawlSpider, self).__init__(*a, **kw)\n",
    "        self._compile_rules()\n",
    " \n",
    "    #首先调用parse()来处理start_urls中返回的response对象\n",
    "    #parse()则将这些response对象传递给了_parse_response()函数处理，并设置回调函数为parse_start_url()\n",
    "    #设置了跟进标志位follow = True\n",
    "    #parse将返回item和跟进了的Request对象    \n",
    "    def parse(self, response):\n",
    "        return self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)\n",
    " \n",
    "    #处理start_url中返回的response，需要重写\n",
    "    def parse_start_url(self, response):\n",
    "        return []\n",
    " \n",
    "    def process_results(self, response, results):\n",
    "        return results\n",
    " \n",
    "    #从response中抽取符合任一用户定义'规则'的链接，并构造成Resquest对象返回\n",
    "    def _requests_to_follow(self, response):\n",
    "        if not isinstance(response, HtmlResponse):\n",
    "            return\n",
    "        seen = set()\n",
    "        #抽取之内的所有链接，只要通过任意一个'规则'，即表示合法\n",
    "        for n, rule in enumerate(self._rules):\n",
    "            links = [l for l in rule.link_extractor.extract_links(response) if l not in seen]\n",
    "            #使用用户指定的process_links处理每个连接\n",
    "            if links and rule.process_links:\n",
    "                links = rule.process_links(links)\n",
    "            #将链接加入seen集合，为每个链接生成Request对象，并设置回调函数为_repsonse_downloaded()\n",
    "            for link in links:\n",
    "                seen.add(link)\n",
    "                #构造Request对象，并将Rule规则中定义的回调函数作为这个Request对象的回调函数\n",
    "                r = Request(url=link.url, callback=self._response_downloaded)\n",
    "                r.meta.update(rule=n, link_text=link.text)\n",
    "               #对每个Request调用process_request()函数。\n",
    "               #该函数默认为indentify，即不做任何处理，直接返回该Request.\n",
    "                yield rule.process_request(r)\n",
    " \n",
    " \n",
    "    #处理通过rule提取出的连接，并返回item以及request\n",
    "    def _response_downloaded(self, response):\n",
    "        rule = self._rules[response.meta['rule']]\n",
    "        return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)\n",
    " \n",
    " \n",
    "    #解析response对象，会用callback解析处理他，并返回request或Item对象\n",
    "    def _parse_response(self, response, callback, cb_kwargs, follow=True):\n",
    "        #首先判断是否设置了回调函数 (该回调函数可能是rule中的解析函数，也可能是 parse_start_url函数）\n",
    "        #如果设置了回调函数（parse_start_url()），那么首先用parse_start_url()处理response对象，\n",
    "        #然后再交给process_results处理。返回cb_res的一个列表\n",
    "        if callback:\n",
    "            #如果是parse调用的，则会解析成Request对象\n",
    "            #如果是rule callback，则会解析成Item\n",
    "            cb_res = callback(response, **cb_kwargs) or ()\n",
    "            cb_res = self.process_results(response, cb_res)\n",
    "            for requests_or_item in iterate_spider_output(cb_res):\n",
    "                yield requests_or_item\n",
    " \n",
    "        #如果需要跟进，那么使用定义的Rule规则提取并返回这些Request对象\n",
    "        if follow and self._follow_links:\n",
    "            #返回每个Request对象\n",
    "            for request_or_item in self._requests_to_follow(response):\n",
    "                yield request_or_item\n",
    " \n",
    "    def _compile_rules(self):\n",
    "        def get_method(method):\n",
    "            if callable(method):\n",
    "                return method\n",
    "            elif isinstance(method, basestring):\n",
    "                return getattr(self, method, None)\n",
    " \n",
    " \n",
    "        self._rules = [copy.copy(r) for r in self.rules]\n",
    "        for rule in self._rules:\n",
    "            rule.callback = get_method(rule.callback)\n",
    "            rule.process_links = get_method(rule.process_links)\n",
    "            rule.process_request = get_method(rule.process_request)\n",
    " \n",
    " \n",
    "    def set_crawler(self, crawler):\n",
    "        super(CrawlSpider, self).set_crawler(crawler)\n",
    "        self._follow_links = crawler.settings.getbool('CRAWLSPIDER_FOLLOW_LINKS', True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
