{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scraper抓取器详解\n",
    "　　参考：[scrapy启动流程源码分析(5)Scraper刮取器](https://blog.csdn.net/csdn_yym/article/details/85577613)<br>\n",
    "　　　　　[scrapy源码3：scraper的源码分析](https://cuiyonghua.blog.csdn.net/article/details/108315506)<br>\n",
    "     \n",
    "## 一、简介\n",
    "　　本节对ExecutionEngine执行引擎篇出现的Scraper进行展开。<br>\n",
    "　　scraper模块是实现爬虫组件去解析响应流并且提取数据的。具体讲就是对spider中间件进行管理，通过中间件完成请求、响应、数据分析等工作。 \n",
    "## 二、Scraper对象\n",
    "　　Scraper对象，主要是通过3个对象：spidermw，itemproc，concurrent_items 完成请求处理、响应处理、数据处理工作的。<br>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  scrapy/core/scraper.py\n",
    "class Scraper(object):\n",
    "    def __init__(self, crawler):\n",
    "        self.slot = None\n",
    "        self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n",
    "        itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n",
    "        self.itemproc = itemproc_cls.from_crawler(crawler)\n",
    "        self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n",
    "        self.crawler = crawler\n",
    "        self.signals = crawler.signals\n",
    "        self.logformatter = crawler.logformatter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1、spidermw对象\n",
    "　　SpiderMiddlewareManger爬虫中间件管理器。<br>\n",
    "　　self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)，同样通过from_crawler方法生成爬虫中间件管理器。\n",
    "这个from_cralwer方法是基类MiddlewareManger（scrapy/middleware.py ->MiddlewareManager）的方法。<br>\n",
    "　　1、首先调用_get_mwlist_from_settings方法从配置文件中生成中间件列表。<br>\n",
    "　　2、然后依次加载中间件模块并构造对象，构造顺序是先尝试调用from_cralwer,再尝试调用from_settings,最后都没有再调用init（从这里可以看出中间件的加载逻辑，from_cralwer优先于init，中间件直接在配置文件中注册名称使用，不必继承基类）。<br>\n",
    "　　3、中间件除了类路径，还有一个优先级，这个决定了后面调用的先后顺序，数字越小调用越靠前。<br>\n",
    "　　默认中间件：<br>\n",
    ">SPIDER_MIDDLEWARES_BASE = {<br>\n",
    "　　\\# Engine side<br>\n",
    "　　'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware': 50,<br>\n",
    "　　'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': 500,<br>\n",
    "　　'scrapy.spidermiddlewares.referer.RefererMiddleware': 700,<br>\n",
    "　　'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware': 800,<br>\n",
    "　　'scrapy.spidermiddlewares.depth.DepthMiddleware': 900,<br>\n",
    "　　\\# Spider side<br>\n",
    "}<br>\n",
    "\n",
    "　　4、spider中间件默认能够识别的所有信号相关处理函数为：open_spider，close_spider，process_spider_input，process_spider_output，process_spider_exception，process_start_requests。<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy/core/spidermw.py\n",
    "\"\"\"\n",
    "Spider Middleware manager\n",
    "\n",
    "See documentation in docs/topics/spider-middleware.rst\n",
    "\"\"\"\n",
    "from itertools import chain, islice\n",
    "\n",
    "import six\n",
    "from twisted.python.failure import Failure\n",
    "from scrapy.exceptions import _InvalidOutput\n",
    "from scrapy.middleware import MiddlewareManager\n",
    "from scrapy.utils.defer import mustbe_deferred\n",
    "from scrapy.utils.conf import build_component_list\n",
    "from scrapy.utils.python import MutableChain\n",
    "\n",
    "\n",
    "def _isiterable(possible_iterator):\n",
    "    return hasattr(possible_iterator, '__iter__')\n",
    "\n",
    "\n",
    "class SpiderMiddlewareManager(MiddlewareManager):\n",
    "\n",
    "    component_name = 'spider middleware'\n",
    "\n",
    "    @classmethod\n",
    "    def _get_mwlist_from_settings(cls, settings):\n",
    "        return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))\n",
    "\n",
    "    def _add_middleware(self, mw):\n",
    "        super(SpiderMiddlewareManager, self)._add_middleware(mw)\n",
    "        if hasattr(mw, 'process_spider_input'):\n",
    "            self.methods['process_spider_input'].append(mw.process_spider_input)\n",
    "        if hasattr(mw, 'process_start_requests'):\n",
    "            self.methods['process_start_requests'].appendleft(mw.process_start_requests)\n",
    "        self.methods['process_spider_output'].appendleft(getattr(mw, 'process_spider_output', None))\n",
    "        self.methods['process_spider_exception'].appendleft(getattr(mw, 'process_spider_exception', None))\n",
    "\n",
    "    def scrape_response(self, scrape_func, response, request, spider):\n",
    "        fname = lambda f:'%s.%s' % (\n",
    "                six.get_method_self(f).__class__.__name__,\n",
    "                six.get_method_function(f).__name__)\n",
    "\n",
    "        def process_spider_input(response):\n",
    "            for method in self.methods['process_spider_input']:\n",
    "                try:\n",
    "                    result = method(response=response, spider=spider)\n",
    "                    if result is not None:\n",
    "                        raise _InvalidOutput('Middleware {} must return None or raise an exception, got {}' \\\n",
    "                                             .format(fname(method), type(result)))\n",
    "                except _InvalidOutput:\n",
    "                    raise\n",
    "                except Exception:\n",
    "                    return scrape_func(Failure(), request, spider)\n",
    "            return scrape_func(response, request, spider)\n",
    "\n",
    "        def process_spider_exception(_failure, start_index=0):\n",
    "            exception = _failure.value\n",
    "            # don't handle _InvalidOutput exception\n",
    "            if isinstance(exception, _InvalidOutput):\n",
    "                return _failure\n",
    "            method_list = islice(self.methods['process_spider_exception'], start_index, None)\n",
    "            for method_index, method in enumerate(method_list, start=start_index):\n",
    "                if method is None:\n",
    "                    continue\n",
    "                result = method(response=response, exception=exception, spider=spider)\n",
    "                if _isiterable(result):\n",
    "                    # stop exception handling by handing control over to the\n",
    "                    # process_spider_output chain if an iterable has been returned\n",
    "                    return process_spider_output(result, method_index+1)\n",
    "                elif result is None:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise _InvalidOutput('Middleware {} must return None or an iterable, got {}' \\\n",
    "                                         .format(fname(method), type(result)))\n",
    "            return _failure\n",
    "\n",
    "        def process_spider_output(result, start_index=0):\n",
    "            # items in this iterable do not need to go through the process_spider_output\n",
    "            # chain, they went through it already from the process_spider_exception method\n",
    "            recovered = MutableChain()\n",
    "\n",
    "            def evaluate_iterable(iterable, index):\n",
    "                try:\n",
    "                    for r in iterable:\n",
    "                        yield r\n",
    "                except Exception as ex:\n",
    "                    exception_result = process_spider_exception(Failure(ex), index+1)\n",
    "                    if isinstance(exception_result, Failure):\n",
    "                        raise\n",
    "                    recovered.extend(exception_result)\n",
    "\n",
    "            method_list = islice(self.methods['process_spider_output'], start_index, None)\n",
    "            for method_index, method in enumerate(method_list, start=start_index):\n",
    "                if method is None:\n",
    "                    continue\n",
    "                # the following might fail directly if the output value is not a generator\n",
    "                try:\n",
    "                    result = method(response=response, result=result, spider=spider)\n",
    "                except Exception as ex:\n",
    "                    exception_result = process_spider_exception(Failure(ex), method_index+1)\n",
    "                    if isinstance(exception_result, Failure):\n",
    "                        raise\n",
    "                    return exception_result\n",
    "                if _isiterable(result):\n",
    "                    result = evaluate_iterable(result, method_index)\n",
    "                else:\n",
    "                    raise _InvalidOutput('Middleware {} must return an iterable, got {}' \\\n",
    "                                         .format(fname(method), type(result)))\n",
    "\n",
    "            return chain(result, recovered)\n",
    "\n",
    "        dfd = mustbe_deferred(process_spider_input, response)\n",
    "        dfd.addCallbacks(callback=process_spider_output, errback=process_spider_exception)\n",
    "        return dfd\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        return self._process_chain('process_start_requests', start_requests, spider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy/middleware.py\n",
    "from collections import defaultdict, deque\n",
    "import logging\n",
    "import pprint\n",
    "\n",
    "from scrapy.exceptions import NotConfigured\n",
    "from scrapy.utils.misc import create_instance, load_object\n",
    "from scrapy.utils.defer import process_parallel, process_chain, process_chain_both\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MiddlewareManager(object):\n",
    "    \"\"\"Base class for implementing middleware managers\"\"\"\n",
    "\n",
    "    component_name = 'foo middleware'\n",
    "\n",
    "    def __init__(self, *middlewares):\n",
    "        self.middlewares = middlewares\n",
    "        self.methods = defaultdict(deque)\n",
    "        for mw in middlewares:\n",
    "            self._add_middleware(mw)\n",
    "\n",
    "    @classmethod\n",
    "    def _get_mwlist_from_settings(cls, settings):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @classmethod\n",
    "    def from_settings(cls, settings, crawler=None):\n",
    "        mwlist = cls._get_mwlist_from_settings(settings)   # 调用_get_mwlist_from_settings方法从配置文件中生成中间件列表。\n",
    "        middlewares = []\n",
    "        enabled = []\n",
    "        for clspath in mwlist:\n",
    "            try:\n",
    "                mwcls = load_object(clspath)\n",
    "                mw = create_instance(mwcls, settings, crawler)\n",
    "                middlewares.append(mw)\n",
    "                enabled.append(clspath)\n",
    "            except NotConfigured as e:\n",
    "                if e.args:\n",
    "                    clsname = clspath.split('.')[-1]\n",
    "                    logger.warning(\"Disabled %(clsname)s: %(eargs)s\",\n",
    "                                   {'clsname': clsname, 'eargs': e.args[0]},\n",
    "                                   extra={'crawler': crawler})\n",
    "\n",
    "        logger.info(\"Enabled %(componentname)ss:\\n%(enabledlist)s\",\n",
    "                    {'componentname': cls.component_name,\n",
    "                     'enabledlist': pprint.pformat(enabled)},\n",
    "                    extra={'crawler': crawler})\n",
    "        return cls(*middlewares)\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls.from_settings(crawler.settings, crawler)\n",
    "\n",
    "    def _add_middleware(self, mw):\n",
    "        if hasattr(mw, 'open_spider'):\n",
    "            self.methods['open_spider'].append(mw.open_spider)\n",
    "        if hasattr(mw, 'close_spider'):\n",
    "            self.methods['close_spider'].appendleft(mw.close_spider)\n",
    "\n",
    "    def _process_parallel(self, methodname, obj, *args):\n",
    "        return process_parallel(self.methods[methodname], obj, *args)\n",
    "\n",
    "    def _process_chain(self, methodname, obj, *args):\n",
    "        return process_chain(self.methods[methodname], obj, *args)\n",
    "\n",
    "    def _process_chain_both(self, cb_methodname, eb_methodname, obj, *args):\n",
    "        return process_chain_both(self.methods[cb_methodname], \\\n",
    "            self.methods[eb_methodname], obj, *args)\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        return self._process_parallel('open_spider', spider)\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        return self._process_parallel('close_spider', spider)\n",
    "\n",
    "\n",
    "#  scrapy/utils/misc.py  -> create_instance    \n",
    "def create_instance(objcls, settings, crawler, *args, **kwargs):\n",
    "    \"\"\"Construct a class instance using its ``from_crawler`` or\n",
    "    ``from_settings`` constructors, if available.\n",
    "\n",
    "    At least one of ``settings`` and ``crawler`` needs to be different from\n",
    "    ``None``. If ``settings `` is ``None``, ``crawler.settings`` will be used.\n",
    "    If ``crawler`` is ``None``, only the ``from_settings`` constructor will be\n",
    "    tried.\n",
    "\n",
    "    ``*args`` and ``**kwargs`` are forwarded to the constructors.\n",
    "\n",
    "    Raises ``ValueError`` if both ``settings`` and ``crawler`` are ``None``.\n",
    "    \"\"\"\n",
    "    if settings is None:\n",
    "        if crawler is None:\n",
    "            raise ValueError(\"Specifiy at least one of settings and crawler.\")\n",
    "        settings = crawler.settings\n",
    "    if crawler and hasattr(objcls, 'from_crawler'):\n",
    "        return objcls.from_crawler(crawler, *args, **kwargs)\n",
    "    elif hasattr(objcls, 'from_settings'):\n",
    "        return objcls.from_settings(settings, *args, **kwargs)\n",
    "    else:\n",
    "        return objcls(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2、itemproc对象\n",
    "　　itemproc：ItemPipelineManager项目管道管理器。<br>\n",
    ">itemproc_cls = load_object(crawler.settings[‘ITEM_PROCESSOR’])<br>\n",
    "self.itemproc = itemproc_cls.from_crawler(crawler)<br>\n",
    "\n",
    "　　itemproc从配置文件中获取‘ITEM_PROCESSOR’，默认为：<br>\n",
    ">ITEM_PROCESSOR = ‘scrapy.pipelines.ItemPipelineManager’<br>\n",
    "\n",
    "　　同样通过from_crawler方法生成项目管道管理器，且ItemPipelineManager同样继承于MiddlewareManager，也属于中间件。<br>\n",
    "　　默认项目管道中间件为空，ITEM_PIPELINES_BASE = {}。<br>\n",
    "　　项目管道能够识别的信号处理函数：open_spider，close_spider，process_item。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy/pipelines/init.py\n",
    "\"\"\"\n",
    "Item pipeline\n",
    "\n",
    "See documentation in docs/item-pipeline.rst\n",
    "\"\"\"\n",
    "\n",
    "from scrapy.middleware import MiddlewareManager\n",
    "from scrapy.utils.conf import build_component_list\n",
    "\n",
    "class ItemPipelineManager(MiddlewareManager):\n",
    "\n",
    "    component_name = 'item pipeline'\n",
    "\n",
    "    @classmethod\n",
    "    def _get_mwlist_from_settings(cls, settings):\n",
    "        return build_component_list(settings.getwithbase('ITEM_PIPELINES'))\n",
    "\n",
    "    def _add_middleware(self, pipe):\n",
    "        super(ItemPipelineManager, self)._add_middleware(pipe)\n",
    "        if hasattr(pipe, 'process_item'):\n",
    "            self.methods['process_item'].append(pipe.process_item)\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        return self._process_chain('process_item', item, spider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3、concurrent_items 数据并发处理控制\n",
    ">self.concurrent_items = crawler.settings.getint(‘CONCURRENT_ITEMS’)，默认配置值为100。\n",
    "\n",
    "　　这个并发数用于控制同时处理的抓取到的 item 数目，通过twisted.internet的task.Cooperator实现并发控制。\n",
    "　　scraper在处理spider的parse结果后会调用handle_spider_output来处理输出，通过parallel来控制同时处理的条目。\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy/core/scraper.py \n",
    "\n",
    "    def handle_spider_output(self, result, request, response, spider):\n",
    "        if not result:\n",
    "            return defer_succeed(None)\n",
    "        it = iter_errback(result, self.handle_spider_error, request, response, spider)\n",
    "        dfd = parallel(it, self.concurrent_items,\n",
    "            self._process_spidermw_output, request, response, spider)\n",
    "        return dfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy.core.scraper.py\n",
    "import logging\n",
    "from collections import deque\n",
    "\n",
    "from twisted.python.failure import Failure\n",
    "from twisted.internet import defer      # 导入Failure和defer延迟。\n",
    "\n",
    "from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n",
    "from scrapy.utils.spider import iterate_spider_output\n",
    "from scrapy.utils.misc import load_object   # 将导入str转为对象的。\n",
    "from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n",
    "from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest   # item被丢弃的异常、关闭爬虫的异常、忽略请求的异常。\n",
    "from scrapy import signals\n",
    "from scrapy.http import Request, Response\n",
    "from scrapy.item import BaseItem\n",
    "from scrapy.core.spidermw import SpiderMiddlewareManager\n",
    "from scrapy.utils.request import referer_str\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# 下面先分析一下导入的方法意义。\n",
    "def defer_result(result):\n",
    "    \"\"\"\n",
    "    对result进行判定， 如果是Deferred对象，那就返回 result;\n",
    "    如果是Failure,返回defer_fail(result);\n",
    "    其他的情况,返回defer_succeed(result)\n",
    "    \"\"\"\n",
    "    if isinstance(result, defer.Deferred):\n",
    "        return result\n",
    "    elif isinstance(result, failure.Failure):\n",
    "        return defer_fail(result)\n",
    "    else:\n",
    "        return defer_succeed(result)\n",
    "    \n",
    "# 再看看defer_fail，defer_succeed 都做了什么 \n",
    "def defer_fail(_failure):\n",
    "    \"\"\"Same as twisted.internet.defer.fail but delay calling errback until\n",
    "    next reactor loop\n",
    "    It delays by 100ms so reactor has a chance to go trough readers and writers\n",
    "    before attending pending delayed calls, so do not set delay to zero.\n",
    "    与twisted.internet.defer.fail相同，但将调用errback延迟到下一个reactor循环\n",
    "    它延迟100ms，因此reactor在处理等待的调用前，有时间完成读写操作，所以不要设置延迟为零。\n",
    "    \"\"\"\n",
    "    d = defer.Deferred()\n",
    "    reactor.callLater(0.1, d.errback, _failure)\n",
    "    return d\n",
    "\n",
    "def defer_succeed(result):\n",
    "    \"\"\"Same as twisted.internet.defer.succeed but delay calling callback until\n",
    "    next reactor loop\n",
    "\n",
    "    It delays by 100ms so reactor has a chance to go trough readers and writers\n",
    "    before attending pending delayed calls, so do not set delay to zero.\n",
    "    同上面defer_fail,延迟100ms,以完成读写操作\n",
    "    \"\"\"\n",
    "    d = defer.Deferred()\n",
    "    reactor.callLater(0.1, d.callback, result)\n",
    "    return d\n",
    "\n",
    "def parallel(iterable, count, callable, *args, **named):\n",
    "    \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n",
    "    using no more than ``count`` concurrent calls.\n",
    "    并行地对给定iterable中的对象执行一个可调用对象，使用的并发数不超过 count 。\n",
    "    Taken from: http://jcalderone.livejournal.com/24285.html\n",
    "    \"\"\"\n",
    "    coop = task.Cooperator()\n",
    "    work = (callable(elem, *args, **named) for elem in iterable)\n",
    "    return defer.DeferredList([coop.coiterate(work) for _ in range(count)])\n",
    "\n",
    "def iter_errback(iterable, errback, *a, **kw):\n",
    "    \"\"\"Wraps an iterable calling an errback if an error is caught while\n",
    "    iterating it.\n",
    "    迭代处理iterable这个对象，如果在迭代时捕获到错误，则可以包装一个可调用的回调函数。\n",
    "    \"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(it)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except:\n",
    "            errback(failure.Failure(), *a, **kw)\n",
    "\n",
    "def iterate_spider_output(result):\n",
    "    # 这个方法定位到另一个位置了。 我们追踪过去看看\n",
    "    return arg_to_iter(result)\n",
    "\n",
    "def arg_to_iter(arg):\n",
    "    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n",
    "    value, or an iterable.\n",
    "    将对象转化为可迭代的对象。如果是none返回[], 如果是不可迭代就返回[arg] 本身可迭代的就返回自身。\n",
    "    _ITERABLE_SINGLE_VALUES = dict, BaseItem, six.text_type, bytes\n",
    "    Exception: if arg is a dict, [arg] will be returned\n",
    "    \"\"\"\n",
    "    if arg is None:\n",
    "        return []\n",
    "    elif not isinstance(arg, _ITERABLE_SINGLE_VALUES) and hasattr(arg, '__iter__'):\n",
    "        return arg\n",
    "    else:\n",
    "        return [arg]   \n",
    " \n",
    "def logformatter_adapter(logkws):\n",
    "    \"\"\"\n",
    "    Helper that takes the dictionary output from the methods in LogFormatter\n",
    "    and adapts it into a tuple of positional arguments for logger.log calls,\n",
    "    handling backward compatibility as well.\n",
    "    # 这个方法是个适配器，将logkws字典对象最终返回一个元组，日志级别，信息，和详细参数。\n",
    "    \"\"\"\n",
    "    if not {'level', 'msg', 'args'} <= set(logkws):\n",
    "        warnings.warn('Missing keys in LogFormatter method',\n",
    "                      ScrapyDeprecationWarning)\n",
    "\n",
    "    if 'format' in logkws:\n",
    "        warnings.warn('`format` key in LogFormatter methods has been '\n",
    "                      'deprecated, use `msg` instead',\n",
    "                      ScrapyDeprecationWarning)\n",
    "\n",
    "    level = logkws.get('level', logging.INFO)\n",
    "    message = logkws.get('format', logkws.get('msg'))\n",
    "    # NOTE: This also handles 'args' being an empty dict, that case doesn't\n",
    "    # play well in logger.log calls\n",
    "    args = logkws if not logkws.get('args') else logkws['args']\n",
    "    return (level, message, args)\n",
    "\n",
    "def failure_to_exc_info(failure):\n",
    "    \"\"\"Extract exc_info from Failure instances\"\"\"\n",
    "    # 用于提取错误实例的信息。\n",
    "    if isinstance(failure, Failure):\n",
    "        return (failure.type, failure.value, failure.getTracebackObject())\n",
    "\n",
    "\n",
    "def referer_str(request):\n",
    "    \"\"\" Return Referer HTTP header suitable for logging. \"\"\"\n",
    "    # 从请求头信息里面获取referer信息。\n",
    "    referrer = request.headers.get('Referer')\n",
    "    if referrer is None:\n",
    "        return referrer\n",
    "    return to_native_str(referrer, errors='replace')\n",
    "\n",
    "\n",
    "class Slot(object):\n",
    "    \"\"\"Scraper slot (one per running spider)\"\"\"\n",
    "\n",
    "    MIN_RESPONSE_SIZE = 1024\n",
    "    \n",
    "    def __init__(self, max_active_size=5000000):\n",
    "        self.max_active_size = max_active_size\n",
    "        self.queue = deque()\n",
    "        self.active = set()\n",
    "        self.active_size = 0\n",
    "        self.itemproc_size = 0\n",
    "        self.closing = None\n",
    "        # 这里设置了，最大活动大小，默认值为5000000， 这个值为何不放到默认配置文件里面呢。疑惑下。\n",
    "        # 构造一个deques,使用集合去存储活动的，活动的大小开始为0\n",
    "        # itemproc_size item处理的大小，关闭状态为none.\n",
    "        \n",
    "    def add_response_request(self, response, request):\n",
    "        deferred = defer.Deferred()\n",
    "        self.queue.append((response, request, deferred))\n",
    "        if isinstance(response, Response):\n",
    "            self.active_size += max(len(response.body), self.MIN_RESPONSE_SIZE)\n",
    "        else:\n",
    "            self.active_size += self.MIN_RESPONSE_SIZE\n",
    "        return deferred\n",
    "        # 这个方法从名字上看 ，应该是添加响应请求吧，\n",
    "        # 创建一个defer对象，队列里面添加一个(response,request,deferred)元祖\n",
    "        # 如果response 是Resposne子类的话，活动的大小就是原来活动的大小+ body的长度h或者最小响应的大小。\n",
    "        # 否则，就设置为最小的响应大小。放回那个deferred.\n",
    "\n",
    "    def next_response_request_deferred(self):\n",
    "        # 从队列中popleft一个元组，活动请求添加request,返回这个元组，response,request,deferred\n",
    "        response, request, deferred = self.queue.popleft()\n",
    "        self.active.add(request)\n",
    "        return response, request, deferred\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         # 从队列中popleft一个元组，活动请求添加request,返回一个元组，response,request,deferred\n",
    "    def finish_response(self, response, request):\n",
    "        self.active.remove(request)\n",
    "        if isinstance(response, Response):\n",
    "            self.active_size -= max(len(response.body), self.MIN_RESPONSE_SIZE)\n",
    "        else:\n",
    "            # 完成响应的话， 就从active活动列表中移除这个请求，active_size 减去对应大小。\n",
    "            self.active_size -= self.MIN_RESPONSE_SIZE\n",
    "\n",
    "    def is_idle(self):\n",
    "        # 是否空闲的判断， 如果队列不为空， 或者active不为空。\n",
    "        return not (self.queue or self.active)   \n",
    "    def needs_backout(self):\n",
    "        # 判断是否超限了。 \n",
    "        return self.active_size > self.max_active_size    \n",
    "\n",
    "\n",
    "class Scraper(object):\n",
    "    def __init__(self, crawler):\n",
    "        # 爬虫中间件管理器从crawler获取，item处理类从crawler.settings获取。然后获取一个item处理类的对象。\n",
    "        # 并发item数量，信号和日志formatter设置都是从crawler获取。\n",
    "        self.slot = None\n",
    "        self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n",
    "        itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n",
    "        self.itemproc = itemproc_cls.from_crawler(crawler)\n",
    "        self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n",
    "        self.crawler = crawler\n",
    "        self.signals = crawler.signals\n",
    "        self.logformatter = crawler.logformatter\n",
    "\n",
    "    @defer.inlineCallbacks\n",
    "    def open_spider(self, spider):\n",
    "        # 这个方法就是打开给定的爬虫，并分配指定的资源，\n",
    "        # 创建一个slot，然后调用对应的itemproessor类创建的处理类去打开爬虫。\n",
    "        # ExecutionEngine 在open_spider里会调用这个方法来初始化scraper:\n",
    "        \"\"\"Open the given spider for scraping and allocate resources for it\"\"\"\n",
    "        self.slot = Slot()\n",
    "        yield self.itemproc.open_spider(spider)\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        \"\"\"Close a spider being scraped and release its resources\"\"\"\n",
    "        slot = self.slot\n",
    "        slot.closing = defer.Deferred()\n",
    "        slot.closing.addCallback(self.itemproc.close_spider)\n",
    "        self._check_if_closing(spider, slot)\n",
    "        return slot.closing  \n",
    "        # 关闭爬虫并且释放资源。\n",
    "        # 获取slot， 然后给slot添加一个closing的事件，然后放回方法。\n",
    "    def is_idle(self):\n",
    "        \"\"\"Return True if there isn't any more spiders to process\"\"\"\n",
    "        return not self.slot \n",
    "        # 如果没有爬虫去处理了。 就返回true了。 \n",
    "\n",
    "    def _check_if_closing(self, spider, slot):\n",
    "        if slot.closing and slot.is_idle():\n",
    "            slot.closing.callback(spider)        \n",
    "        # 如果closing不为空，不为空闲，  就调用指定spider的关闭回调。\n",
    "\n",
    "    def enqueue_scrape(self, response, request, spider):\n",
    "        slot = self.slot\n",
    "        dfd = slot.add_response_request(response, request)\n",
    "        def finish_scraping(_):\n",
    "            slot.finish_response(response, request)\n",
    "            self._check_if_closing(spider, slot)\n",
    "            self._scrape_next(spider, slot)\n",
    "            return _\n",
    "        dfd.addBoth(finish_scraping)\n",
    "        dfd.addErrback(\n",
    "            lambda f: logger.error('Scraper bug processing %(request)s',\n",
    "                                    {'request': request},\n",
    "                                    exc_info=failure_to_exc_info(f),\n",
    "                                    extra={'spider': spider}))\n",
    "        self._scrape_next(spider, slot)\n",
    "        return dfd\n",
    "        # 调用add_response_request添加返回一个defferd对象，定义一个完成的方法，\n",
    "        给成功和失败都添加一个finish_scraping的回调。\n",
    "        # 给错误的在添加一个匿名的回调方法。\n",
    "        # 调用_scrape_next 处理下一个。 \n",
    "    def _scrape_next(self, spider, slot):\n",
    "        while slot.queue:\n",
    "            response, request, deferred = slot.next_response_request_deferred()\n",
    "            self._scrape(response, request, spider).chainDeferred(deferred)   \n",
    "        # 这里如果slot的queue有内容的haunted， 就一直循环下去， 调用_scrape去处理。\n",
    "\n",
    "    def _scrape(self, response, request, spider):\n",
    "        \"\"\"Handle the downloaded response or failure through the spider\n",
    "        callback/errback\"\"\"\n",
    "        assert isinstance(response, (Response, Failure))\n",
    "\n",
    "        dfd = self._scrape2(response, request, spider) # returns spiders processed output\n",
    "        dfd.addErrback(self.handle_spider_error, request, response, spider)\n",
    "        dfd.addCallback(self.handle_spider_output, request, response, spider)\n",
    "        return dfd    \n",
    "        # 这个方法就是处理下载响应或者失败，通过给爬虫指定的成功和错误的回调方法。\n",
    "        # 先断言这个是响应流或者failure，调用_scrape2获取爬虫处理的输出\n",
    "        # 添加错误回调和成功回调。\n",
    "    def _scrape2(self, request_result, request, spider):\n",
    "        \"\"\"Handle the different cases of request's result been a Response or a\n",
    "        Failure\"\"\"\n",
    "        if not isinstance(request_result, Failure):\n",
    "            return self.spidermw.scrape_response(\n",
    "                self.call_spider, request_result, request, spider)\n",
    "        else:\n",
    "            # FIXME: don't ignore errors in spider middleware\n",
    "            dfd = self.call_spider(request_result, request, spider)\n",
    "            return dfd.addErrback(\n",
    "                self._log_download_errors, request_result, request, spider)\n",
    "        # 如果响应是成功的的。调用自己的爬虫中间件去处理响应。如果是错误的，调用call_spider方法，给dfd添加一个错误的回调。 \n",
    "\n",
    "    def call_spider(self, result, request, spider):\n",
    "        result.request = request\n",
    "        dfd = defer_result(result)\n",
    "        dfd.addCallbacks(request.callback or spider.parse, request.errback)\n",
    "        return dfd.addCallback(iterate_spider_output)\n",
    "        defer_result 这个方法我们上面已经看了。 主要是等100ms读写的。 添加成功的回调。\n",
    "        # 这个地方注意了。 先使用request。callback , 如果没有指定的话，默认采用spider.parse方法。\n",
    "        # 这就是我们的爬虫为何使用parse方法解析response的原因了。 \n",
    "        # 添加一个成功回调。 iterate_spider_output 这个上面已经看过了， 就是返回一个可迭代的对象。 \n",
    "    def handle_spider_error(self, _failure, request, response, spider):\n",
    "        exc = _failure.value\n",
    "        if isinstance(exc, CloseSpider):\n",
    "            self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n",
    "            return\n",
    "        logger.error(\n",
    "            \"Spider error processing %(request)s (referer: %(referer)s)\",\n",
    "            {'request': request, 'referer': referer_str(request)},\n",
    "            exc_info=failure_to_exc_info(_failure),\n",
    "            extra={'spider': spider}\n",
    "        )\n",
    "        self.signals.send_catch_log(\n",
    "            signal=signals.spider_error,\n",
    "            failure=_failure, response=response,\n",
    "            spider=spider\n",
    "        )\n",
    "        self.crawler.stats.inc_value(\n",
    "            \"spider_exceptions/%s\" % _failure.value.__class__.__name__,\n",
    "            spider=spider\n",
    "        )\n",
    "        # 这个方法就是处理爬虫错误的 ， 如果是关闭爬虫异常， 调用对应引擎去关闭爬虫，返回\n",
    "        # 其他情况，就记录下日志信息。 发送对应的爬虫错误信号， 统计信息的添加。\n",
    "    def handle_spider_output(self, result, request, response, spider):\n",
    "        if not result:\n",
    "            return defer_succeed(None)\n",
    "        it = iter_errback(result, self.handle_spider_error, request, response, spider)\n",
    "        dfd = parallel(it, self.concurrent_items,\n",
    "            self._process_spidermw_output, request, response, spider)\n",
    "        return dfd   \n",
    "        # 处理爬虫的输出，如果结果不为空 调用defer_succeed,错误的话调用错误回调，平行处理， _process_spidermw_output去处理\n",
    "\n",
    "    def _process_spidermw_output(self, output, request, response, spider):\n",
    "        \"\"\"Process each Request/Item (given in the output parameter) returned\n",
    "        from the given spider\n",
    "        \"\"\"\n",
    "        if isinstance(output, Request):\n",
    "            self.crawler.engine.crawl(request=output, spider=spider)\n",
    "        elif isinstance(output, (BaseItem, dict)):\n",
    "            self.slot.itemproc_size += 1\n",
    "            dfd = self.itemproc.process_item(output, spider)\n",
    "            dfd.addBoth(self._itemproc_finished, output, response, spider)\n",
    "            return dfd\n",
    "        elif output is None:\n",
    "            pass\n",
    "        else:\n",
    "            typename = type(output).__name__\n",
    "            logger.error('Spider must return Request, BaseItem, dict or None, '\n",
    "                            'got %(typename)r in %(request)s',\n",
    "                            {'request': request, 'typename': typename},\n",
    "                            extra={'spider': spider})\n",
    "        # 处理每个请求或者从给定的爬虫得到的item\n",
    "        # 如果output是个请求的话， 调用engine.crawl抓取。 \n",
    "        # 如果是baseitem或者dict的话， 处理个数加1，调用item处理类的process_item去处理item。\n",
    "        # 添加处理完毕事件。 \n",
    "        # 其他请求输出日志。 报告你的返回类型不是给定的item类或者字典类。 或者请求。 \n",
    "        # 这里就是限定了。 我们爬虫里面的parse方法只能返回这3类的原因了。 \n",
    "\n",
    "    def _log_download_errors(self, spider_failure, download_failure, request, spider):\n",
    "        \"\"\"Log and silence errors that come from the engine (typically download\n",
    "        errors that got propagated thru here)\n",
    "        \"\"\"\n",
    "        if (isinstance(download_failure, Failure) and\n",
    "                not download_failure.check(IgnoreRequest)):\n",
    "            if download_failure.frames:\n",
    "                logger.error('Error downloading %(request)s',\n",
    "                                {'request': request},\n",
    "                                exc_info=failure_to_exc_info(download_failure),\n",
    "                                extra={'spider': spider})\n",
    "            else:\n",
    "                errmsg = download_failure.getErrorMessage()\n",
    "                if errmsg:\n",
    "                    logger.error('Error downloading %(request)s: %(errmsg)s',\n",
    "                                    {'request': request, 'errmsg': errmsg},\n",
    "                                    extra={'spider': spider})\n",
    "\n",
    "        if spider_failure is not download_failure:\n",
    "            return spider_failure    \n",
    "        # 这里定义个方法下载错误的， 如果是错误 并且不是ignorerequest的话进入if块。\n",
    "        # 如果错误frames不为空，记录错误信息。\n",
    "        # 否则调用geterrmessage方法，记录错误。\n",
    "        # 如果错误不是下载错误，返回爬虫的错误。\n",
    "\n",
    "    def _itemproc_finished(self, output, item, response, spider):\n",
    "        \"\"\"ItemProcessor finished for the given ``item`` and returned ``output``\n",
    "        \"\"\"\n",
    "        self.slot.itemproc_size -= 1\n",
    "        if isinstance(output, Failure):\n",
    "            ex = output.value\n",
    "            if isinstance(ex, DropItem):\n",
    "                logkws = self.logformatter.dropped(item, ex, response, spider)\n",
    "                logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n",
    "                return self.signals.send_catch_log_deferred(\n",
    "                    signal=signals.item_dropped, item=item, response=response,\n",
    "                    spider=spider, exception=output.value)\n",
    "            else:\n",
    "                logger.error('Error processing %(item)s', {'item': item},\n",
    "                                exc_info=failure_to_exc_info(output),\n",
    "                                extra={'spider': spider})\n",
    "        else:\n",
    "            logkws = self.logformatter.scraped(output, response, spider)\n",
    "            logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n",
    "            return self.signals.send_catch_log_deferred(\n",
    "                signal=signals.item_scraped, item=output, response=response,\n",
    "                spider=spider)\n",
    "        # itemprocess处理类结束，如果输出㐊错误。 判定他是不是dropitem。  分别记录日志。  \n",
    "        # 正常情况下，记录日志 。通过日志适配器将logkws 转出logger.log方法接受的参数。 \n",
    "        # 发送itemscraped信号。 \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
