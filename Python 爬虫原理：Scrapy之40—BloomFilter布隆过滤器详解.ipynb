{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d390e21b",
   "metadata": {},
   "source": [
    "# BloomFilter布隆过滤器详解\n",
    "　　参考：[ Bloom Filter 的对接](https://www.cnblogs.com/ciquankun/p/13329292.html)<br>\n",
    "　　　　　[]()<br>\n",
    "　　　　　[]()<br>\n",
    "　　　　　[]()<br>\n",
    "　　　　　[]()<br>\n",
    "　　　　　[]()<br>\n",
    "　　　　　[]()<br>\n",
    "\n",
    "## 一、BloomFilter原理\n",
    "　　首先回顾一下 Scrapy-Redis 的去重机制。Scrapy-Redis 将 Request 的指纹存储到了 Redis 集合中，每个指纹的长度为 40，例如 27adcc2e8979cdee0c9cecbbe8bf8ff51edefb61 就是一个指纹，它的每一位都是 16 进制数。\n",
    "　　我们计算一下用这种方式耗费的存储空间。每个十六进制数占用 4b（byte），2B   1个指纹用 40 个十六进制数表示，占用空间为 20 B，1 万个指纹即占用空间 200 KB，1 亿个指纹占用 2 GB。当爬取数量达到上亿级别时，Redis 的占用的内存就会变得很大，而且这仅仅是指纹的存储。Redis 还存储了爬取队列，内存占用会进一步提高，更别说有多个 Scrapy 项目同时爬取的情况了。当爬取达到亿级别规模时，Scrapy-Redis 提供的集合去重已经不能满足我们的要求。所以我们需要使用一个更加节省内存的去重算法 Bloom Filter。\n",
    "### 1.1、了解 BloomFilter\n",
    "　　Bloom Filter，中文名称叫作布隆过滤器，是 1970 年由 Bloom 提出的，它可以被用来检测一个元素是否在一个集合中。Bloom Filter 的空间利用效率很高，使用它可以大大节省存储空间。Bloom Filter 使用位数组表示一个待检测集合，并可以快速地通过概率算法判断一个元素是否存在于这个集合中。利用这个算法我们可以实现去重效果。\n",
    "　　本节我们来了解 Bloom Filter 的基本算法，以及 Scrapy-Redis 中对接 Bloom Filter 的方法。\n",
    "### 1.2、BloomFilter 算法\n",
    "　　在 Bloom Filter 中使用位数组来辅助实现检测判断。在初始状态下，我们声明一个包含 m 位的位数组，它的所有位都是 0，如图 14-7 所示。\n",
    "图 14-7 初始位数组\n",
    "　　现在我们有了一个待检测集合，我们表示为 S={x1, x2, ..., xn}，我们接下来需要做的就是检测一个 x 是否已经存在于集合 S 中。在 Bloom Filter 算法中首先使用 k 个相互独立的、随机的哈希函数来将这个集合 S 中的每个元素 x1、x2、...、xn 映射到这个长度为 m 的位数组上，哈希函数得到的结果记作位置索引，然后将位数组该位置索引的位置 1。例如这里我们取 k 为 3，即有三个哈希函数，x1 经过三个哈希函数映射得到的结果分别为 1、4、8，x2 经过三个哈希函数映射得到的结果分别为 4、6、10，那么就会将位数组的 1、4、6、8、10 这五位置 1，如图 14-8 所示：\n",
    "图 14-8 映射后位数组\n",
    "\n",
    "　　这时如果再有一个新的元素 x，我们要判断 x 是否属于 S 这个集合，我们便会将仍然用 k 个哈希函数对 x 求映射结果，如果所有结果对应的位数组位置均为 1，那么我们就认为 x 属于 S 这个集合，否则如果有一个不为 1，则 x 不属于 S 集合。\n",
    "　　例如一个新元素 x 经过三个哈希函数映射的结果为 4、6、8，对应的位置均为 1，则判断 x 属于 S 这个集合。如果结果为 4、6、7，7 对应的位置为 0，则判定 x 不属于 S 这个集合。\n",
    "　　注意这里 m、n、k 满足的关系是 m>nk，也就是说位数组的长度 m 要比集合元素 n 和哈希函数 k 的乘积还要大。\n",
    "　　这样的判定方法很高效，但是也是有代价的，它可能把不属于这个集合的元素误认为属于这个集合，我们来估计一下它的错误率。当集合 S={x1, x2,…, xn} 的所有元素都被 k 个哈希函数映射到 m 位的位数组中时，这个位数组中某一位还是 0 的概率是：\n",
    "　　因为哈希函数是随机的，所以任意一个哈希函数选中这一位的概率为 1/m，那么 1-1/m 就代表哈希函数一次没有选中这一位的概率，要把 S 完全映射到 m 位数组中，需要做 kn 次哈希运算，所以最后的概率就是 1-1/m 的 kn 次方。\n",
    "　　一个不属于 S 的元素 x 如果要被误判定为在 S 中，那么这个概率就是 k 次哈希运算得到的结果对应的位数组位置都为 1，所以误判概率为：\n",
    "\n",
    "根据：\n",
    "\n",
    "可以将误判概率转化为：\n",
    "在给定 m、n 时，可以求出使得 f 最小化的 k 值为：\n",
    "在这里将误判概率归纳如下：\n",
    "表 14-1　误判概率\n",
    "\n",
    "m/n\t最优 k\tk=1\tk=2\tk=3\tk=4\tk=5\tk=6\tk=7\tk=8\n",
    "2\t1.39\t0.393\t0.400\t\t\t\t\t\t\n",
    "3\t2.08\t0.283\t0.237\t0.253\t\t\t\t\t\n",
    "4\t2.77\t0.221\t0.155\t0.147\t0.160\t\t\t\t\n",
    "5\t3.46\t0.181\t0.109\t0.092\t0.092\t0.101\t\t\t\n",
    "6\t4.16\t0.154\t0.0804\t0.0609\t0.0561\t0.0578\t0.0638\t\t\n",
    "7\t4.85\t0.133\t0.0618\t0.0423\t0.0359\t0.0347\t0.0364\t\t\n",
    "8\t5.55\t0.118\t0.0489\t0.0306\t0.024\t0.0217\t0.0216\t0.0229\t\n",
    "9\t6.24\t0.105\t0.0397\t0.0228\t0.0166\t0.0141\t0.0133\t0.0135\t0.0145\n",
    "10\t6.93\t0.0952\t0.0329\t0.0174\t0.0118\t0.00943\t0.00844\t0.00819\t0.00846\n",
    "11\t7.62\t0.0869\t0.0276\t0.0136\t0.00864\t0.0065\t0.00552\t0.00513\t0.00509\n",
    "12\t8.32\t0.08\t0.0236\t0.0108\t0.00646\t0.00459\t0.00371\t0.00329\t0.00314\n",
    "13\t9.01\t0.074\t0.0203\t0.00875\t0.00492\t0.00332\t0.00255\t0.00217\t0.00199\n",
    "14\t9.7\t0.0689\t0.0177\t0.00718\t0.00381\t0.00244\t0.00179\t0.00146\t0.00129\n",
    "15\t10.4\t0.0645\t0.0156\t0.00596\t0.003\t0.00183\t0.00128\t0.001\t0.000852\n",
    "16\t11.1\t0.0606\t0.0138\t0.005\t0.00239\t0.00139\t0.000935\t0.000702\t0.000574\n",
    "17\t11.8\t0.0571\t0.0123\t0.00423\t0.00193\t0.00107\t0.000692\t0.000499\t0.000394\n",
    "18\t12.5\t0.054\t0.0111\t0.00362\t0.00158\t0.000839\t0.000519\t0.00036\t0.000275\n",
    "19\t13.2\t0.0513\t0.00998\t0.00312\t0.0013\t0.000663\t0.000394\t0.000264\t0.000194\n",
    "20\t13.9\t0.0488\t0.00906\t0.0027\t0.00108\t0.00053\t0.000303\t0.000196\t0.00014\n",
    "21\t14.6\t0.0465\t0.00825\t0.00236\t0.000905\t0.000427\t0.000236\t0.000147\t0.000101\n",
    "22\t15.2\t0.0444\t0.00755\t0.00207\t0.000764\t0.000347\t0.000185\t0.000112\t7.46e-05\n",
    "23\t15.9\t0.0425\t0.00694\t0.00183\t0.000649\t0.000285\t0.000147\t8.56e-05\t5.55e-05\n",
    "24\t16.6\t0.0408\t0.00639\t0.00162\t0.000555\t0.000235\t0.000117\t6.63e-05\t4.17e-05\n",
    "25\t17.3\t0.0392\t0.00591\t0.00145\t0.000478\t0.000196\t9.44e-05\t5.18e-05\t3.16e-05\n",
    "26\t18\t0.0377\t0.00548\t0.00129\t0.000413\t0.000164\t7.66e-05\t4.08e-05\t2.42e-05\n",
    "27\t18.7\t0.0364\t0.0051\t0.00116\t0.000359\t0.000138\t6.26e-05\t3.24e-05\t1.87e-05\n",
    "28\t19.4\t0.0351\t0.00475\t0.00105\t0.000314\t0.000117\t5.15e-05\t2.59e-05\t1.46e-05\n",
    "29\t20.1\t0.0339\t0.00444\t0.000949\t0.000276\t9.96e-05\t4.26e-05\t2.09e-05\t1.14e-05\n",
    "30\t20.8\t0.0328\t0.00416\t0.000862\t0.000243\t8.53e-05\t3.55e-05\t1.69e-05\t9.01e-06\n",
    "31\t21.5\t0.0317\t0.0039\t0.000785\t0.000215\t7.33e-05\t2.97e-05\t1.38e-05\t7.16e-06\n",
    "32\t22.2\t0.0308\t0.00367\t0.000717\t0.000191\t6.33e-05\t2.5e-05\t1.13e-05\t5.73e-06\n",
    "表 14-1 中第一列为 m/n 的值，第二列为最优 k 值，其后列为不同 k 值的误判概率，可以看到当 k 值确定时，随着 m/n 的增大，误判概率逐渐变小。当 m/n 的值确定时，当 k 越靠近最优 K 值，误判概率越小。另外误判概率总体来看都是极小的，在容忍此误判概率的情况下，大幅减小存储空间和判定速度是完全值得的。\n",
    "\n",
    "接下来我们就将 BloomFilter 算法应用到 Scrapy-Redis 分布式爬虫的去重过程中，以解决 Redis 内存不足的问题。\n",
    "\n",
    "3. 对接 Scrapy-Redis\n",
    "实现 BloomFilter 时，我们首先要保证不能破坏 Scrapy-Redis 分布式爬取的运行架构，所以我们需要修改 Scrapy-Redis 的源码，将它的去重类替换掉。同时 BloomFilter 的实现需要借助于一个位数组，所以既然当前架构还是依赖于 Redis 的，那么正好位数组的维护直接使用 Redis 就好了。\n",
    "\n",
    "首先我们实现一个基本的哈希算法，可以实现将一个值经过哈希运算后映射到一个 m 位位数组的某一位上，代码实现如下：\n",
    "\n",
    "class HashMap(object):\n",
    "    def __init__(self, m, seed):\n",
    "        self.m = m\n",
    "        self.seed = seed\n",
    "    \n",
    "    def hash(self, value):\n",
    "        \"\"\"\n",
    "        Hash Algorithm\n",
    "        :param value: Value\n",
    "        :return: Hash Value\n",
    "        \"\"\"\n",
    "        ret = 0\n",
    "        for i in range(len(value)):\n",
    "            ret += self.seed * ret + ord(value[i])\n",
    "        return (self.m - 1) & ret\n",
    "在这里新建了一个 HashMap 类，构造函数传入两个值，一个是 m 位数组的位数，另一个是种子值 seed，不同的哈希函数需要有不同的 seed，这样可以保证不同的哈希函数的结果不会碰撞。\n",
    "\n",
    "在 hash() 方法的实现中，value 是要被处理的内容，在这里我们遍历了该字符的每一位并利用 ord() 方法取到了它的 ASCII 码值，然后混淆 seed 进行迭代求和运算，最终会得到一个数值。这个数值的结果就由 value 和 seed 唯一确定，然后我们再将它和 m 进行按位与运算，即可获取到 m 位数组的映射结果，这样我们就实现了一个由字符串和 seed 来确定的哈希函数。当 m 固定时，只要 seed 值相同，就代表是同一个哈希函数，相同的 value 必然会映射到相同的位置。所以如果我们想要构造几个不同的哈希函数，只需要改变其 seed 就好了，以上便是一个简易的哈希函数的实现。\n",
    "\n",
    "接下来我们再实现 BloomFilter，BloomFilter 里面需要用到 k 个哈希函数，所以在这里我们需要对这几个哈希函数指定相同的 m 值和不同的 seed 值，在这里构造如下：\n",
    "\n",
    "BLOOMFILTER_HASH_NUMBER = 6\n",
    "BLOOMFILTER_BIT = 30\n",
    "\n",
    "class BloomFilter(object):\n",
    "    def __init__(self, server, key, bit=BLOOMFILTER_BIT, hash_number=BLOOMFILTER_HASH_NUMBER):\n",
    "        \"\"\"\n",
    "        Initialize BloomFilter\n",
    "        :param server: Redis Server\n",
    "        :param key: BloomFilter Key\n",
    "        :param bit: m = 2 ^ bit\n",
    "        :param hash_number: the number of hash function\n",
    "        \"\"\"\n",
    "        # default to 1 << 30 = 10,7374,1824 = 2^30 = 128MB, max filter 2^30/hash_number = 1,7895,6970 fingerprints\n",
    "        self.m = 1 << bit\n",
    "        self.seeds = range(hash_number)\n",
    "        self.maps = [HashMap(self.m, seed) for seed in self.seeds]\n",
    "        self.server = server\n",
    "        self.key = key\n",
    "由于我们需要亿级别的数据的去重，即前文介绍的算法中的 n 为 1 亿以上，哈希函数的个数 k 大约取 10 左右的量级，而 m>kn，所以这里 m 值大约保底在 10 亿，由于这个数值比较大，所以这里用移位操作来实现，传入位数 bit，定义 30，然后做一个移位操作 1 << 30，相当于 2 的 30 次方，等于 1073741824，量级也是恰好在 10 亿左右，由于是位数组，所以这个位数组占用的大小就是 2^30b=128MB，而本文开头我们计算过 Scrapy-Redis 集合去重的占用空间大约在 2G 左右，可见 BloomFilter 的空间利用效率之高。\n",
    "\n",
    "随后我们再传入哈希函数的个数，用它来生成几个不同的 seed，用不同的 seed 来定义不同的哈希函数，这样我们就可以构造一个哈希函数列表，遍历 seed，构造带有不同 seed 值的 HashMap 对象，保存成变量 maps 供后续使用。\n",
    "\n",
    "另外 server 就是 Redis 连接对象，key 就是这个 m 位数组的名称。\n",
    "\n",
    "接下来我们就要实现比较关键的两个方法了，一个是判定元素是否重复的方法 exists()，另一个是添加元素到集合中的方法 insert()，实现如下：\n",
    "\n",
    "def exists(self, value):\n",
    "    \"\"\"\n",
    "    if value exists\n",
    "    :param value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not value:\n",
    "        return False\n",
    "    exist = 1\n",
    "    for map in self.maps:\n",
    "        offset = map.hash(value)\n",
    "        exist = exist & self.server.getbit(self.key, offset)\n",
    "    return exist\n",
    "\n",
    "def insert(self, value):\n",
    "    \"\"\"\n",
    "    add value to bloom\n",
    "    :param value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for f in self.maps:\n",
    "        offset = f.hash(value)\n",
    "        self.server.setbit(self.key, offset, 1)\n",
    "首先我们先看下 insert() 方法，BloomFilter 算法中会逐个调用哈希函数对放入集合中的元素进行运算得到在 m 位位数组中的映射位置，然后将位数组对应的位置置 1，所以这里在代码中我们遍历了初始化好的哈希函数，然后调用其 hash() 方法算出映射位置 offset，再利用 Redis 的 setbit() 方法将该位置 1。\n",
    "\n",
    "在 exists() 方法中我们就需要实现判定是否重复的逻辑了，方法参数 value 即为待判断的元素，在这里我们首先定义了一个变量 exist，然后遍历了所有哈希函数对 value 进行哈希运算，得到映射位置，然后我们用 getbit() 方法取得该映射位置的结果，依次进行与运算。这样只有每次 getbit() 得到的结果都为 1 时，最后的 exist 才为 True，即代表 value 属于这个集合。如果其中只要有一次 getbit() 得到的结果为 0，即 m 位数组中有对应的 0 位，那么最终的结果 exist 就为 False，即代表 value 不属于这个集合。这样此方法最后的返回结果就是判定重复与否的结果了。\n",
    "\n",
    "到现在为止 BloomFilter 的实现就已经完成了，我们可以用一个实例来测试一下，代码如下：\n",
    "\n",
    "conn = StrictRedis(host='localhost', port=6379, password='foobared')\n",
    "bf = BloomFilter(conn, 'testbf', 5, 6)\n",
    "bf.insert('Hello')\n",
    "bf.insert('World')\n",
    "result = bf.exists('Hello')\n",
    "print(bool(result))\n",
    "result = bf.exists('Python')\n",
    "print(bool(result))\n",
    "在这里我们首先定义了一个 Redis 连接对象，然后传递给 BloomFilter，为了避免内存占用过大这里传的位数 bit 比较小，设置为 5，哈希函数的个数设置为 6。\n",
    "\n",
    "首先我们调用 insert() 方法插入了 Hello 和 World 两个字符串，随后判断了一下 Hello 和 Python 这两个字符串是否存在，最后输出它的结果，运行结果如下：\n",
    "\n",
    "True\n",
    "False\n",
    "很明显，结果完全没有问题，这样我们就借助于 Redis 成功实现了 BloomFilter 的算法。\n",
    "\n",
    "接下来我们需要继续修改 Scrapy-Redis 的源码，将它的 dupefilter 逻辑替换为 BloomFilter 的逻辑，在这里主要是修改 RFPDupeFilter 类的 request_seen() 方法，实现如下：\n",
    "\n",
    "def request_seen(self, request):\n",
    "    fp = self.request_fingerprint(request)\n",
    "    if self.bf.exists(fp):\n",
    "        return True\n",
    "    self.bf.insert(fp)\n",
    "    return False\n",
    "首先还是利用 request_fingerprint() 方法获取了 Request 的指纹，然后调用 BloomFilter 的 exists() 方法判定了该指纹是否存在，如果存在，则证明该 Request 是重复的，返回 True，否则调用 BloomFilter 的 insert() 方法将该指纹添加并返回 False，这样就成功利用 BloomFilter 替换了 Scrapy-Redis 的集合去重。\n",
    "\n",
    "对于 BloomFilter 的初始化定义，我们可以将 init() 方法修改为如下内容：\n",
    "\n",
    "def __init__(self, server, key, debug, bit, hash_number):\n",
    "    self.server = server\n",
    "    self.key = key\n",
    "    self.debug = debug\n",
    "    self.bit = bit\n",
    "    self.hash_number = hash_number\n",
    "    self.logdupes = True\n",
    "    self.bf = BloomFilter(server, self.key, bit, hash_number)\n",
    "其中 bit 和 hash_number 需要使用 from_settings() 方法传递，修改如下：\n",
    "\n",
    "@classmethod\n",
    "def from_settings(cls, settings):\n",
    "    server = get_redis_from_settings(settings)\n",
    "    key = defaults.DUPEFILTER_KEY % {'timestamp': int(time.time())}\n",
    "    debug = settings.getbool('DUPEFILTER_DEBUG', DUPEFILTER_DEBUG)\n",
    "    bit = settings.getint('BLOOMFILTER_BIT', BLOOMFILTER_BIT)\n",
    "    hash_number = settings.getint('BLOOMFILTER_HASH_NUMBER', BLOOMFILTER_HASH_NUMBER)\n",
    "    return cls(server, key=key, debug=debug, bit=bit, hash_number=hash_number)\n",
    "其中常量的定义 DUPEFILTER_DEBUG 和 BLOOMFILTER_BIT 统一定义在 defaults.py 中，默认如下：\n",
    "\n",
    "BLOOMFILTER_HASH_NUMBER = 6\n",
    "BLOOMFILTER_BIT = 30\n",
    "到此为止我们就成功实现了 BloomFilter 和 Scrapy-Redis 的对接。\n",
    "\n",
    "4. 本节代码\n",
    "本节代码地址为：https://github.com/Python3WebSpider/ScrapyRedisBloomFilter。\n",
    "\n",
    "5. 使用\n",
    "为了方便使用，本节的代码已经打包成了一个 Python 包并发布到了 PyPi，链接为：https://pypi.python.org/pypi/scrapy-redis-bloomfilter，因此我们以后如果想使用 ScrapyRedisBloomFilter 直接使用就好了，不需要再自己实现一遍。\n",
    "\n",
    "我们可以直接使用 Pip 来安装，命令如下：\n",
    "\n",
    "pip3 install scrapy-redis-bloomfilter\n",
    "使用的方法和 Scrapy-Redis 基本相似，在这里说明几个关键配置：\n",
    "\n",
    "# 去重类，要使用 BloomFilter 请替换 DUPEFILTER_CLASS\n",
    "DUPEFILTER_CLASS = \"scrapy_redis_bloomfilter.dupefilter.RFPDupeFilter\"\n",
    "# 哈希函数的个数，默认为 6，可以自行修改\n",
    "BLOOMFILTER_HASH_NUMBER = 6\n",
    "# BloomFilter 的 bit 参数，默认 30，占用 128MB 空间，去重量级 1 亿\n",
    "BLOOMFILTER_BIT = 30\n",
    "DUPEFILTER_CLASS 是去重类，如果要使用 BloomFilter 需要将 DUPEFILTER_CLASS 修改为该包的去重类。\n",
    "\n",
    "BLOOMFILTER_HASH_NUMBER 是 BloomFilter 使用的哈希函数的个数，默认为 6，可以根据去重量级自行修改。\n",
    "\n",
    "BLOOMFILTER_BIT 即前文所介绍的 BloomFilter 类的 bit 参数，它决定了位数组的位数，如果 BLOOMFILTER_BIT 为 30，那么位数组位数为 2 的 30 次方，将占用 Redis 128MB 的存储空间，去重量级在 1 亿左右，即对应爬取量级 1 亿左右。如果爬取量级在 10 亿、20 亿甚至 100 亿，请务必将此参数对应调高。\n",
    "\n",
    "6. 测试\n",
    "在源代码中附有一个测试项目，放在 tests 文件夹，该项目使用了 Scrapy-RedisBloomFilter 来去重，Spider 的实现如下：\n",
    "\n",
    "from scrapy import Request, Spider\n",
    "\n",
    "class TestSpider(Spider):\n",
    "    name = 'test'\n",
    "    base_url = 'https://www.baidu.com/s?wd='\n",
    "    \n",
    "    def start_requests(self):\n",
    "        for i in range(10):\n",
    "            url = self.base_url + str(i)\n",
    "            yield Request(url, callback=self.parse)\n",
    "            \n",
    "        # Here contains 10 duplicated Requests    \n",
    "        for i in range(100): \n",
    "            url = self.base_url + str(i)\n",
    "            yield Request(url, callback=self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        self.logger.debug('Response of ' + response.url)\n",
    "在 start_requests() 方法中首先循环 10 次，构造参数为 0-9 的 URL，然后重新循环了 100 次，构造了参数为 0-99 的 URL，那么这里就会包含 10 个重复的 Request，我们运行项目测试一下：\n",
    "\n",
    "scrapy crawl test\n",
    "可以看到最后的输出结果如下：\n",
    "\n",
    "{'bloomfilter/filtered': 10,\n",
    " 'downloader/request_bytes': 34021,\n",
    " 'downloader/request_count': 100,\n",
    " 'downloader/request_method_count/GET': 100,\n",
    " 'downloader/response_bytes': 72943,\n",
    " 'downloader/response_count': 100,\n",
    " 'downloader/response_status_count/200': 100,\n",
    " 'finish_reason': 'finished',\n",
    " 'finish_time': datetime.datetime(2017, 8, 11, 9, 34, 30, 419597),\n",
    " 'log_count/DEBUG': 202,\n",
    " 'log_count/INFO': 7,\n",
    " 'memusage/max': 54153216,\n",
    " 'memusage/startup': 54153216,\n",
    " 'response_received_count': 100,\n",
    " 'scheduler/dequeued/redis': 100,\n",
    " 'scheduler/enqueued/redis': 100,\n",
    " 'start_time': datetime.datetime(2017, 8, 11, 9, 34, 26, 495018)}\n",
    "可以看到最后统计的第一行的结果：\n",
    "\n",
    "'bloomfilter/filtered': 10,\n",
    "这就是 BloomFilter 过滤后的统计结果，可以看到它的过滤个数为 10 个，也就是它成功将重复的 10 个 Reqeust 识别出来了，测试通过。\n",
    "\n",
    "7. 结语\n",
    "以上便是 BloomFilter 的原理及对接实现，使用了 BloomFilter 可以大大节省 Redis 内存，在数据量大的情况下推荐使用此方案。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
