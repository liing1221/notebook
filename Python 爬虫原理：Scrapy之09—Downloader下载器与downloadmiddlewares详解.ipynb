{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy的下载中间件详解\n",
    "　　参考：[官方文档](https://docs.scrapy.org/en/2.0/topics/settings.html?highlight=DOWNLOADER_MIDDLEWARES#downloader-middlewares-base)<br>\n",
    "　　　　　[Scrapy爬虫之下载中间件的处理流程](https://blog.csdn.net/qq_35556064/article/details/91346493)<br>\n",
    "　　　　　[一些常用中间件编写](https://www.jianshu.com/p/a91c663579eb)<br>\n",
    "　　　　　[彻底搞懂scrapy中间件](https://www.pianshen.com/article/8549378131/)<br>\n",
    "　　　　　[middlewares下载中间件、断点爬取、设置文件参数](https://www.jianshu.com/p/a91c663579eb)<br>\n",
    "　　　　　[scrapy启动流程源码分析(6)Downloader下载器](https://blog.csdn.net/csdn_yym/article/details/85721661)<br>\n",
    "　　　　　[scrapy源码9：middleware的源码分析](https://cuiyonghua.blog.csdn.net/article/details/110386947)<br>\n",
    "　　　　　[scrapy源码7：downloader的源码分析](https://cuiyonghua.blog.csdn.net/article/details/108335043)<br>\n",
    "　　　　　[Scrapy的抓取流程—Downloader](https://blog.csdn.net/okm6666/article/details/89575552)<br>\n",
    "　　　　　[Downloader 中间件（Downloader Middleware）详解](https://blog.csdn.net/qq_20288327/article/details/113524585)<br>\n",
    "　　　　　[scrapy源码分析（十一）----------下载器Downloader](https://blog.csdn.net/happyanger6/article/details/53572072/)<br>\n",
    "　　　　　[彻底搞懂Scrapy的中间件（一）](https://www.cnblogs.com/xieqiankun/p/know_middleware_of_scrapy_1.html)<br>\n",
    "　　　　　[Scrapy进阶知识点总结（六）——中间件详解](https://www.cnblogs.com/fengf233/p/11453375.html)<br>\n",
    "　　　　　[学习笔记 2-5 Scrapy的中间件](https://blog.csdn.net/kissazhu/article/details/80865739)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、Downloader下载器\n",
    "　　Downloader包含了从调度器调取url之后到scraper获取返回的网页内容之前的所有步骤，关系到网页如何下载，网络通信/HTTP协议/服务器等一系列知识，是最复杂的一部分内容。<br>\n",
    "### 1.1、Downloader初始化\n",
    "　　在之前Crawler.crawl()创建ExecutionEngine执行引擎的时候，就已经初始化了Downloader对象。此对象在配置中定义，默认为（scrapy/settings/default_settings.py）：<br>\n",
    ">DOWNLOADER = 'scrapy.core.downloader.Downloader'<br>\n",
    "\n",
    "　　关键对象有4个:slots,active,DownloadHandlers,DownloaderMiddlewareManager以及一些配置选项。<br>\n",
    "### 1.2、slots对象\n",
    "　　1、这个slots是一个存储Slot对象的字典，key是request对应的域名，值是一个Slot对象。<br>\n",
    "　　2、Slot对象用来控制一种Request下载请求，通常这种下载请求是对于同一个域名。<br>\n",
    "　　3、这个Slot对象还控制了访问这个域名的并发度，下载延迟控制，随机延时等，主要是为了控制对一个域名的访问策略，一定程度上避免流量过大被封IP。<br>\n",
    "\n",
    "### 1.3、active对象\n",
    "　　active是一个活动集合，用于记录当前正在下载的request集合。<br>\n",
    "\n",
    "### 1.4、DownloadHandlers对象\n",
    "　　是一个DownloadHandlers对象，它控制了许多handlers,对于不同的下载协议使用不同的handlers。<br>\n",
    "　　后面下载网页会调用handler的download_request方法。<br>\n",
    "　　默认支持handlers如下（scrapy/settings/default_settings.py）:<br>\n",
    ">DOWNLOAD_HANDLERS_BASE = {<br>\n",
    "　　'data': 'scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler',<br>\n",
    "　　'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',<br>\n",
    "　　'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',<br>\n",
    "　　'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',<br>\n",
    "　　's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',<br>\n",
    "　　'ftp': 'scrapy.core.downloader.handlers.ftp.FTPDownloadHandler',<br>\n",
    "}<br>\n",
    "\n",
    "### 1.5、DownloaderMiddlewareManager对象\n",
    "　　Downloader的中间件实现与scraper中间件的实现过程一样，也是通过Manager类管理。<br>\n",
    "　　默认中间件（scrapy/settings/default_settings.py）：<br>\n",
    ">DOWNLOADER_MIDDLEWARES_BASE = {<br>\n",
    "　　\\# Engine side<br>\n",
    "　　'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,<br>\n",
    "　　'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,<br>\n",
    "　　'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,<br>\n",
    "　　'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,<br>\n",
    "　　'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500,<br>\n",
    "　　'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,<br>\n",
    "　　'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,<br>\n",
    "　　'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,<br>\n",
    "　　'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,<br>\n",
    "　　'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,<br>\n",
    "　　'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,<br>\n",
    "　　'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,<br>\n",
    "　　'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,<br>\n",
    "　　'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,<br>\n",
    "　　\\# Downloader side<br>\n",
    "}<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、Downloader源码解析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy.core.downloader.__init__.py\n",
    "from __future__ import absolute_import\n",
    "# 建议尽可能多的使用绝对导入，因此在你的代码中使用from pkg improt string是适宜的。\n",
    "\n",
    "import random\n",
    "import warnings     # 这就是导入随机数和警告\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from collections import deque     # 这些也是导入基本的包， 没啥问题。 \n",
    "\n",
    "import six\n",
    "from twisted.internet import reactor, defer, task\n",
    "# 这里导入了兼容包six， 以及twisted 下的reactor反应堆， defer以及task。\n",
    "from scrapy.utils.defer import mustbe_deferred    # 导入强制deferred \n",
    "from scrapy.utils.httpobj import urlparse_cached   # 这个我们定位过去看看\n",
    "from scrapy.resolver import dnscache    # dnscache = LocalCache(10000) ，说明dnscache 是个dict 的。 \n",
    "from scrapy import signals              # 导入信号\n",
    "from .middleware import DownloaderMiddlewareManager\n",
    "from .handlers import DownloadHandlers    # 导入了下载中间件和下载处理器\n",
    "\n",
    "\n",
    "_urlparse_cache = weakref.WeakKeyDictionary()\n",
    "def urlparse_cached(request_or_response):\n",
    "    \"\"\"Return urlparse.urlparse caching the result, where the argument can be a\n",
    "    Request or Response object\n",
    "    方法是返回一个结果urlparse,urlparse的缓存结果。 参数可以是请求也可以是响应。 \n",
    "    如果对象不再字典里面， 就缓存下， 然后返回。\n",
    "    \"\"\"\n",
    "    if request_or_response not in _urlparse_cache:\n",
    "        _urlparse_cache[request_or_response] = urlparse(request_or_response.url)\n",
    "    return _urlparse_cache[request_or_response]\n",
    "\n",
    "\n",
    "class Slot(object):\n",
    "    \"\"\"Downloader slot\"\"\"\n",
    "\n",
    "    def __init__(self, concurrency, delay, randomize_delay):\n",
    "        # 初始化工作 。 并发，延迟， 是否随机延迟， 活动集合， 队列， 正在传输集合， 最后调用， \n",
    "        self.concurrency = concurrency\n",
    "        self.delay = delay\n",
    "        self.randomize_delay = randomize_delay   # 是否开启随机延迟\n",
    "\n",
    "        self.active = set()\n",
    "        self.queue = deque()\n",
    "        self.transferring = set()\n",
    "        self.lastseen = 0\n",
    "        self.latercall = None\n",
    "\n",
    "    def free_transfer_slots(self):\n",
    "        # 返回：空闲的传输槽 = 并发量 - 当前传输的个数\n",
    "        return self.concurrency - len(self.transferring)\n",
    "\n",
    "\n",
    "    def download_delay(self):\n",
    "        # 下载延迟，如果随机延迟开启的话 在0.5 ~ 1.5倍 delay 之间返回一个值，否则直接使用delay值。 \n",
    "        if self.randomize_delay:\n",
    "            return random.uniform(0.5 * self.delay, 1.5 * self.delay)\n",
    "        return self.delay\n",
    "\n",
    "    def close(self):\n",
    "        # 关闭下载器，如果最后一个调用存在，代用cancel取消。\n",
    "        if self.latercall and self.latercall.active():\n",
    "            self.latercall.cancel()\n",
    "\n",
    "    def __repr__(self):\n",
    "        cls_name = self.__class__.__name__\n",
    "        return \"%s(concurrency=%r, delay=%0.2f, randomize_delay=%r)\" % (\n",
    "            cls_name, self.concurrency, self.delay, self.randomize_delay)\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            \"<downloader.Slot concurrency=%r delay=%0.2f randomize_delay=%r \"\n",
    "            \"len(active)=%d len(queue)=%d len(transferring)=%d lastseen=%s>\" % (\n",
    "                self.concurrency, self.delay, self.randomize_delay,\n",
    "                len(self.active), len(self.queue), len(self.transferring),\n",
    "                datetime.fromtimestamp(self.lastseen).isoformat()\n",
    "            )\n",
    "        )\n",
    "    # 这2个就是基本的方法了。 没啥问题的。 \n",
    "\n",
    "    \n",
    "def _get_concurrency_delay(concurrency, spider, settings):\n",
    "    # 获取并发的延迟，从设置里面获取download_delay 也就是获取下载延迟设置。 \n",
    "    # 如果爬虫有对应的下载延迟设置DOWNLOAD_DELAY，提示警告（已废弃）。并采用该延迟设置值。\n",
    "    # 如果爬虫有对应的download_delay设置，设置延迟值。\n",
    "    # 如果有最大并发请求个数，获取该最大并发数\n",
    "    # 最终返回 （最大并发数，延迟量） \n",
    "    delay = settings.getfloat('DOWNLOAD_DELAY')\n",
    "    if hasattr(spider, 'DOWNLOAD_DELAY'):\n",
    "        warnings.warn(\"%s.DOWNLOAD_DELAY attribute is deprecated, use %s.download_delay instead\" %\n",
    "                      (type(spider).__name__, type(spider).__name__))\n",
    "        delay = spider.DOWNLOAD_DELAY\n",
    "    if hasattr(spider, 'download_delay'):\n",
    "        delay = spider.download_delay\n",
    "\n",
    "    if hasattr(spider, 'max_concurrent_requests'):\n",
    "        concurrency = spider.max_concurrent_requests\n",
    "\n",
    "    return concurrency, delay\n",
    "\n",
    "\n",
    "class Downloader(object):\n",
    "\n",
    "    DOWNLOAD_SLOT = 'download_slot'\n",
    "\n",
    "    def __init__(self, crawler):\n",
    "        # 这个是downloader的初始化方法， 根据一个crawler去初始化，\n",
    "        self.settings = crawler.settings\n",
    "        self.signals = crawler.signals    # 获取crawler的设置和信号\n",
    "        self.slots = {}\n",
    "        self.active = set()    # 构造一个slots字典，和一个活动集合（正在并发请求的任务集合，用于限制并发数）。 \n",
    "        self.handlers = DownloadHandlers(crawler)    # 根据crawler构造一个下载处理对象。\n",
    "        self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS')\n",
    "        self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n",
    "        self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')\n",
    "        self.randomize_delay = self.settings.getbool('RANDOMIZE_DOWNLOAD_DELAY')\n",
    "        self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n",
    "        # 从设置获取并发请求数，每个域名的最大请求数，每个ip的并发请求限制，随机下载延迟，构造中间件管理器。 \n",
    "        self._slot_gc_loop = task.LoopingCall(self._slot_gc)    # 是一个定时的循环\n",
    "        self._slot_gc_loop.start(60)   # # 这2句话的意思就是每60s就去执行下self._slot_gc方法（打印slot信息）。\n",
    "\n",
    "    def fetch(self, request, spider):\n",
    "        # 活动集合添加请求，下载中间件完成下载获取deffered对象， 添加一个回调方法移除请求。\n",
    "        def _deactivate(response):\n",
    "            self.active.remove(request)\n",
    "            return response\n",
    "\n",
    "        self.active.add(request)\n",
    "        dfd = self.middleware.download(self._enqueue_request, request, spider)    # 走下载器中间件\n",
    "        return dfd.addBoth(_deactivate)\n",
    "\n",
    "    def needs_backout(self):\n",
    "        # 判断当前活动的个数是不是超出了总的并发量\n",
    "        return len(self.active) >= self.total_concurrency\n",
    "\n",
    "    def _get_slot(self, request, spider):\n",
    "        # 获取 key -> 请求域名\n",
    "        # 如果key不在slots里面：如果ip并发限制了，就使用ip限制，否则使用域名并发限制。 \n",
    "        # 从设置里面获取并发和延迟，并根据key创建一个slot\n",
    "        # 返回key 和 slot\n",
    "        key = self._get_slot_key(request, spider)\n",
    "        if key not in self.slots:\n",
    "            conc = self.ip_concurrency if self.ip_concurrency else self.domain_concurrency\n",
    "            conc, delay = _get_concurrency_delay(conc, spider, self.settings)\n",
    "            self.slots[key] = Slot(conc, delay, self.randomize_delay)\n",
    "\n",
    "        return key, self.slots[key]\n",
    "\n",
    "    def _get_slot_key(self, request, spider):\n",
    "        # 这个先看看meta有没有download_slot ，如果有的话就直接返回\n",
    "        # 没有话，就从urlparse缓冲里面获取到主机名，如果为空，就设置为‘’ \n",
    "        # 如果有ip并发限制，从dns缓冲中获取\n",
    "        # 返回的 download_slot  是一个请求的 域名\n",
    "        if 'download_slot' in request.meta:\n",
    "            return request.meta['download_slot']\n",
    "\n",
    "        key = urlparse_cached(request).hostname or ''\n",
    "        if self.ip_concurrency:\n",
    "            key = dnscache.get(key, key)\n",
    "\n",
    "        return key \n",
    "\n",
    "    def _enqueue_request(self, request, spider):\n",
    "        # 根据 request 和 spider 获取 key、slot , 设置请求的meta。\n",
    "        # slot的active集合添加请求，添加回调方法去移除active中完成了的请求request。\n",
    "        # 发送一个信号量： 请求到达下载器\n",
    "        # slot的队列添加一个 request、deferred。\n",
    "        # 调用_process_queue 去处理队列。 返回deferred对象。\n",
    "        key, slot = self._get_slot(request, spider)\n",
    "        request.meta['download_slot'] = key\n",
    "\n",
    "        def _deactivate(response):\n",
    "            slot.active.remove(request)\n",
    "            return response\n",
    "\n",
    "        slot.active.add(request)\n",
    "        self.signals.send_catch_log(signal=signals.request_reached_downloader,\n",
    "                                    request=request,spider=spider)\n",
    "        deferred = defer.Deferred().addBoth(_deactivate)\n",
    "        slot.queue.append((request, deferred))\n",
    "        self._process_queue(spider, slot)\n",
    "        return deferred\n",
    "\n",
    "    def _process_queue(self, spider, slot):\n",
    "        # 处理队列的方法，如果最后一个调用在，就返回， \n",
    "        # 获取当前时间，从slot获取下载延迟。如果有延迟，slot记录的上次的lastseen + 延迟时间 - now 得到\n",
    "        # 上次使用距离当前的时间差值，如果大于0 说明等待时间小于延迟时间了，可以进行下一步操作了。 \n",
    "        # 设置下lastcall 设置下几秒之后执行方法。 也就是设置等待几秒去执行self._process_queue 方法。\n",
    "\n",
    "        # 如果还有空闲的传输clost的话：\n",
    "        # 设置lastseen为now,并从队列取出一个请求，调用_download返回一个deferred对象，添加到deferred链上，\n",
    "        # 如果有延迟，递归执行 _process_queue 方法。\n",
    "        if slot.latercall and slot.latercall.active():\n",
    "            return\n",
    "\n",
    "        # Delay queue processing if a download_delay is configured\n",
    "        now = time()\n",
    "        delay = slot.download_delay()\n",
    "        if delay:\n",
    "            penalty = delay - now + slot.lastseen\n",
    "            if penalty > 0:\n",
    "                slot.latercall = reactor.callLater(penalty, self._process_queue, spider, slot)\n",
    "                return\n",
    "        \"\"\"\n",
    "        在process_request部分的最后，会将下载器中间件下载前，方法返回的request重新扔回给downloader进行下载，\n",
    "        _enqueue_request方法中将request加入属于downloader的slot对象的队列中，然后对这个队列进行消费。如\n",
    "        果配置文件中设置了DOWNLOAD_DELAY，则会在延迟时间之后再重新调用_process_queue。\n",
    "        _process_queue方法中，从slot的queue中取出request，调用_download方法对该请求进行下载。\n",
    "        使用DownloadHandlers的实例对request进行下载。下载结束后，从slot的transferring集合中取出。（这里\n",
    "        是为了配合配置文件中的同时下载数使用）\n",
    "        \"\"\"\n",
    "        # Process enqueued requests if there are free slots to transfer for this slot\n",
    "        while slot.queue and slot.free_transfer_slots() > 0:\n",
    "            slot.lastseen = now\n",
    "            request, deferred = slot.queue.popleft()\n",
    "            dfd = self._download(slot, request, spider)\n",
    "            dfd.chainDeferred(deferred)\n",
    "            # prevent burst if inter-request delays were configured\n",
    "            if delay:\n",
    "                self._process_queue(spider, slot)\n",
    "                break\n",
    "\n",
    "    def _download(self, slot, request, spider):\n",
    "        # The order is very important for the following deferreds. Do not change!\n",
    "        # 创建下载的deferred， \n",
    "        # 在查询任务队列获取下一次请求之前，通知响应下载器监听最近的下载。\n",
    "        # 在接收到响应之后，将请求从下载状态中删除，以释放传输槽（slot），以便之后的请求任务使用（包括\n",
    "        # 来自下载中间件的请求）。\n",
    "        # 1. Create the download deferred\n",
    "        dfd = mustbe_deferred(self.handlers.download_request, request, spider)\n",
    "\n",
    "        # 2. Notify response_downloaded listeners about the recent download\n",
    "        # before querying queue for next request\n",
    "        def _downloaded(response):\n",
    "            self.signals.send_catch_log(signal=signals.response_downloaded,\n",
    "                                        response=response,\n",
    "                                        request=request,\n",
    "                                        spider=spider)\n",
    "            return response\n",
    "        dfd.addCallback(_downloaded)\n",
    "\n",
    "        # 3. After response arrives,  remove the request from transferring\n",
    "        # state to free up the transferring slot so it can be used by the\n",
    "        # following requests (perhaps those which came from the downloader\n",
    "        # middleware itself)\n",
    "        slot.transferring.add(request)\n",
    "\n",
    "        def finish_transferring(_):\n",
    "            slot.transferring.remove(request)\n",
    "            self._process_queue(spider, slot)\n",
    "            return _\n",
    "\n",
    "        return dfd.addBoth(finish_transferring)\n",
    "\n",
    "    def close(self):\n",
    "        # 关闭方法，关闭gc_loop ，以及所有slot。\n",
    "        self._slot_gc_loop.stop()\n",
    "        for slot in six.itervalues(self.slots):\n",
    "            slot.close()\n",
    "\n",
    "    def _slot_gc(self, age=60):\n",
    "        # 获取近一分钟前的时间\n",
    "        # 遍历slots。如果slot的状态不是active,且 上次访问时间 + 延迟 < mintime的话（早于1分钟前），\n",
    "        # 就从slots里面pop出来，并关闭。\n",
    "        mintime = time() - age\n",
    "        for key, slot in list(self.slots.items()):\n",
    "            if not slot.active and slot.lastseen + slot.delay < mintime:\n",
    "                self.slots.pop(key).close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 三、Middleware（下载中间件）使用\n",
    "　　Download Middleware是Scrapy的请求/响应处理的钩子框架。它是一个轻量级的低级系统，用于全局改变Scrapy的请求和响应。常用来添加代理，添加cookie，失败重新发起请求等等。<br>\n",
    "\n",
    "### 3.1、激活Download Middleware\n",
    "　　要激活Download Middleware，请在settings.py中激活 **DOWNLOADER_MIDDLEWARES**设置，该设置是一个dict，其键是中间件类路径，其值是中间件处理顺序值。要禁用某个中间件，只需把值设为None即可。<br>\n",
    "　　这是一个例子：\n",
    ">DOWNLOADER_MIDDLEWARES = {<br>\n",
    "    'myproject.middlewares.CustomDownloaderMiddleware': 543,<br>\n",
    "}<br>\n",
    "\n",
    "　　如果要禁用内置中间件DOWNLOADER_MIDDLEWARES_BASE默认定义和启用的中间件 ，则必须在项目的DOWNLOADER_MIDDLEWARES设置中定义它为None。例如，如果要禁用用户代理中间件：<br>\n",
    ">DOWNLOADER_MIDDLEWARES = {<br>\n",
    "    'myproject.middlewares.CustomDownloaderMiddleware': 543,<br>\n",
    "    'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,<br>\n",
    "}\n",
    "\n",
    "### 3.2、download方法详解<br>\n",
    "　　DownloaderMiddlewareManager的download方法，在内部实现了process_request、process_response、process_exception三个方法，用于调用各个中间件的同名方法对请求、响应、异常的处理。此三个方法的处理逻辑如下图：<br>\n",
    "　　![下载中间件对请求、响应、异常的处理流程图](./images/scrapy_middleware流程图.png)\n",
    "  \n",
    "|方法|可返回类型|处理流程|\n",
    "|:---|:---|:---|\n",
    "|process_request(self, request, spider)|1、None<br>2、Response<br>3、Request<br>4、Raise IgnoreRequest|1、返回None的处理流程<br>2、返回Response的处理流程<br>3、返回Request的处理流程<br>4、返回Exception的处理流程|\n",
    "|process_response(self, request, response,spider)|A、Response<br>B、Request<br>C、Raise IgnoreRequest|A、返回Response的处理流程<br>B、返回Request的处理流程<br>C、返回Exception的处理流程<br>|\n",
    "|process_exception(self, request, exception, spider)|i、None<br>ii、Response<br>iii、Request|i、返回None的处理流程<br>ii、返回Response的处理流程<br>iii、返回Request的处理流程<br>|\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 3.2.1、process_request(self, request, spider)\n",
    "　　返回值：<br>\n",
    "　　1、返回None：Scrapy将请求传给后面的中间件的process_request函数继续处理此请求，直到最后。<br>\n",
    "　　2、返回 Response 对象：Scrapy将不再调用其他process_request()或process_exception()方法，而是直接进入最后一个中间件（倒序处理response）的process_response函数里来处理该Response对象，所有已安装的中间件的process_response()方法都会被调用。<br>\n",
    "　　3、返回 Request 对象：Scrapy将停止调用之后的process_request方法并重新处理返回的请求（既进入第一个中间件的process_request）。<br>\n",
    "　　4、引发 IgnoreRequest 异常：已安装的下载中间件的process_exception()方法将被调用（从最后一个开始倒序调用）。如果它们都不处理异常，则调用request()(Request.errback)中的errback函数。如果没有代码处理引发的异常，则会忽略它并且不会记录。<br>\n",
    "　　参数：<br>\n",
    "　　request（Request对象） - 正在处理的请求<br>\n",
    "　　spider（Spider对象） - 此请求所针对的蜘蛛<br>\n",
    "### 3.2.2、process_response(self, request, response,spider)\n",
    "　　返回值：<br>\n",
    "　　1、返回 Response 对象：将响应交给下一个中间件的 process_response 继续处理。也就是对其他中间件没影响。<br>\n",
    "　　2、返回 Request 对象：不再调用 process_response，而是将返回 request 重新加入到调度队列，进而从新从第一个中间件的process_request 处理该请求对象。<br>\n",
    "　　3、引发 IgnoreRequest 异常：注意此时不会进入process_exception函数，而是调用Reuest对象的errorblack回调函数来处理，如果没有定义这个回调则忽略。（已安装的下载中间件的process_exception()方法将被调用。如果它们都不处理异常，则调用request()(Request.errback)中的errback函数。如果没有代码处理引发的异常，则会忽略它并且不会记录。） **不同解释，需考证**<br>\n",
    "　　参数：<br>\n",
    "　　request（Request对象） - 发起响应的请求<br>\n",
    "　　response（Response对象） - 正在处理的响应<br>\n",
    "　　spider（Spider对象） - 此响应所针对的蜘蛛<br>\n",
    "### 3.2.3、process_exception(requset, exception, spider)\n",
    "　　返回值：<br>\n",
    "　　1、返回None，Scrapy将继续处理此异常，执行任何其他已安装中间件的process_exception()方法，直到没有剩下中间件并且默认异常处理开始。<br>\n",
    "　　2、返回 Response 对象：Scrapy不再调用其他中间件的process_exception()方法，而是将响应交从最后一个中间件开始的process_response方法处理。<br>\n",
    "　　3、返回 Request 对象：重新开始处理请求，即重新加入到调度队列，进而重新进入第一个中间件的process_request函数。这个用于失败重复调用时很有用。<br>\n",
    "　　参数：<br>\n",
    "　　request（是一个Request对象） - 生成异常的请求<br>\n",
    "　　exception（一个Exception对象） - 引发的异常<br>\n",
    "　　spider（Spider对象） - 此请求所针对的蜘蛛<br>\n",
    "### 3.2.4 注意\n",
    "　　中间件需要安装才能处理，请记得安装配置中间件的优先级。<br>\n",
    "　　中间件的优先级：数字越小越靠近引擎，数字越大越靠近下载器。请求最先从引擎处开始传递，逐次经过优先级数字越来越大的中间件；相反，下载器返回响应时，则逐次经过优先级数字越来越小的中间件处理。因此，以上图为例：不要认为一个请求最先是中间件1处理的，那么响应也会最先由中间件1来处理，这是错误的。请求最先由优先级数字最小的中间件来处理，而响应最先由优先级数字最大的中间件处理。<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy.core.downloader.middleware.py\n",
    "import six  # 导入six兼容包\n",
    "from twisted.internet import defer   # 导入defer\n",
    "from scrapy.http import Request, Response    # 导入请求和响应\n",
    "from scrapy.middleware import MiddlewareManager\n",
    "from scrapy.utils.defer import mustbe_deferred    # 导入中间件管理 和mustbe_defferred\n",
    "from scrapy.utils.conf import build_component_list    # 导入构建中间件列表函数\n",
    "\n",
    "\n",
    "class DownloaderMiddlewareManager(MiddlewareManager):\n",
    "\n",
    "    component_name = 'downloader middleware'\n",
    "\n",
    "    @classmethod\n",
    "    def _get_mwlist_from_settings(cls, settings):\n",
    "        # 从设置里面获取下载中间件， 然后构建下载中间件列表\n",
    "        return build_component_list(\n",
    "            settings.getwithbase('DOWNLOADER_MIDDLEWARES'))\n",
    "    \n",
    "    def _add_middleware(self, mw):\n",
    "        # 添加下载中间件，获取定义的几个方法，并添加到对应的列表中。 \n",
    "        # 从此处可以看出，我们自定义下载中间件时，就是自定义process_request，process_response，process_exception。 \n",
    "        if hasattr(mw, 'process_request'):\n",
    "            self.methods['process_request'].append(mw.process_request)\n",
    "        if hasattr(mw, 'process_response'):\n",
    "            self.methods['process_response'].insert(0, mw.process_response)\n",
    "        if hasattr(mw, 'process_exception'):\n",
    "            self.methods['process_exception'].insert(0, mw.process_exception)\n",
    "\n",
    "    def download(self, download_func, request, spider):\n",
    "        # download里面定义了3个方法对应上面的三个方法：迭代调用各个列表中的对应方法。 \n",
    "        @defer.inlineCallbacks\n",
    "        def process_request(request):\n",
    "            # 遍历 process_request 的方法列表（获取每个中间件的对应方法）。\n",
    "            # 获取对应方法处理的响应，断言响应不为none,response,request,抛出异常。\n",
    "            # 如果响应不为空的话， 就直接defer.returnValue。\n",
    "            # 遍历完毕方法链之后，使用download_func获取结果。\n",
    "            '''如果在某个Middleware中间件的process_request中处理完之后，生成了一个response对象，那么会直接\n",
    "            将这个response return 出去，跳出循环，不再处理其他的process_request，之前我们的header，proxy中\n",
    "            间件，都只是加个user-agent，加个proxy，并不做任何return值。还需要注意一点:就是这个return的必须\n",
    "            是Response对象。'''\n",
    "            for method in self.methods['process_request']:\n",
    "                response = yield method(request=request, spider=spider)\n",
    "                assert response is None or isinstance(response, (Response, Request)), \\\n",
    "                        'Middleware %s.process_request must return None, Response or Request, got %s' % \\\n",
    "                        (six.get_method_self(method).__class__.__name__, response.__class__.__name__)\n",
    "                if response:\n",
    "                    defer.returnValue(response)\n",
    "            '''如果在上面的所有process_request中，都没有返回任何Response对象的话,最后会将这个加工过的Request送\n",
    "            往download_func，进行下载，返回的就是一个Response对象,然后依次经过各个Middleware中间件的\n",
    "            process_response方法进行加工'''\n",
    "            defer.returnValue((yield download_func(request=request,spider=spider))) # 回调_enqueue_request传入request\n",
    "\n",
    "        @defer.inlineCallbacks\n",
    "        def process_response(response):\n",
    "            # 断言响应为none,抛出异常。\n",
    "            # 判定响应，如果响应是请求，调用defer.returnValue处理。\n",
    "            # 遍历 process_response 的方法列表（获取每个中间件的对应方法）。\n",
    "            # 断言响应结果非 Response or Request， 抛出异常。 如果响应是request的话，调用defer.returnValue处理。\n",
    "            # 最后使用defer.returnValue处理响应。 \n",
    "            assert response is not None, 'Received None in process_response'\n",
    "            if isinstance(response, Request):\n",
    "                defer.returnValue(response)\n",
    "\n",
    "            for method in self.methods['process_response']:\n",
    "                response = yield method(request=request, response=response,\n",
    "                                        spider=spider)\n",
    "                assert isinstance(response, (Response, Request)), \\\n",
    "                    'Middleware %s.process_response must return Response or Request, got %s' % \\\n",
    "                    (six.get_method_self(method).__class__.__name__, type(response))\n",
    "                if isinstance(response, Request):\n",
    "                    defer.returnValue(response)\n",
    "            defer.returnValue(response)\n",
    "\n",
    "        @defer.inlineCallbacks\n",
    "        def process_exception(_failure):\n",
    "            # 遍历 process_exception 的方法列表（获取每个中间件的对应方法），处理异常。\n",
    "            # 断言响应结果非 Response or Request，抛出异常。\n",
    "            # 返回失败值。\n",
    "            exception = _failure.value\n",
    "            for method in self.methods['process_exception']:\n",
    "                response = yield method(request=request, exception=exception,\n",
    "                                        spider=spider)\n",
    "                assert response is None or isinstance(response, (Response, Request)), \\\n",
    "                    'Middleware %s.process_exception must return None, Response or Request, got %s' % \\\n",
    "                    (six.get_method_self(method).__class__.__name__, type(response))\n",
    "                if response:\n",
    "                    defer.returnValue(response)\n",
    "            defer.returnValue(_failure)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
