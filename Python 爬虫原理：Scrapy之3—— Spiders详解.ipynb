{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiders详解\n",
    "　　参考：[]()<BR>    \n",
    "　　Scrapy提供了Spider、CrawlSpider (XMLFeedSpider、CSVFeedSpider、SitemapSpider此处未做注解)。<br>\n",
    "　　Scrapy_redis提供了RedisSpider、RedisCrawlSpider爬虫采集类。<br>\n",
    "### 一、Spider源码详解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'object_ref' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-205f70ebf3ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mSpider\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mclass\u001b[0m\u001b[1;33m.\u001b[0m                                                                 \u001b[0m所有的Scrapy的爬虫类必须继承Spider\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'object_ref' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class object_ref(object):\n",
    "    \"\"\"Inherit from this class (instead of object) to a keep a record of live\n",
    "    instances\"\"\"\n",
    "\n",
    "    __slots__ = ()\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        obj = object.__new__(cls)\n",
    "        live_refs[cls][obj] = time()    # 记录爬虫存活时间\n",
    "        return obj\n",
    "\n",
    "    \n",
    "class Spider(object_ref):\n",
    "    \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n",
    "    class.                                                                 所有的Scrapy的爬虫类必须继承Spider\n",
    "    \"\"\"\n",
    "\n",
    "    name = None   # 定义爬虫的名字\n",
    "    custom_settings = None   # 定义爬虫自己的配置参数（最高优先级），是专属于Spider的配置，此方法会覆盖全局的配置，\n",
    "    # 此设置必须在初始化前被更新，必须定义成类变量。\n",
    "\n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        if name is not None:\n",
    "            self.name = name   # 定义爬虫给名字\n",
    "        elif not getattr(self, 'name', None):\n",
    "            raise ValueError(\"%s must have a name\" % type(self).__name__)\n",
    "        self.__dict__.update(kwargs)\n",
    "        if not hasattr(self, 'start_urls'):\n",
    "            self.start_urls = []   # 初始化爬虫开始的urls\n",
    "\n",
    "    @property\n",
    "    def logger(self):   # 构造爬虫的日志器\n",
    "        logger = logging.getLogger(self.name)\n",
    "        return logging.LoggerAdapter(logger, {'spider': self})\n",
    "\n",
    "    def log(self, message, level=logging.DEBUG, **kw):   # 日志记录： msg 日志级别\n",
    "        \"\"\"Log the given message at the given log level\n",
    "\n",
    "        This helper wraps a log call to the logger within the spider, but you\n",
    "        can use it directly (e.g. Spider.logger.info('msg')) or use any other\n",
    "        Python logger too.\n",
    "        \"\"\"\n",
    "        self.logger.log(level, message, **kw)\n",
    "\n",
    "    @classmethod    # 修饰符对应的函数不需要实例化，不需要 self 参数，但第一个参数需要是表示自身类的 cls 参数，可以来调用类的属性，类的方法，实例化对象等。\n",
    "    def from_crawler(cls, crawler, *args, **kwargs):   # args、kwargs 为传递给init的参数  此方法和Pipeline里面使用是一样的。\n",
    "        spider = cls(*args, **kwargs)\n",
    "        spider._set_crawler(crawler)\n",
    "        return spider\n",
    "\n",
    "    def _set_crawler(self, crawler):   # 初始化crawler、settings、爬虫关闭信号\n",
    "        self.crawler = crawler    # 代表本Spider对应的Crawler对象，包含了许多项目组件。可以利用它来获取项目中的一些配置信息，最常见的就是从settings.py里面获取项目的配置信息。\n",
    "        self.settings = crawler.settings\n",
    "        crawler.signals.connect(self.close, signals.spider_closed)\n",
    "\n",
    "    def start_requests(self):         # 爬虫请求入口，用于生成初始请求，它必须必须返回一个可迭代对象。\n",
    "        cls = self.__class__\n",
    "        if method_is_overridden(cls, Spider, 'make_requests_from_url'):\n",
    "            warnings.warn(     # make_requests_from_url 方法被启用，建议重写 start_requests 方法\n",
    "                \"Spider.make_requests_from_url method is deprecated; it \"\n",
    "                \"won't be called in future Scrapy releases. Please \"\n",
    "                \"override Spider.start_requests method instead (see %s.%s).\" % (\n",
    "                    cls.__module__, cls.__name__\n",
    "                ),\n",
    "            )\n",
    "            for url in self.start_urls:\n",
    "                yield self.make_requests_from_url(url)\n",
    "        else:\n",
    "            for url in self.start_urls:\n",
    "                yield Request(url, dont_filter=True)    # 默认使用 self.parse 函数解析\n",
    "\n",
    "    def make_requests_from_url(self, url):\n",
    "        \"\"\" This method is deprecated. \"\"\"     # 方法被弃用\n",
    "        return Request(url, dont_filter=True)\n",
    "\n",
    "    def parse(self, response):     # 必须在自定义爬虫文件中实现的解析函数\n",
    "        # 该方法及其他的Request回调函数必须返回一个包含 Request、dict 或 Item 的可迭代的对象。\n",
    "        raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))\n",
    "\n",
    "    @classmethod\n",
    "    def update_settings(cls, settings):\n",
    "        settings.setdict(cls.custom_settings or {}, priority='spider')\n",
    "\n",
    "    @classmethod\n",
    "    def handles_request(cls, request):   # 判断请求的 url 是否符合 allowed_domains，不符合的请求不发送\n",
    "        return url_is_from_spider(request.url, cls)\n",
    "\n",
    "    @staticmethod   # 静态方法，当spider关闭时，该函数被调用\n",
    "    def close(spider, reason):   # 从close的源码可以看出，如果需要在爬虫结束的时候进行一些操作，那么就可以通过改写 close 方法，\n",
    "        # 或者在编写的爬虫类中实现 closed 方法。\n",
    "        closed = getattr(spider, 'closed', None)\n",
    "        if callable(closed):\n",
    "            return closed(reason)\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<%s %r at 0x%0x>\" % (type(self).__name__, self.name, id(self))\n",
    "\n",
    "    __repr__ = __str__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二、CrawlSpider源码详解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrawlSpider(Spider):            # 通用爬虫类\n",
    "\n",
    "    rules = ()                  # 链接提取处理规则类\n",
    "\n",
    "    def __init__(self, *a, **kw):\n",
    "        super(CrawlSpider, self).__init__(*a, **kw)\n",
    "        self._compile_rules()      # 实列话各个 链接提取处理规则类\n",
    "\n",
    "    def parse(self, response):\n",
    "        return self._parse_response(response, self.parse_start_url, cb_kwargs={}, follow=True)\n",
    "\n",
    "    def parse_start_url(self, response):\n",
    "        return []    # start_url首页的页面解析：返回一个包含 Request、dict 或 Item 的可迭代的对象。\n",
    "\n",
    "    def process_results(self, response, results):\n",
    "        return results\n",
    "\n",
    "    def _build_request(self, rule, link):\n",
    "        r = Request(url=link.url, callback=self._response_downloaded)    # 根据链接提取器提取的链接 构建请求以跟进\n",
    "        r.meta.update(rule=rule, link_text=link.text)      # 记录 链接提取器 rule:  meta['rule']为整数，是在self.rules中的index值\n",
    "        return r\n",
    "\n",
    "    def _requests_to_follow(self, response):\n",
    "        if not isinstance(response, HtmlResponse):\n",
    "            return\n",
    "        seen = set()    # 链接过滤\n",
    "        for n, rule in enumerate(self._rules):       # 提取连接，返回链接的请求\n",
    "            links = [lnk for lnk in rule.link_extractor.extract_links(response)\n",
    "                     if lnk not in seen]\n",
    "            if links and rule.process_links:\n",
    "                links = rule.process_links(links)\n",
    "            for link in links:\n",
    "                seen.add(link)\n",
    "                request = self._build_request(n, link)\n",
    "                yield rule._process_request(request, response)\n",
    "\n",
    "    def _response_downloaded(self, response):\n",
    "        rule = self._rules[response.meta['rule']]      # 链接跟进的回调函数的配置\n",
    "        return self._parse_response(response, rule.callback, rule.cb_kwargs, rule.follow)\n",
    "\n",
    "    def _parse_response(self, response, callback, cb_kwargs, follow=True):\n",
    "        if callback:\n",
    "            cb_res = callback(response, **cb_kwargs) or ()      # 跟进连接的解析处理  生成器\n",
    "            cb_res = self.process_results(response, cb_res)       # 结果处理函数  生成器\n",
    "            for requests_or_item in iterate_spider_output(cb_res):\n",
    "                yield requests_or_item\n",
    "\n",
    "        if follow and self._follow_links:\n",
    "            for request_or_item in self._requests_to_follow(response):   # 继续跟进链接\n",
    "                yield request_or_item\n",
    "\n",
    "    def _compile_rules(self):\n",
    "        self._rules = [copy.copy(r) for r in self.rules]              # 浅拷贝\n",
    "        for rule in self._rules:                            # 初始化各个rule实例\n",
    "            rule._compile(self)\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler, *args, **kwargs):\n",
    "        spider = super(CrawlSpider, cls).from_crawler(crawler, *args, **kwargs)\n",
    "        spider._follow_links = crawler.settings.getbool(\n",
    "            'CRAWLSPIDER_FOLLOW_LINKS', True)     # 设置是否跟进连接， 默认跟进\n",
    "        return spider\n",
    "\n",
    "    \n",
    "class Rule(object):      # 链接提取处理类\n",
    "\n",
    "    def __init__(self, link_extractor=None, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None):\n",
    "        self.link_extractor = link_extractor or _default_link_extractor     # 链接提取规则\n",
    "        self.callback = callback   # 链接处理回调函数\n",
    "        self.cb_kwargs = cb_kwargs or {}\n",
    "        self.process_links = process_links   # 链接处理函数\n",
    "        self.process_request = process_request or _identity    # 连接跟进请求处理函数\n",
    "        self.process_request_argcount = None\n",
    "        self.follow = follow if follow is not None else not callback    # 链接是否跟进设置\n",
    "\n",
    "    def _compile(self, spider):              # 初始化链接处理对象规则\n",
    "        self.callback = _get_method(self.callback, spider)   # 设置回调函数\n",
    "        self.process_links = _get_method(self.process_links, spider)   # 设置链接处理函数\n",
    "        self.process_request = _get_method(self.process_request, spider)   # 设置连接请求处理函数\n",
    "        self.process_request_argcount = len(get_func_args(self.process_request))    # 判断 请求处理函数参数设置对错\n",
    "        if self.process_request_argcount == 1:\n",
    "            msg = 'Rule.process_request should accept two arguments (request, response), accepting only one is deprecated'\n",
    "            warnings.warn(msg, category=ScrapyDeprecationWarning, stacklevel=2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三、RedisSpider源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisSpider(RedisMixin, Spider):\n",
    "    \"\"\"Spider that reads urls from redis queue when idle.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    redis_key : str (default: REDIS_START_URLS_KEY)    # 获取任务 redis key  字符串\n",
    "        Redis key where to fetch start URLs from..\n",
    "    redis_batch_size : int (default: CONCURRENT_REQUESTS)    # 一次从redis获取的任务数量：默认为spider的并发量CONCURRENT_REQUESTS\n",
    "        Number of messages to fetch from redis on each attempt.\n",
    "    redis_encoding : str (default: REDIS_ENCODING)     # reids数据的编码\n",
    "        Encoding to use when decoding messages from redis queue.\n",
    "\n",
    "    Settings\n",
    "    --------\n",
    "    REDIS_START_URLS_KEY : str (default: \"<spider.name>:start_urls\")     # settings设置：设置获取任务的redis key  字符串\n",
    "        Default Redis key where to fetch start URLs from..\n",
    "    REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS)    # 可从一次从redis获取的任务数量（\n",
    "        Default number of messages to fetch from redis on each attempt.\n",
    "    REDIS_START_URLS_AS_SET : bool (default: False)                             # redis 任务队列使用集合还是列表，默认列表\n",
    "        Use SET operations to retrieve messages from the redis queue. If False,\n",
    "        the messages are retrieve using the LPOP command.\n",
    "    REDIS_ENCODING : str (default: \"utf-8\")                          # 默认的redis编码\n",
    "        Default encoding to use when decoding messages from redis queue.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(self, crawler, *args, **kwargs):\n",
    "        obj = super(RedisSpider, self).from_crawler(crawler, *args, **kwargs)\n",
    "        obj.setup_redis(crawler)              # 初始化redis链接\n",
    "        return obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 四、RedisCrawlSpider源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisCrawlSpider(RedisMixin, CrawlSpider):\n",
    "    \"\"\"Spider that reads urls from redis queue when idle.\n",
    "\n",
    "    Attributes                     # 同RedisSpider\n",
    "    ----------\n",
    "    redis_key : str (default: REDIS_START_URLS_KEY)\n",
    "        Redis key where to fetch start URLs from..\n",
    "    redis_batch_size : int (default: CONCURRENT_REQUESTS)\n",
    "        Number of messages to fetch from redis on each attempt.\n",
    "    redis_encoding : str (default: REDIS_ENCODING)\n",
    "        Encoding to use when decoding messages from redis queue.\n",
    "\n",
    "    Settings\n",
    "    --------\n",
    "    REDIS_START_URLS_KEY : str (default: \"<spider.name>:start_urls\")\n",
    "        Default Redis key where to fetch start URLs from..\n",
    "    REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS)\n",
    "        Default number of messages to fetch from redis on each attempt.\n",
    "    REDIS_START_URLS_AS_SET : bool (default: True)\n",
    "        Use SET operations to retrieve messages from the redis queue.\n",
    "    REDIS_ENCODING : str (default: \"utf-8\")\n",
    "        Default encoding to use when decoding messages from redis queue.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(self, crawler, *args, **kwargs):\n",
    "        obj = super(RedisCrawlSpider, self).from_crawler(crawler, *args, **kwargs)\n",
    "        obj.setup_redis(crawler)   # 初始化redis链接\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 五、RedisMixin 源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisMixin(object):\n",
    "    \"\"\"Mixin class to implement reading urls from a redis queue.\"\"\"\n",
    "    redis_key = None       # redis  任务的redis key  字符串\n",
    "    redis_batch_size = None\n",
    "    redis_encoding = None\n",
    "\n",
    "    # Redis client placeholder.\n",
    "    server = None      # redis连接服务\n",
    "\n",
    "    def start_requests(self):\n",
    "        \"\"\"Returns a batch of start requests from redis.\"\"\"\n",
    "        return self.next_requests()     # 返回 batch_size 个请求任务\n",
    "\n",
    "    def setup_redis(self, crawler=None):    # 初始化redis链接  与 redis任务空闲信号\n",
    "        \"\"\"Setup redis connection and idle signal.\n",
    "\n",
    "        This should be called after the spider has set its crawler object.\n",
    "        \"\"\"\n",
    "        if self.server is not None:\n",
    "            return\n",
    "\n",
    "        if crawler is None:\n",
    "            # We allow optional crawler argument to keep backwards\n",
    "            # compatibility.\n",
    "            # XXX: Raise a deprecation warning.\n",
    "            crawler = getattr(self, 'crawler', None)\n",
    "\n",
    "        if crawler is None:\n",
    "            raise ValueError(\"crawler is required\")\n",
    "\n",
    "        settings = crawler.settings             # 获取爬虫设置\n",
    "        if self.redis_key is None:\n",
    "            self.redis_key = settings.get(\n",
    "                'REDIS_START_URLS_KEY', defaults.START_URLS_KEY,\n",
    "            )\n",
    "\n",
    "        self.redis_key = self.redis_key % {'name': self.name}   # redis  key 有spider name 与 key 字符串组成\n",
    "\n",
    "        if not self.redis_key.strip():\n",
    "            raise ValueError(\"redis_key must not be empty\")\n",
    "\n",
    "        if self.redis_batch_size is None:\n",
    "            # TODO: Deprecate this setting (REDIS_START_URLS_BATCH_SIZE).\n",
    "            self.redis_batch_size = settings.getint(\n",
    "                'REDIS_START_URLS_BATCH_SIZE',\n",
    "                settings.getint('CONCURRENT_REQUESTS'),     # 默认self.redis_batch_size设置为：CONCURRENT_REQUESTS\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            self.redis_batch_size = int(self.redis_batch_size)\n",
    "        except (TypeError, ValueError):\n",
    "            raise ValueError(\"redis_batch_size must be an integer\")\n",
    "\n",
    "        if self.redis_encoding is None:\n",
    "            self.redis_encoding = settings.get('REDIS_ENCODING', defaults.REDIS_ENCODING)\n",
    "\n",
    "        self.logger.info(\"Reading start URLs from redis key '%(redis_key)s' \"\n",
    "                         \"(batch size: %(redis_batch_size)s, encoding: %(redis_encoding)s\",\n",
    "                         self.__dict__)\n",
    "\n",
    "        self.server = connection.from_settings(crawler.settings)           # 创建redis链接\n",
    "        # The idle signal is called when the spider has no requests left,\n",
    "        # that's when we will schedule new requests from redis queue\n",
    "        crawler.signals.connect(self.spider_idle, signal=signals.spider_idle)    # 绑定redis空闲信号\n",
    "\n",
    "    def next_requests(self):\n",
    "        \"\"\"Returns a request to be scheduled or none.\"\"\"\n",
    "        use_set = self.settings.getbool('REDIS_START_URLS_AS_SET', defaults.START_URLS_AS_SET)   # 判断 redis任务使用的是set 还是 list\n",
    "        fetch_one = self.server.spop if use_set else self.server.lpop    # 根据 任务数据结构选择  任务获取方法\n",
    "        # XXX: Do we need to use a timeout here?\n",
    "        found = 0\n",
    "        # TODO: Use redis pipeline execution.\n",
    "        while found < self.redis_batch_size:\n",
    "            data = fetch_one(self.redis_key)    # 从redis 获取任务\n",
    "            if not data:\n",
    "                # Queue empty.\n",
    "                break\n",
    "            req = self.make_request_from_data(data)       # 根据任务构造 请求\n",
    "            if req:\n",
    "                yield req\n",
    "                found += 1\n",
    "            else:\n",
    "                self.logger.debug(\"Request not made from data: %r\", data)\n",
    "\n",
    "        if found:\n",
    "            self.logger.debug(\"Read %s requests from '%s'\", found, self.redis_key)\n",
    "\n",
    "    def make_request_from_data(self, data):\n",
    "        \"\"\"Returns a Request instance from data coming from Redis.\n",
    "\n",
    "        By default, ``data`` is an encoded URL. You can override this method to\n",
    "        provide your own message decoding.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : bytes\n",
    "            Message from redis.\n",
    "\n",
    "        \"\"\"\n",
    "        url = bytes_to_str(data, self.redis_encoding)     # 处理redis任务数据的编码格式\n",
    "        return self.make_requests_from_url(url)\n",
    "\n",
    "    def schedule_next_requests(self):\n",
    "        \"\"\"Schedules a request if available\"\"\"\n",
    "        # TODO: While there is capacity, schedule a batch of redis requests.\n",
    "        for req in self.next_requests():                         # 爬虫spider空闲时，调用该方法，继续从redis获取新的任务\n",
    "            self.crawler.engine.crawl(req, spider=self)\n",
    "\n",
    "    def spider_idle(self):\n",
    "        \"\"\"Schedules a request if available, otherwise waits.\"\"\"\n",
    "        # XXX: Handle a sentinel to close the spider.\n",
    "        self.schedule_next_requests()                             # 爬虫空闲信号的绑定处理方法\n",
    "        raise DontCloseSpider\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
