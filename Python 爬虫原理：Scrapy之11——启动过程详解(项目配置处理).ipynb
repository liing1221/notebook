{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spiders详解\n",
    "　　参考：[scrapy 源码解析 （一）：启动流程源码分析(一)命令行启动](https://www.cnblogs.com/qiu-hua/p/12930422.html)<BR>\n",
    "    \n",
    "### 一、概述  \n",
    "　　先不谈使用了各种框架的复杂情况，如scrapyd服务、redis分布式队列等。只看最简单的情况，假设只写了一个简单爬虫myspider。<BR>\n",
    "　　输入命令,运行myspider爬虫：<BR>\n",
    ">scrapy crawl myspider\n",
    "    \n",
    "　　那么，这个过程到底发生了什么？ scrapy命令与crawl、myspider参数从何而来？命令是如何执行的呢？<BR>\n",
    "### 二、Scrapy命令\n",
    "#### 2.1、命令里的scrapy是一个可执行文件，后面的crawl myspider是scrapy的参数。<BR>\n",
    "　　可执行文件scrapy在/usr/local/python/bin目录里（linux系统使用which scrapy查看），是一个python脚本，有效代码为：<BR>\n",
    ">from scrapy.cmdline import execute<BR>\n",
    "<BR>\n",
    "if __name__ == '__main__':<BR>\n",
    "　　sys.argv[0] = re.sub(r'(-script\\.pyw?|\\.exe)?$', '', sys.argv\\[0])<BR>\n",
    "　　sys.exit(execute())<BR>\n",
    "\n",
    "　　这个文件的作用就是从命令行里读取命令，然后传递给scrapy.cmdline包的execute()方法进行下一步操作（scrapy项目启动入口点）。<BR>\n",
    "#### 2.2 scrapy命令的生成原理\n",
    "　　安装scrapy后, scrapy的setup.py文件生明并指定了程序的入口点为：scrapy.cmdline.execute方法。  \n",
    "### 二、execute方法\n",
    "从python的第三方库目录里找到scrapy/cmdline.py文件，可以看到代码中有execute()方法：<BR>\n",
    ">def execute(argv=None, settings=None):<BR>\n",
    "　　if argv is None:<BR>\n",
    "　　　　argv = sys.argv<BR>\n",
    "    <BR>        \n",
    "　　if settings is None:<BR>\n",
    "　　　　settings = get_project_settings()<BR>\n",
    "　　　　# set EDITOR from environment if available<BR>\n",
    "　　　　try:<BR>\n",
    "　　　　　　editor = os.environ['EDITOR']<BR>\n",
    "　　　　except KeyError: pass<BR>\n",
    "　　　　else:<BR>\n",
    "　　　　　　settings['EDITOR'] = editor<BR>\n",
    "　　check_deprecated_settings(settings)<BR>\n",
    "  <BR>\n",
    "　　inproject = inside_project()<BR>\n",
    "　　cmds = _get_commands_dict(settings, inproject)<BR>\n",
    "　　cmdname = _pop_command_name(argv)<BR>\n",
    "　　parser = optparse.OptionParser(formatter=optparse.TitledHelpFormatter(), \\<BR>\n",
    "　　　　conflict_handler='resolve')<BR>\n",
    "　　if not cmdname:<BR>\n",
    "　　　　_print_commands(settings, inproject)<BR>\n",
    "　　　　sys.exit(0)<BR>\n",
    "　　elif cmdname not in cmds:<BR>\n",
    "　　　　_print_unknown_command(settings, cmdname, inproject)<BR>\n",
    "　　　　sys.exit(2)<BR>\n",
    "    <BR>\n",
    "　　cmd = cmds[cmdname]<BR>\n",
    "　　parser.usage = \"scrapy %s %s\" % (cmdname, cmd.syntax())<BR>\n",
    "　　parser.description = cmd.long_desc()<BR>\n",
    "　　settings.setdict(cmd.default_settings, priority='command')<BR>\n",
    "　　cmd.settings = settings<BR>\n",
    "　　cmd.add_options(parser)<BR>\n",
    "　　opts, args = parser.parse_args(args=argv[1:])<BR>\n",
    "　　_run_print_help(parser, cmd.process_options, args, opts)<BR>\n",
    "  <BR>\n",
    "　　cmd.crawler_process = CrawlerProcess(settings)<BR>\n",
    "　　_run_print_help(parser, _run_command, cmd, args, opts)<BR>\n",
    "　　sys.exit(cmd.exitcode)<BR>\n",
    "  <BR>    \n",
    "def _run_command(cmd, args, opts):<BR>\n",
    "　　if opts.profile:<BR>\n",
    "　　　　_run_command_profiled(cmd, args, opts)<BR>\n",
    "　　else:<BR>\n",
    "　　　　cmd.run(args, opts)<BR>\n",
    "\n",
    "**execute方法中的重要操作有：**<BR>\n",
    "　　1.执行get_project_settings()方法，导入全局配置文件scrapy.cfg，进而导入项目的settings .py。这里用到了scrapy/utils/project.py中的get_project_settings()方法，这里先不展开。<BR>\n",
    "　　2.解析命令行scrapy之后的命令，并把命令中的配置项导入到settings中，中间的一大段代码就是执行此功能，每个有效的命令均对应scrapy/commands中以命令命名的文件中的Command类。<BR>\n",
    "　　3.cmd.crawler_process = CrawlerProcess(settings)这句代码建立CrawlerProcess对象，并把所有的设置settings传递给此对象。<BR>\n",
    "　　4._run_print_help(parser, _run_command, cmd, args, opts)调用_run_command执行命令。如果指定了profile命令行参数，则用cProfile运行命令，cProfile是一个标准模块，这里不必考虑。无论如何，最后都会执行命令的run()方法，也就是scrapy/commands/crawl.py中Command类的run()方法。<BR>\n",
    "\n",
    "### 三、执行crawl命令\n",
    "　　scrapy/commands/crawl.py#Command:\n",
    ">def run(self, args, opts):<BR>\n",
    "　　if len(args) < 1:<BR>\n",
    "　　　　raise UsageError()<BR>\n",
    "　　elif len(args) > 1:<BR>\n",
    "　　　　raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")<BR>\n",
    "　　spname = args[0]<BR>\n",
    "<BR>\n",
    "　　self.crawler_process.crawl(spname, **opts.spargs)<BR>\n",
    "　　self.crawler_process.start()<BR>\n",
    "\n",
    "**这里有2个重要操作：**\n",
    "　　1.调用CrawlerProcess的crawl方法，执行初始化（创建新爬虫对象）。<BR>\n",
    "　　2.调用CrawlerProcess的start方法，正式运行。<BR>\n",
    "CrawlerProcess便是scrapy运行过程中最根本的进程，是所有爬虫运行的基础。<BR>\n",
    "\n",
    "### 四、导入配置文件\n",
    "　　get_project_settings()导入配置文件的过程<BR>\n",
    "　　scrapy/utils/project.py:<BR>\n",
    ">...<BR>\n",
    "ENVVAR = 'SCRAPY_SETTINGS_MODULE'<BR>\n",
    "...<BR>\n",
    "def get_project_settings():<BR>\n",
    "　　if ENVVAR not in os.environ:<BR>\n",
    "　　　　project = os.environ.get('SCRAPY_PROJECT', 'default')<BR>\n",
    "　　　　init_env(project)<BR>\n",
    "<BR>\n",
    "　　settings = Settings()<BR>\n",
    "　　settings_module_path = os.environ.get(ENVVAR)<BR>\n",
    "　　if settings_module_path:<BR>\n",
    "　　　　settings.setmodule(settings_module_path, priority='project')<BR>\n",
    "<BR>\n",
    "　　# XXX: remove this hack<BR>\n",
    "　　pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")<BR>\n",
    "　　if pickled_settings:<BR>\n",
    "　　　　settings.setdict(pickle.loads(pickled_settings), priority='project')<BR>\n",
    "<BR>\n",
    "　　# XXX: deprecate and remove this functionality<BR>\n",
    "　　env_overrides = {k[7:]: v for k, v in os.environ.items() if<BR>\n",
    "　　　　　　　　　　　k.startswith('SCRAPY_')}<BR>\n",
    "　　if env_overrides:<BR>\n",
    "　　　　settings.setdict(env_overrides, priority='project')<BR>\n",
    "\n",
    "　　return settings<BR>\n",
    "\n",
    "　　get_project_settings会首先判断是否设置了SCRAPY_SETTINGS_MODULE环境变量，这个环境变量用来指定工程的配置模块。如果没有这个环境变量，则会调用init_env来初始化环境变量，由于我们没有设置SCRAPY_PROJECT，所以会用default默认值来执行init_env(在scrapy/utils/conf.py)。<BR>\n",
    "  \n",
    "### 五、init_env初始化环境变量\n",
    "\n",
    "　　scrapy/utils/conf.py:<BR>\n",
    ">def init_env(project='default', set_syspath=True):<BR>\n",
    "　　\"\"\"Initialize environment to use command-line tool from inside a project<BR>\n",
    "　　dir. This sets the Scrapy settings module and modifies the Python path to<BR>\n",
    "　　be able to locate the project module.<BR>\n",
    "　　\"\"\"<BR>\n",
    "　　cfg = get_config()<BR>\n",
    "　　if cfg.has_option('settings', project):<BR>\n",
    "　　　　os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('settings', project)<BR>\n",
    "　　closest = closest_scrapy_cfg()<BR>\n",
    "　　if closest:<BR>\n",
    "　　　　projdir = os.path.dirname(closest)<BR>\n",
    "　　　　if set_syspath and projdir not in sys.path:<BR>\n",
    "　　　　　　sys.path.append(projdir)<BR>\n",
    "<BR>\n",
    "def get_config(use_closest=True):<BR>\n",
    "　　\"\"\"Get Scrapy config file as a SafeConfigParser\"\"\"<BR>\n",
    "　　sources = get_sources(use_closest)<BR>\n",
    "　　cfg = SafeConfigParser()<BR>\n",
    "　　cfg.read(sources)<BR>\n",
    "　　return cfg<BR>\n",
    "<BR>\n",
    "def get_sources(use_closest=True):<BR>\n",
    "　　xdg_config_home = os.environ.get('XDG_CONFIG_HOME') or \\\\<BR>\n",
    "　　　　os.path.expanduser('\\~/.config')<BR>\n",
    "　　sources = ['/etc/scrapy.cfg', r'c:\\scrapy\\scrapy.cfg',<BR>\n",
    "　　　　　　　　xdg_config_home + '/scrapy.cfg',<BR>\n",
    "　　　　　　　　os.path.expanduser('~/.scrapy.cfg')]<BR>\n",
    "　　if use_closest:<BR>\n",
    "　　　　sources.append(closest_scrapy_cfg())<BR>\n",
    "　　return sources<BR>\n",
    "\n",
    "init_env首先调用get_config()获取cfg配置文件，这个配置文件获取的优先级是：<BR>\n",
    "　　1./etc/scrapy.cfg，c:\\scrapy\\scrapy.cfg<BR>\n",
    "　　2.XDG_CONFIG_HOME环境变量指定的目录下的scrapy.cfg<BR>\n",
    "　　3.~/.scrapy.cfg<BR>\n",
    "　　4.当前执行目录下的scrapy.cfg或者父目录中的scrapy.cfg<BR>\n",
    "由于1，2，3默认都不设置，所以就使用当前执行命令下的scrapy.cfg，也就是工程目录下的scrapy.cfg。这个文件的内容很简单：<BR>\n",
    ">[settings]<BR>\n",
    "default = myspider.settings<BR>\n",
    "[deploy]<BR>\n",
    "#url = http://localhost:6800/<BR>\n",
    "project = myspider<BR>\n",
    "\n",
    "根据default = myspider.settings找到对应的配置模块，后面会执行一系列导入settings.py配置项的操作，过程其实很复杂，这里不再详细说了。\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python36]",
   "language": "python",
   "name": "conda-env-python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
