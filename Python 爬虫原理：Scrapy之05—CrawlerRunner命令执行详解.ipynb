{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CrawlerRunner命令执行详解\n",
    "　　参考：[Scrapy 源码阅读（二）：看源码](https://zhuanlan.zhihu.com/p/150120517)<br>\n",
    "　　　　　[Scrapy的抓取流程——CrawlerProcess](https://blog.csdn.net/okm6666/article/details/89160886)<br>\n",
    "　　　　　[通过核心ＡＰＩ启动单个或多个scrapy爬虫](https://www.jianshu.com/p/add5c59d698a)<br>\n",
    "　　　　　[Scrapy进阶-命令行的工作原理（以runspider为例）](https://www.jianshu.com/p/8e252b2272d8)<br>\n",
    "　　　　　[同时运行多个scrapy爬虫的几种方法（自定义scrapy项目命令）](https://www.cnblogs.com/rwxwsblog/p/4578764.html)<br>\n",
    "　　　　　[scrapy项目下spiders内多个爬虫同时运行](https://blog.csdn.net/beyond_f/article/details/74626451)<br>\n",
    "　　　　　[python scrapy项目下spiders内多个爬虫同时运行](https://blog.csdn.net/qq_38282706/article/details/80977576)<br>\n",
    "　　　　　[scrapy启动流程源码分析(2)CrawlerProcess主进程](https://blog.csdn.net/csdn_yym/article/details/85423656)<br>\n",
    "   \n",
    "## 一、简介\n",
    "　　从上一节，我们知道scrapy命令执行时，会配置项目环境、解析命令行、启动一个主进程运行爬虫任务。本节讨论进程启动后，爬虫任务是如何执行的？<br>\n",
    "　　CrawlerProcess是CrawlerRunner的子类，它控制了twisted的reactor(wisted中的reactor相当于asyncio中loop，deferred相当于 future)，也就是整个事件循环。它负责配置reactor并启动事件循环，最后在所有爬取结束后停止reactor。另外还控制了一些信号操作，使用户可以手动终止爬取任务。<br>\n",
    "　　此类在scrapy/crawler.py中定义，此模块有三个类：Crawler、CrawlerRunner和CrawlerProcess。<br>\n",
    "　　Crawler是实际执行爬取的类，并管理了自身的启停，接受控制信号和setting配置等，其实例，里面使用一种spider，代表了一个爬取任务。CrawlerRunner 是对crawler的调度，其需要twised框架。CrawlerProcess相当于封装了twisted需要的reactor以后的 CrawlerRunner，可以控制多个Crawler同时进行多种爬取任务。CrawlerProcess通过实现start方法来启动一个Twisted的reactor（另有shutdown信号处理、顶层logging功能）。<br>\n",
    "　　由execute()函数通过一系列解析动作,调用 CrawlerProcess对象 的run() 方法执行具体的爬虫任务。这里，CrawlerProcess实例和其内部的Crawler,调用Crawler里的engine实例的 engine.open_spider 做准备工作（scheduler和其他需要对应crawler实例化的东西），然后调用 Crawler里的engine.start开启引擎，最后调用Crawlerprocess.start方法启动reactor。<br>\n",
    "　　爬取正式开始，流程图如下：<br>\n",
    "　　![Crawlerprocess抓取流程图](./images/scrapy_Crawlerprocess流程图.png)\n",
    "  \n",
    "## 二、crawl.py与CrawlerRunner介绍\n",
    "### 2.1、几个重要Class的关系\n",
    "　　\\[CrawlerRunner　\\[Crawler　\\[\\[Spider\\],　\\[ExecutionEngine　\\[spider,slot　\\[scheduler\\],downloader, scraper\\]\\]\\]\\]\\]<br>\n",
    "　　1、Crawler 可以理解为爬虫的一个容器<br>\n",
    "　　2、CrawlerRunner 对 Crawler 做了一些封装，可以让我们更方便的运行爬虫。类似的还有 CrawlerProcess，它是 CrawlerRunner 的子类<br>\n",
    "　　3、Spider 就是我们编写爬虫文件时依赖的类，ExecutionEngine 则是 Scrapy 调度的核心<br>\n",
    "　　4、spider，Crawler 中传递过来的 Spider 对象<br>\n",
    "　　5、slot，插槽，用于请求存储以及调度<br>\n",
    "　　6、scheduler，一般是 scrapy.core.scheduler.Scheduler 的对象<br>\n",
    "　　7、downloader，一般是 crapy.core.downloader.Downloader 的对象<br>\n",
    "　　8、scraper，一般是 scrapy.core.scraper.Scraper 的对象，与 Spider Middleware 和 Item Pipelines 有关。<br>\n",
    "　　**注意：**<br>\n",
    "　　1、spider是程序员编写的爬虫代码模块，一般是存放在项目里spiders文件夹内，并给每个爬虫模块赋予独立的名称，命令行启动时通过不同的名称启动不同的spider；<br>\n",
    "　　2、crawler是爬取任务，每次在命令行启动，都会新建一个新的crawler爬取任务，可以为同一个spider新建多个crawler，表现在命令里就是同样的命令可以重复执行多次，同一个spider对应的多个crawler共同占有同样的私有配置、同一个任务队列。<br>\n",
    "\n",
    "  \n",
    "  \n",
    "## 一、通过自定义scrapy命令的方式来运行\n",
    "　　配置说明：https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/commands.html<br>\n",
    "1、创建commands目录<br>\n",
    ">    mkdir commands<br>\n",
    "\n",
    "　　注意：commands和spiders目录是同级的<br>\n",
    "2、在commands下面添加一个文件crawlall.py<br>\n",
    "　　注意：这里主要通过修改scrapy的crawl命令来完成同时执行spider的效果。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scrapy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-16809395bb44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommands\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mScrapyCommand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCrawlerRunner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marglist_to_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mCommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mScrapyCommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scrapy'"
     ]
    }
   ],
   "source": [
    "from scrapy.commands import ScrapyCommand  \n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.conf import arglist_to_dict\n",
    "\n",
    "class Command(ScrapyCommand):\n",
    "  \n",
    "    requires_project = True\n",
    "  \n",
    "    def syntax(self):  \n",
    "        return '[options]'  \n",
    "  \n",
    "    def short_desc(self):  \n",
    "        return 'Runs all of the spiders'  \n",
    "\n",
    "    def add_options(self, parser):\n",
    "        ScrapyCommand.add_options(self, parser)\n",
    "        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n",
    "                          help=\"set spider argument (may be repeated)\")\n",
    "        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\",\n",
    "                          help=\"dump scraped items into FILE (use - for stdout)\")\n",
    "        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n",
    "                          help=\"format to use for dumping items with -o\")\n",
    "\n",
    "    def process_options(self, args, opts):\n",
    "        ScrapyCommand.process_options(self, args, opts)\n",
    "        try:\n",
    "            opts.spargs = arglist_to_dict(opts.spargs)\n",
    "        except ValueError:\n",
    "            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n",
    "\n",
    "    def run(self, args, opts):\n",
    "        #settings = get_project_settings()\n",
    "\n",
    "        spider_loader = self.crawler_process.spider_loader\n",
    "        for spidername in args or spider_loader.list():\n",
    "            print(\"*********cralall spidername************\" + spidername)\n",
    "            self.crawler_process.crawl(spidername, **opts.spargs)   # 执行初始化（创建新爬虫对象）\n",
    "\n",
    "        self.crawler_process.start()    # 正式运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　　这里主要是用了self.crawler_process.spider_loader.list()方法获取项目下所有的spider，然后利用self.crawler_process.crawl运行spider<br>\n",
    "\n",
    "　　3、commands命令下添加__init__.py文件<br>\n",
    "\n",
    "touch __init__.py<br>\n",
    "　　注意：这一步一定不能省略。我就是因为这个问题折腾了一天。囧。。。就怪自己半路出家的吧。<br>\n",
    "\n",
    "　　如果省略了会报这样一个异常<br>\n",
    ">Traceback (most recent call last):<br>\n",
    "　　File \"/usr/local/bin/scrapy\", line 9, in \\<module\\><br>\n",
    "　　　　load_entry_point('Scrapy==1.0.0rc2', 'console_scripts', 'scrapy')()<br>\n",
    "　　File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.0.0rc2-py2.7.egg/scrapy/cmdline.py\", line 122, in execute<br>\n",
    "　　　　cmds = \\_get_commands_dict(settings, inproject)<br>\n",
    "　　File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.0.0rc2-py2.7.egg/scrapy/cmdline.py\", line 50, in \\_get_commands_dict<br>\n",
    "　　　　cmds.update(\\_get_commands_from_module(cmds_module, inproject))<br>\n",
    "　　File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.0.0rc2-py2.7.egg/scrapy/cmdline.py\", line 29, in \\_get_commands_from_module<br>\n",
    "　　　　for cmd in \\_iter_command_classes(module):<br>\n",
    "　　File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.0.0rc2-py2.7.egg/scrapy/cmdline.py\", line 20, in \\_iter_command_classes<br>\n",
    "　　　　for module in walk_modules(module_name):<br>\n",
    "　　File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.0.0rc2-py2.7.egg/scrapy/utils/misc.py\", line 63, in walk_modules<br>\n",
    "　　　　mod = import_module(path)<br>\n",
    "　　File \"/usr/local/lib/python2.7/importlib/\\_\\_init__.py\", line 37, in import_module<br>\n",
    "　　　　\\_\\_import__(name)<br>\n",
    "ImportError: No module named commands<br>\n",
    "\n",
    "　　一开始怎么找都找不到原因在哪。耗了我一整天，后来到http://stackoverflow.com/上得到了网友的帮助。再次感谢万能的互联网，要是没有那道墙该是多么的美好呀！扯远了，继续回来<br>\n",
    "\n",
    "　　4、settings.py目录下创建setup.py（这一步去掉也没影响，不知道官网帮助文档这么写有什么具体的意义。<br>\n",
    "\n",
    ">from setuptools import setup, find_packages<br>\n",
    "<br>\n",
    "setup(name='scrapy-mymodule',<br>\n",
    "　　entry_points={<br>\n",
    "　　　　'scrapy.commands': [<br>\n",
    "　　　　　　'crawlall=cnblogs.commands:crawlall',<br>\n",
    "　　　　],<br>\n",
    "　　},<br>\n",
    ")<br>\n",
    "\n",
    "　　这个文件的含义是定义了一个crawlall命令，cnblogs.commands为命令文件目录，crawlall为命令名。<br>\n",
    "\n",
    "　　5. 在settings.py中添加配置：<br>\n",
    ">COMMANDS_MODULE = 'cnblogs.commands'<br>\n",
    "\n",
    "　　6. 运行命令scrapy crawlall<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-2-629684eb49c5>, line 239)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-629684eb49c5>\"\u001b[1;36m, line \u001b[1;32m239\u001b[0m\n\u001b[1;33m    def create_crawler(self, crawler_or_spidercls):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# --coding:utf-8--\n",
    "# scrapy.crawler.py\n",
    "import logging\n",
    "import pprint\n",
    "import signal\n",
    "import warnings\n",
    " \n",
    "from twisted.internet import defer\n",
    "from zope.interface.exceptions import DoesNotImplement\n",
    " \n",
    "try:\n",
    "    # zope >= 5.0 only supports MultipleInvalid\n",
    "    from zope.interface.exceptions import MultipleInvalid\n",
    "except ImportError:\n",
    "    MultipleInvalid = None\n",
    " \n",
    "from zope.interface.verify import verifyClass\n",
    " \n",
    "from scrapy import signals, Spider\n",
    "from scrapy.core.engine import ExecutionEngine\n",
    "from scrapy.exceptions import ScrapyDeprecationWarning\n",
    "from scrapy.extension import ExtensionManager\n",
    "from scrapy.interfaces import ISpiderLoader\n",
    "from scrapy.settings import overridden_settings, Settings\n",
    "from scrapy.signalmanager import SignalManager\n",
    "from scrapy.utils.log import (\n",
    "    configure_logging,\n",
    "    get_scrapy_root_handler,\n",
    "    install_scrapy_root_handler,\n",
    "    log_scrapy_info,\n",
    "    LogCounterHandler,\n",
    ")\n",
    "from scrapy.utils.misc import create_instance, load_object\n",
    "from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n",
    "from scrapy.utils.reactor import install_reactor, verify_installed_reactor\n",
    " \n",
    "# scrapy 中文解释及其注释\n",
    "logger = logging.getLogger(__name__)\n",
    " \n",
    "#实际爬取执行的类\n",
    "class Crawler:\n",
    " \n",
    "    def __init__(self, spidercls, settings=None):\n",
    "        if isinstance(spidercls, Spider):\n",
    "            #要求传入的是类而不是实例\n",
    "            raise ValueError('The spidercls argument must be a class, not an object')\n",
    " \n",
    "        if isinstance(settings, dict) or settings is None:\n",
    "            #转化为setting对象\n",
    "            settings = Settings(settings)\n",
    " \n",
    "        self.spidercls = spidercls\n",
    "        self.settings = settings.copy()\n",
    "        #然后使用spidercls类的update_setting方式来更新设置：导入spider的custom_settings\n",
    "        self.spidercls.update_settings(self.settings)\n",
    " \n",
    "        self.signals = SignalManager(self)   # 声明一个SignalManager对象，这个对象主要是利用开源的python库\n",
    "        # pydispatch作消息的发送和路由.scrapy使用它发送关键的消息事件给关心者，如爬取开始，爬取结束等消息.\n",
    "\t    # 通过send_catch_log_deferred来发送消息，通过connect方法来注册关心消息的处理函数\n",
    "\n",
    "        #从类的setting中的STATS_CLASS拿到stats\n",
    "        self.stats = load_object(self.settings['STATS_CLASS'])(self)\n",
    "        #从setting中拿到loglevel 将初始化的LogCounterHandler 加入到logging.root\n",
    "        handler = LogCounterHandler(self, level=self.settings.get('LOG_LEVEL'))\n",
    "        logging.root.addHandler(handler)\n",
    "        # 显示出来所有被复写的setting\n",
    "        d = dict(overridden_settings(self.settings))\n",
    "        logger.info(\"Overridden settings:\\n%(settings)s\",\n",
    "                    {'settings': pprint.pformat(d)})\n",
    " \n",
    "        if get_scrapy_root_handler() is not None:\n",
    "            # scrapy root handler already installed: update it with new settings\n",
    "            install_scrapy_root_handler(self.settings)\n",
    "        # lambda is assigned to Crawler attribute because this way it is not\n",
    "        # garbage collected after leaving __init__ scope\n",
    "        self.__remove_handler = lambda: logging.root.removeHandler(handler)\n",
    "        #将该signals.engine_stopped信号的callback注册到self.__remove_handler函数上\n",
    "        self.signals.connect(self.__remove_handler, signals.engine_stopped)   # 注册引擎结束消息处理函数\n",
    "        # log格式指定\n",
    "        lf_cls = load_object(self.settings['LOG_FORMATTER'])\n",
    "        self.logformatter = lf_cls.from_crawler(self)\n",
    "        # 扩展 还没看\n",
    "        self.extensions = ExtensionManager.from_crawler(self)    # 添加ExtensionManager。\n",
    " \n",
    "        self.settings.freeze()\n",
    "        self.crawling = False\n",
    "        self.spider = None\n",
    "        self.engine = None\n",
    "        # spider、engine 属性仅设置为 None，会在 crawl() 方法中具体实例化\n",
    "    #defer.inlineCallbacks 装饰器 是指当使用异步调用该方法时候，\n",
    "    # 该方法可以用类似同步语法的方法写异步的工作，其中yield deferred对象后\n",
    "    # 后续代码会等待这个yield出去的deferred成功返回后再进行下一步\n",
    "    # 其中等待时间交还给reactor。\n",
    "    @defer.inlineCallbacks\n",
    "    def crawl(self, *args, **kwargs):    # 使用了Twisted的defer.inlineCallbacks装饰器，表明此函数非阻塞，异步执行\n",
    "        '''\n",
    "        调用Crawler的crawl方法开启一个爬取任务，通过调用spider的from_crawler方法来创建一个spider对象，\n",
    "        这样，许多spider类都可以使用crawler的方法和数据，属于依赖注入。spider的代码由程序员自己编写，\n",
    "        不同的爬虫类除了调用父类的from_crawler外，可以重定义这个方法来实现个性化实现。\n",
    "        '''\n",
    "        if self.crawling:\n",
    "            raise RuntimeError(\"Crawling already taking place\")\n",
    "        self.crawling = True\n",
    "        # 初始当前化抓取任务的 spider、engine、start_requests\n",
    "        try:\n",
    "            self.spider = self._create_spider(*args, **kwargs)\n",
    "            self.engine = self._create_engine()\n",
    "            # 从self.spider.start_requests()中拿到requests\n",
    "            start_requests = iter(self.spider.start_requests())\n",
    "            # 调用异步方法，开始爬虫爬取工作\n",
    "            yield self.engine.open_spider(self.spider, start_requests)   # 调用执行引擎打开spider；\n",
    "            # 调用核心的start方法，并将返回值包装成deferred对象\n",
    "            # 启动执行引擎。此时仍然并未真正开始爬取，仍然是CrawlerProcess.start()之前的预处理步骤。\n",
    "            yield defer.maybeDeferred(self.engine.start)\n",
    "        except Exception:\n",
    "            self.crawling = False\n",
    "            if self.engine is not None:\n",
    "                yield self.engine.close()\n",
    "            raise\n",
    " \n",
    "    def _create_spider(self, *args, **kwargs):\n",
    "        #调用传入spider类的from_crawler(self, args, *kwargs)\n",
    "        return self.spidercls.from_crawler(self, *args, **kwargs)\n",
    " \n",
    "    def _create_engine(self):\n",
    "        #ExecutionEngine(self, lambda _: self.stop()) 传入stop函数 实例化engine\n",
    "        return ExecutionEngine(self, lambda _: self.stop())\n",
    " \n",
    "    #重置self.crawling 为false 同时发出 异步命令self.engine.stop 包装一下self.engine.stop成为deferred对象\n",
    "    @defer.inlineCallbacks\n",
    "    def stop(self):\n",
    "        \"\"\"Starts a graceful stop of the crawler and returns a deferred that is\n",
    "        fired when the crawler is stopped.\"\"\"\n",
    "        if self.crawling:\n",
    "            self.crawling = False\n",
    "            yield defer.maybeDeferred(self.engine.stop)\n",
    " \n",
    "# 简单的说 如果你自己的应用用到reactor 可以考虑用这个类控制spider启停等，不然用CrawlerProcess\n",
    "class CrawlerRunner:\n",
    "    \"\"\"\n",
    "    This is a convenient helper class that keeps track of, manages and runs\n",
    "    crawlers inside an already setup :mod:`~twisted.internet.reactor`.\n",
    "    The CrawlerRunner object must be instantiated with a\n",
    "    :class:`~scrapy.settings.Settings` object.\n",
    "    This class shouldn't be needed (since Scrapy is responsible of using it\n",
    "    accordingly) unless writing scripts that manually handle the crawling\n",
    "    process. See :ref:`run-from-script` for an example.\n",
    "    \"\"\"\n",
    "    #property()函数是用来指定当前属性的文件描述符类的方法，这里就是把lambda作为 他的getter 返回的是self._crawlers\n",
    "    crawlers = property(\n",
    "        lambda self: self._crawlers,\n",
    "        doc=\"Set of :class:`crawlers <scrapy.crawler.Crawler>` started by \"\n",
    "            \":meth:`crawl` and managed by this class.\"\n",
    "    )\n",
    "    #从setting里 建立spiderloader实例\n",
    "    @staticmethod\n",
    "    def _get_spider_loader(settings):\n",
    "        \"\"\" Get SpiderLoader instance from settings \"\"\"\n",
    "        cls_path = settings.get('SPIDER_LOADER_CLASS')\n",
    "        #load_object(cls_path) 是将xx.xx路径转为实例的类\n",
    "        loader_cls = load_object(cls_path)\n",
    "        excs = (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement\n",
    "        try:\n",
    "            verifyClass(ISpiderLoader, loader_cls)\n",
    "        except excs:\n",
    "            warnings.warn(\n",
    "                'SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does '\n",
    "                'not fully implement scrapy.interfaces.ISpiderLoader interface. '\n",
    "                'Please add all missing methods to avoid unexpected runtime errors.',\n",
    "                category=ScrapyDeprecationWarning, stacklevel=2\n",
    "            )\n",
    "        #这里返回的是 setting中spiderloader加载目前的setting后的实例\n",
    "        return loader_cls.from_settings(settings.frozencopy())\n",
    " \n",
    "    def __init__(self, settings=None):\n",
    "        if isinstance(settings, dict) or settings is None:\n",
    "            settings = Settings(settings)\n",
    "        self.settings = settings\n",
    "        self.spider_loader = self._get_spider_loader(settings)  \n",
    "        # 会加载项目内所有 spider 类，并进行名称重复检查。\n",
    "        self._crawlers = set()    # _crawlers 保存所有的爬虫任务，每个任务是一个实例化的 Crawler 对象（通过\n",
    "        # 蜘蛛名或已存在的 Crawler 实例化），默认情况下只会存在一个 spider 的抓取任务，但也可以通过脚本\n",
    "        # 同时运行多个 spdier：同一进程运行多个 spider\n",
    "        self._active = set()     # 保存所有 Crawler 实例的 crawl() 方法，Crawler.crawl() 方\n",
    "        # 法被@defer.inlineCallbacks 装饰后返回一个 Deferred。\n",
    "        self.bootstrap_failed = False\n",
    "        self._handle_twisted_reactor()\n",
    " \n",
    "    @property\n",
    "    def spiders(self):\n",
    "        warnings.warn(\"CrawlerRunner.spiders attribute is renamed to \"\n",
    "                      \"CrawlerRunner.spider_loader.\",\n",
    "                      category=ScrapyDeprecationWarning, stacklevel=2)\n",
    "        return self.spider_loader\n",
    " \n",
    "    #生成一个crawler对象 然后调用其crawl方法，其中有些管理crawler的deferred的部分 在_crawl里\n",
    "    def crawl(self, crawler_or_spidercls, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Run a crawler with the provided arguments.\n",
    "        It will call the given Crawler's :meth:`~Crawler.crawl` method, while\n",
    "        keeping track of it so it can be stopped later.\n",
    "        If ``crawler_or_spidercls`` isn't a :class:`~scrapy.crawler.Crawler`\n",
    "        instance, this method will try to create one using this parameter as\n",
    "        the spider class given to it.\n",
    "        Returns a deferred that is fired when the crawling is finished.\n",
    "        :param crawler_or_spidercls: already created crawler, or a spider class\n",
    "            or spider's name inside the project to create it\n",
    "        :type crawler_or_spidercls: :class:`~scrapy.crawler.Crawler` instance,\n",
    "            :class:`~scrapy.spiders.Spider` subclass or string\n",
    "        :param args: arguments to initialize the spider\n",
    "        :param kwargs: keyword arguments to initialize the spider\n",
    "        \"\"\"\n",
    "        if isinstance(crawler_or_spidercls, Spider):\n",
    "            raise ValueError(\n",
    "                'The crawler_or_spidercls argument cannot be a spider object, '\n",
    "                'it must be a spider class (or a Crawler object)')\n",
    "        crawler = self.create_crawler(crawler_or_spidercls)   \n",
    "        # 创建 Crawler 实例（通过传入蜘蛛名或已存在的 Crawler 初始化）\n",
    "        return self._crawl(crawler, *args, **kwargs)\n",
    "        # 返回一个Deferred对象给CrawlerProcess，把Deferred对象加入_active集合，然后就可以在必要时\n",
    "        # 结束Crawler，并通过向Deferred中添加_done callback来跟踪一个Crawler的结束。\n",
    " \n",
    "    def _crawl(self, crawler, *args, **kwargs):\n",
    "        self.crawlers.add(crawler) #这个集合里是crawler\n",
    "        d = crawler.crawl(*args, **kwargs)\n",
    "        self._active.add(d) # 这个集合里是执行crawl后的deferred对象\n",
    "        # 将新创建的 Crawler 及 Crawler.crawl() 分别添加进 self.crawlers、self._active。\n",
    " \n",
    "        def _done(result):\n",
    "            # discard 相当于不报错的remove\n",
    "            self.crawlers.discard(crawler)\n",
    "            self._active.discard(d)\n",
    "            #a|=2等价于a=a|2(按位或)\n",
    "            self.bootstrap_failed |= not getattr(crawler, 'spider', None)\n",
    "            return result\n",
    "        # 运行crawl前 加入到管理集合，给其deferred对象添加结束后清理管理集合的代码\n",
    "        return d.addBoth(_done)    # Deferred\n",
    "        # 为 Crawler.crawl() 添加回调_done，当前抓取任务完毕后从 self.crawlers、self._active 中删除\n",
    "        # 上一步所做的添加。最终返回的是 Crawler.crawl() 这个 Deferred。\n",
    "'''\n",
    "crawl() 回调链是被立即激活的 (@defer.inlineCallbacks 性质如此)，随着函数一步步深入的执行，最终阻塞并\n",
    "等待内层 Deferred 的激活（这些 Deferred 使用类似 reactor.callLater() 之类的方法注册），reactor.run() \n",
    "执行后激活等待中的 Deferred，调度并开始抓取/处理数据。\n",
    "所以：在 crawl() 执行中，scrapy 并没有开始抓取数据，只是做了系列的初始化动作。\n",
    "'''\n",
    "\n",
    "    def create_crawler(self, crawler_or_spidercls):\n",
    "        \"\"\"\n",
    "        如果crawler_or_spidercls（命令行输入的spider名称）是一个Spider的子类（已经运行）则创建一个\n",
    "        新的Crawler，如果crawler_or_spidercls是一个字符串（未运行），则根据名称来查找对应的spider\n",
    "        并创建一个Crawler实例并执行Crawler的初始化。\n",
    "        Return a :class:`~scrapy.crawler.Crawler` object.\n",
    "        * If ``crawler_or_spidercls`` is a Crawler, it is returned as-is.\n",
    "        * If ``crawler_or_spidercls`` is a Spider subclass, a new Crawler\n",
    "          is constructed for it.\n",
    "        * If ``crawler_or_spidercls`` is a string, this function finds\n",
    "          a spider with this name in a Scrapy project (using spider loader),\n",
    "          then creates a Crawler instance for it.\n",
    "        \"\"\"\n",
    "        if isinstance(crawler_or_spidercls, Spider):\n",
    "            raise ValueError(\n",
    "                'The crawler_or_spidercls argument cannot be a spider object, '\n",
    "                'it must be a spider class (or a Crawler object)')\n",
    "        if isinstance(crawler_or_spidercls, Crawler):\n",
    "            return crawler_or_spidercls\n",
    " \n",
    "        return self._create_crawler(crawler_or_spidercls)\n",
    " \n",
    "    def _create_crawler(self, spidercls):\n",
    "        if isinstance(spidercls, str):\n",
    "            spidercls = self.spider_loader.load(spidercls)\n",
    "        # 实际实例化Crawler进行的地方 传入的是spider的类和setting\n",
    "        return Crawler(spidercls, self.settings)\n",
    " \n",
    "    def stop(self):\n",
    "        \"\"\"\n",
    "        Stops simultaneously all the crawling jobs taking place.\n",
    "        Returns a deferred that is fired when they all have ended.\n",
    "        \"\"\"\n",
    "        #跟crawler不一样，这个是一个由每个crawler执行stop函数后返回的deferred对象列表\n",
    "        return defer.DeferredList([c.stop() for c in list(self.crawlers)])\n",
    " \n",
    "    # 跟多进程的join类似，等待所有crawler完成任务\n",
    "    # 上面说到这个_active集合是所有crawler的deferred对象 把他们yield出去 以便调用后续callback\n",
    "    @defer.inlineCallbacks\n",
    "    def join(self):\n",
    "        \"\"\"\n",
    "        join()\n",
    "        Returns a deferred that is fired when all managed :attr:`crawlers` have\n",
    "        completed their executions.\n",
    "        \"\"\"\n",
    "        # 此函数首先调用join函数来对前面所有Crawler的crawl方法返回的Deferred对象添加一个_stop_reactor方法，\n",
    "        # 当所有Crawler对象都结束时用来关闭reactor。\n",
    "        while self._active:\n",
    "            yield defer.DeferredList(self._active)\n",
    "    # 这个方法返回从传入setting 字符中加载 用 load_object()加载进来的recator实例 或者啥也不做\n",
    "    def _handle_twisted_reactor(self):\n",
    "        if self.settings.get(\"TWISTED_REACTOR\"):\n",
    "            verify_installed_reactor(self.settings[\"TWISTED_REACTOR\"])\n",
    " \n",
    "#这个crawlerProcess 是用来在不用recator的应用里使用 同时在一个进程里使用多个spider的\n",
    "# 如果只是使用scrapy就不用改这个，除非想把scrapy放到你自己应用里　\n",
    "# 实际上这个是上一个 CrawlerRunner 添加了reactor后的东西\n",
    "class CrawlerProcess(CrawlerRunner):\n",
    "    \"\"\"\n",
    "    A class to run multiple scrapy crawlers in a process simultaneously.\n",
    "    This class extends :class:`~scrapy.crawler.CrawlerRunner` by adding support\n",
    "    for starting a :mod:`~twisted.internet.reactor` and handling shutdown\n",
    "    signals, like the keyboard interrupt command Ctrl-C. It also configures\n",
    "    top-level logging.\n",
    "    This utility should be a better fit than\n",
    "    :class:`~scrapy.crawler.CrawlerRunner` if you aren't running another\n",
    "    :mod:`~twisted.internet.reactor` within your application.\n",
    "    The CrawlerProcess object must be instantiated with a\n",
    "    :class:`~scrapy.settings.Settings` object.\n",
    "    :param install_root_handler: whether to install root logging handler\n",
    "        (default: True)\n",
    "    This class shouldn't be needed (since Scrapy is responsible of using it\n",
    "    accordingly) unless writing scripts that manually handle the crawling\n",
    "    process. See :ref:`run-from-script` for an example.\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, settings=None, install_root_handler=True):\n",
    "        super().__init__(settings)\n",
    "        #将shutdownhandler 加载为函数_signal_shutdown\n",
    "        install_shutdown_handlers(self._signal_shutdown)      # 注册关闭句柄，如ctrl+c\n",
    "        configure_logging(self.settings, install_root_handler)     # 配置logging\n",
    "        log_scrapy_info(self.settings)       # 打印当前scrapy概况\n",
    " \n",
    "    def _signal_shutdown(self, signum, _):\n",
    "        from twisted.internet import reactor\n",
    "        #将shutdownhandler注册为 _signal_kill\n",
    "        install_shutdown_handlers(self._signal_kill)\n",
    "        signame = signal_names[signum]\n",
    "        logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \",\n",
    "                    {'signame': signame})\n",
    "        #使用reactor.callFromThread(self._graceful_stop_reactor)命令调用自身的结束语句\n",
    "        reactor.callFromThread(self._graceful_stop_reactor)\n",
    " \n",
    "    def _signal_kill(self, signum, _):\n",
    "        from twisted.internet import reactor\n",
    "        #将shutdownhandler注册为signal.SIG_IGN\n",
    "        install_shutdown_handlers(signal.SIG_IGN)\n",
    "        signame = signal_names[signum]\n",
    "        logger.info('Received %(signame)s twice, forcing unclean shutdown',\n",
    "                    {'signame': signame})\n",
    "        #直接将recator关闭\n",
    "        reactor.callFromThread(self._stop_reactor)\n",
    " \n",
    "    def start(self, stop_after_crawl=True):\n",
    "        \"\"\"\n",
    "        This method starts a :mod:`~twisted.internet.reactor`, adjusts its pool\n",
    "        size to :setting:`REACTOR_THREADPOOL_MAXSIZE`, and installs a DNS cache\n",
    "        based on :setting:`DNSCACHE_ENABLED` and :setting:`DNSCACHE_SIZE`.\n",
    "        If ``stop_after_crawl`` is True, the reactor will be stopped after all\n",
    "        crawlers have finished, using :meth:`join`.\n",
    "        :param bool stop_after_crawl: stop or not the reactor when all\n",
    "            crawlers have finished\n",
    "        \"\"\"\n",
    "        # 注册系列 reactor 相关配置，调用 reactor.run() 开始事件循环。\n",
    "        from twisted.internet import reactor\n",
    "        #设置如果爬完后关闭recator的话 就添加相应的callback结束callback 这就是上面join的用处 ，如果这里不设置为true\n",
    "        # 那么这个reactor就会留着不销毁\n",
    "        if stop_after_crawl:\n",
    "            d = self.join()\n",
    "            # 通过 join() 方法返回一个由 self._active 初始化的 DeferredList，即：包含的是一个个\n",
    "            # 抓取任务（Crawler.crawl()）\n",
    "            # Don't start the reactor if the deferreds are already fired\n",
    "            if d.called:\n",
    "                return\n",
    "            d.addBoth(self._stop_reactor)     # 为 DeferredList 添加关闭 reactor 的回调\n",
    "        #加载一个配置 threadpool和dns_resolver的配置到recator\n",
    "        resolver_class = load_object(self.settings[\"DNS_RESOLVER\"])\n",
    "        resolver = create_instance(resolver_class, self.settings, self, reactor=reactor)\n",
    "        resolver.install_on_reactor()   # 配置 dns 缓存、线程池、系统事件触发器（没有了解过这几个 API）\n",
    "        tp = reactor.getThreadPool()\n",
    "        tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n",
    "        reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n",
    "        # 启动reactor事件循环，标志着所有爬虫正式运行，如果没有手动结束，就只会在所有爬虫全部爬取完成后\n",
    "        # 才会自动结束。\n",
    "        reactor.run(installSignalHandlers=False)  # blocking call\n",
    " \n",
    "    def _graceful_stop_reactor(self):\n",
    "        # 给所有crawler 的deferred对象后面添加一个完成后销毁的动作\n",
    "        d = self.stop()\n",
    "        d.addBoth(self._stop_reactor)\n",
    "        return d\n",
    " \n",
    "    def _stop_reactor(self, _=None):\n",
    "        from twisted.internet import reactor\n",
    "        try:\n",
    "            reactor.stop()\n",
    "        except RuntimeError:  # raised if already stopped or in shutdown stage\n",
    "            pass\n",
    " \n",
    "    def _handle_twisted_reactor(self):\n",
    "        if self.settings.get(\"TWISTED_REACTOR\"):\n",
    "            install_reactor(self.settings[\"TWISTED_REACTOR\"], self.settings[\"ASYNCIO_EVENT_LOOP\"])\n",
    "        super()._handle_twisted_reactor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
