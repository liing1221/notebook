{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapy之爬虫启动\n",
    "　　参考：[Scrapy的抓取流程——CrawlerProcess](https://blog.csdn.net/okm6666/article/details/89160886)<br>\n",
    "　　　　　[通过核心ＡＰＩ启动单个或多个scrapy爬虫](https://www.jianshu.com/p/add5c59d698a)<br>\n",
    "　　　　　[Scrapy进阶-命令行的工作原理（以runspider为例）](https://www.jianshu.com/p/8e252b2272d8)<br>\n",
    "　　　　　[同时运行多个scrapy爬虫的几种方法（自定义scrapy项目命令）](https://www.cnblogs.com/rwxwsblog/p/4578764.html)<br>\n",
    "　　　　　[scrapy项目下spiders内多个爬虫同时运行](https://blog.csdn.net/beyond_f/article/details/74626451)<br>\n",
    "　　　　　[python scrapy项目下spiders内多个爬虫同时运行](https://blog.csdn.net/qq_38282706/article/details/80977576)<br>\n",
    "     \n",
    "## 一、通过自定义scrapy命令的方式来运行\n",
    "　　配置说明：https://scrapy-chs.readthedocs.io/zh_CN/1.0/topics/commands.html<br>\n",
    "1、创建commands目录<br>\n",
    ">    mkdir commands<br>\n",
    "\n",
    "　　注意：commands和spiders目录是同级的<br>\n",
    "2、在commands下面添加一个文件crawlall.py<br>\n",
    "　　注意：这里主要通过修改scrapy的crawl命令来完成同时执行spider的效果。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scrapy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-16809395bb44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommands\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mScrapyCommand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcrawler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCrawlerRunner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscrapy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marglist_to_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mCommand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mScrapyCommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scrapy'"
     ]
    }
   ],
   "source": [
    "from scrapy.commands import ScrapyCommand  \n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.conf import arglist_to_dict\n",
    "\n",
    "class Command(ScrapyCommand):\n",
    "  \n",
    "    requires_project = True\n",
    "  \n",
    "    def syntax(self):  \n",
    "        return '[options]'  \n",
    "  \n",
    "    def short_desc(self):  \n",
    "        return 'Runs all of the spiders'  \n",
    "\n",
    "    def add_options(self, parser):\n",
    "        ScrapyCommand.add_options(self, parser)\n",
    "        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n",
    "                          help=\"set spider argument (may be repeated)\")\n",
    "        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\",\n",
    "                          help=\"dump scraped items into FILE (use - for stdout)\")\n",
    "        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n",
    "                          help=\"format to use for dumping items with -o\")\n",
    "\n",
    "    def process_options(self, args, opts):\n",
    "        ScrapyCommand.process_options(self, args, opts)\n",
    "        try:\n",
    "            opts.spargs = arglist_to_dict(opts.spargs)\n",
    "        except ValueError:\n",
    "            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n",
    "\n",
    "    def run(self, args, opts):\n",
    "        #settings = get_project_settings()\n",
    "\n",
    "        spider_loader = self.crawler_process.spider_loader\n",
    "        for spidername in args or spider_loader.list():\n",
    "            print(\"*********cralall spidername************\" + spidername)\n",
    "            self.crawler_process.crawl(spidername, **opts.spargs)\n",
    "\n",
    "        self.crawler_process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "　　这里主要是用了self.crawler_process.spider_loader.list()方法获取项目下所有的spider，然后利用self.crawler_process.crawl运行spider<br>\n",
    "\n",
    "　　3、commands命令下添加__init__.py文件<br>\n",
    "\n",
    "touch __init__.py<br>\n",
    "　　注意：这一步一定不能省略。我就是因为这个问题折腾了一天。囧。。。就怪自己半路出家的吧。<br>\n",
    "\n",
    "　　如果省略了会报这样一个异常<br>\n",
    ">Traceback (most recent call last):<br>\n",
    "　　File \"/usr/local/bin/scrapy\", line 9, in \\<module\\><br>\n",
    "　　　　load_entry_point('Scrapy==1.0.0rc2', 'console_scripts', 'scrapy')()<br>\n",
    "　　File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.0.0rc2-py2.7.egg/scrapy/cmdline.py\", line 122, in execute<br>\n",
    "　　　　cmds = \\_get_commands_dict(settings, inproject)<br>\n",
    "　　File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.0.0rc2-py2.7.egg/scrapy/cmdline.py\", line 50, in \\_get_commands_dict<br>\n",
    "　　　　cmds.update(\\_get_commands_from_module(cmds_module, inproject))<br>\n",
    "　　File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.0.0rc2-py2.7.egg/scrapy/cmdline.py\", line 29, in \\_get_commands_from_module<br>\n",
    "　　　　for cmd in \\_iter_command_classes(module):<br>\n",
    "　　File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.0.0rc2-py2.7.egg/scrapy/cmdline.py\", line 20, in \\_iter_command_classes<br>\n",
    "　　　　for module in walk_modules(module_name):<br>\n",
    "　　File \"/usr/local/lib/python2.7/site-packages/Scrapy-1.0.0rc2-py2.7.egg/scrapy/utils/misc.py\", line 63, in walk_modules<br>\n",
    "　　　　mod = import_module(path)<br>\n",
    "　　File \"/usr/local/lib/python2.7/importlib/\\_\\_init__.py\", line 37, in import_module<br>\n",
    "　　　　\\_\\_import__(name)<br>\n",
    "ImportError: No module named commands<br>\n",
    "\n",
    "　　一开始怎么找都找不到原因在哪。耗了我一整天，后来到http://stackoverflow.com/上得到了网友的帮助。再次感谢万能的互联网，要是没有那道墙该是多么的美好呀！扯远了，继续回来<br>\n",
    "\n",
    "　　4、settings.py目录下创建setup.py（这一步去掉也没影响，不知道官网帮助文档这么写有什么具体的意义。<br>\n",
    "\n",
    ">from setuptools import setup, find_packages<br>\n",
    "<br>\n",
    "setup(name='scrapy-mymodule',<br>\n",
    "　　entry_points={<br>\n",
    "　　　　'scrapy.commands': [<br>\n",
    "　　　　　　'crawlall=cnblogs.commands:crawlall',<br>\n",
    "　　　　],<br>\n",
    "　　},<br>\n",
    ")<br>\n",
    "\n",
    "　　这个文件的含义是定义了一个crawlall命令，cnblogs.commands为命令文件目录，crawlall为命令名。<br>\n",
    "\n",
    "　　5. 在settings.py中添加配置：<br>\n",
    ">COMMANDS_MODULE = 'cnblogs.commands'<br>\n",
    "\n",
    "　　6. 运行命令scrapy crawlall<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
