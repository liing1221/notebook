{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## urllib.robotparser详解\n",
    "　　参考：[urllib.robotparser --- robots.txt 语法分析程序¶](https://docs.python.org/zh-cn/3.7/library/urllib.robotparser.html)<br>\n",
    "　　　　　[urllib.robotparser](https://www.jianshu.com/p/21fc41453784)<br>　\n",
    "   \n",
    "　　此模块提供了一个单独的类 RobotFileParser，它可以回答关于某个特定用户代理User-Agent是否能在 Web 站点获取发布 robots.txt 文件的 URL 的问题。 有关 robots.txt 文件结构的更多细节请参阅 http://www.robotstxt.org/orig.html.<br>\n",
    "　　1、利用 urllib.robotparser 模块可以对网站的 Robots 协议进行分析<br>\n",
    "　　2、Robots协议（也称为爬虫协议、机器人协议等）的全称是“网络爬虫排除标准”（Robots Exclusion Protocol），网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。robots.txt文件是一个文本文件，放在站点的根目录下。<br>\n",
    "　　3、当一个搜索蜘蛛访问一个站点时，它会首先检查该站点根目录下是否存在robots.txt，如果存在，搜索机器人就会按照该文件中的内容来确定访问的范围；如果该文件不存在，所有的搜索蜘蛛将能够访问网站上所有没有被口令保护的页面。<br>\n",
    "\n",
    "class urllib.robotparser.RobotFileParser(url='')<br>　\n",
    "　　这个类提供了一些可以读取、解析和回答关于 url 上的 robots.txt 文件的问题的方法。<br>\n",
    "set_url(url)<br>　\n",
    "　　设置指向 robots.txt 文件的 URL。<br>\n",
    "read()<br>　\n",
    "　　读取 robots.txt URL 并将其输入解析器。<br>\n",
    "parse(lines)<br>　\n",
    "　　解析行参数。<br>\n",
    "can_fetch(useragent, url)<br>\n",
    "　　如果允许 useragent 按照被解析 robots.txt 文件中的规则来获取 url 则返回 True。<br>\n",
    "mtime()<br>　\n",
    "　　返回最近一次获取 robots.txt 文件的时间。 这适用于需要定期检查 robots.txt 文件更新情况的长时间运行的网页爬虫。<br>\n",
    "modified()<br>　\n",
    "　　将最近一次获取 robots.txt 文件的时间设置为当前时间。<br>\n",
    "crawl_delay(useragent)<br>　\n",
    "　　为指定的 useragent 从 robots.txt 返回 Crawl-delay 形参。 如果此形参不存在或不适用于指定的 useragent 或者此形参的 robots.txt 条目存在语法错误，则返回 None。<br>\n",
    "\n",
    "3.6 新版功能.<br>　\n",
    "request_rate(useragent)<br>　\n",
    "　　以 named tuple RequestRate(requests, seconds) 的形式从 robots.txt 返回 Request-rate 形参的内容。 如果此形参不存在或不适用于指定的 useragent 或者此形参的 robots.txt 条目存在语法错误，则返回 None。<br>　\n",
    "\n",
    "RobotFileParser 类的基本用法:<br>　\n",
    ">import urllib.robotparser<br>\n",
    "rp = urllib.robotparser.RobotFileParser()<br>\n",
    "rp.set_url(\"http://www.musi-cal.com/robots.txt\")<br>\n",
    "rp.read()<br>\n",
    "rrate = rp.request_rate(\"*\")<br>\n",
    "rrate.requests<br>\n",
    "3<br>\n",
    "rrate.seconds<br>\n",
    "20<br>\n",
    "rp.crawl_delay(\"*\")<br>\n",
    "6<br>\n",
    "rp.can_fetch(\"*\", \"http://www.musi-cal.com/cgi-bin/search?city=San+Francisco\")<br>\n",
    "False<br>\n",
    "rp.can_fetch(\"*\", \"http://www.musi-cal.com/\")<br>\n",
    "True<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
